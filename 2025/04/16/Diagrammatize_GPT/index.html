<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>《GPT图解》学习笔记 | Merick'Blog</title><meta name="author" content="Mercik"><meta name="copyright" content="Mercik"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Chapter1：N-grams &amp; Bag-of-words  N-grams模型 N-grams是指将文本分割为连续的长度为N的文本片段，统计每个片段的频数以计算每个片段出现的条件概率，从而计算完整句子的出现概率。 该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现">
<meta property="og:type" content="article">
<meta property="og:title" content="《GPT图解》学习笔记">
<meta property="og:url" content="https://duqi666.github.io/2025/04/16/Diagrammatize_GPT/index.html">
<meta property="og:site_name" content="Merick&#39;Blog">
<meta property="og:description" content="Chapter1：N-grams &amp; Bag-of-words  N-grams模型 N-grams是指将文本分割为连续的长度为N的文本片段，统计每个片段的频数以计算每个片段出现的条件概率，从而计算完整句子的出现概率。 该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://duqi666.github.io/imgs/GPT2.jpg">
<meta property="article:published_time" content="2025-04-16T03:29:14.257Z">
<meta property="article:modified_time" content="2025-05-05T15:18:46.972Z">
<meta property="article:author" content="Mercik">
<meta property="article:tag" content="学习">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://duqi666.github.io/imgs/GPT2.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "《GPT图解》学习笔记",
  "url": "https://duqi666.github.io/2025/04/16/Diagrammatize_GPT/",
  "image": "https://duqi666.github.io/imgs/GPT2.jpg",
  "datePublished": "2025-04-16T03:29:14.257Z",
  "dateModified": "2025-05-05T15:18:46.972Z",
  "author": [
    {
      "@type": "Person",
      "name": "Mercik",
      "url": "https://duqi666.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://duqi666.github.io/2025/04/16/Diagrammatize_GPT/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '《GPT图解》学习笔记',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/universe.css"><link rel="stylesheet" href="/css/category.css"><link rel="stylesheet" href="/css/cat.css"><link rel="stylesheet" href="/css/neon_lamp.css"><link rel="stylesheet" href="/css/discuss.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      if ($loadingBox.classList.contains('loaded')) return
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()

  if (document.readyState === 'complete') {
    preloader.endLoading()
  } else {
    window.addEventListener('load', preloader.endLoading)
    document.addEventListener('DOMContentLoaded', preloader.endLoading)
    // Add timeout protection: force end after 7 seconds
    setTimeout(preloader.endLoading, 7000)
  }

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/imgs/I.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 模式</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="javascript:void(0)"><i class="fa-fw fa-regular fa-moon"></i><span> dark</span></a></li><li><a class="site-page child" href="javascript:void(1)"><i class="fa-fw fa-regular fa-sun"></i><span> light</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/travel/"><i class="fa-fw fas fa-video"></i><span> 足迹</span></a></div><div class="menus_item"><a class="site-page" href="/charts/"><i class="fa-fw fas fa-video"></i><span> 图表</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/imgs/GPT2.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Merick'Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">《GPT图解》学习笔记</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 模式</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="javascript:void(0)"><i class="fa-fw fa-regular fa-moon"></i><span> dark</span></a></li><li><a class="site-page child" href="javascript:void(1)"><i class="fa-fw fa-regular fa-sun"></i><span> light</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/travel/"><i class="fa-fw fas fa-video"></i><span> 足迹</span></a></div><div class="menus_item"><a class="site-page" href="/charts/"><i class="fa-fw fas fa-video"></i><span> 图表</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">《GPT图解》学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-04-16T03:29:14.257Z" title="Created 2025-04-16 11:29:14">2025-04-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-05-05T15:18:46.972Z" title="Updated 2025-05-05 23:18:46">2025-05-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">15.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>63mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><meta name="referrer" content="no-referrer" />
<h1 id="chapter1n-grams-bag-of-words"><a class="markdownIt-Anchor" href="#chapter1n-grams-bag-of-words"></a> Chapter1：N-grams &amp; Bag-of-words</h1>
<h2 id="n-grams模型"><a class="markdownIt-Anchor" href="#n-grams模型"></a> N-grams模型</h2>
<p>N-grams是指将文本分割为连续的长度为N的文本片段，统计每个片段的频数以计算每个片段出现的条件概率，从而计算完整句子的出现概率。</p>
<p>该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram，下面具体解释其数学实现：</p>
<p>对于一个有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">m</span></span></span></span>个词语的语句，其条件概率为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>3</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>w</mi><mi>m</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>2</mn></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>3</mn></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>m</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>m</mi><mo>−</mo><mn>2</mn></mrow></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>w</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w_1,w_2,w_3...w_m) = P(w_1)P(w_2|w_1)P(w_3|w_2,w_1)...P(w_m|w_{m-1},w_{m-2}...w_1)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>可以利用<strong>马尔科夫假设</strong>（当前状态只与前面n个状态相关）简化上述公式，具体体现为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>m</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>w</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>m</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>m</mi><mo>−</mo><mn>2</mn></mrow></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>w</mi><mrow><mi>m</mi><mo>−</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w_m|w_1,w_2...w_{m-1}) = P(w_m|w_{m-1},w_{m-2}...w_{m-n})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.25833100000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>当n取1时，既每个状态只与前面一个状态相关，公式可以简化为</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>3</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>w</mi><mi>m</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>2</mn></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>3</mn></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>m</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w_1,w_2,w_3...w_m) = P(w_1)P(w_2|w_1)P(w_3|w_2)...P(w_m|w_{m-1})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>这就是N-grams模型的数学基础，通过语料中的统计学结果计算一句话的概率，具体应用场景可以是，根据一部分语料预测接下来的完整句子（只需要找到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span>最大的句子表达）</p>
<p><strong>举个例子</strong>，当N取2时，对于句子“我爱你”，可以分为“我爱”，“爱你”两种文本片段，假设我们有一大堆语料文本，可以统计得到“我X”出现了100次，其中“我爱”出现了60次，则“我爱”片段条件概率为60%，那么当文本最后一个字是“我”时，我们会选择概率最大的“爱”作为后续输出。</p>
<p>下面通过具体例子实现N-Grams：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">corpus = [ <span class="string">&quot;我喜欢吃苹果&quot;</span>,</span><br><span class="line">        <span class="string">&quot;我喜欢吃香蕉&quot;</span>,</span><br><span class="line">        <span class="string">&quot;她喜欢吃葡萄&quot;</span>,</span><br><span class="line">        <span class="string">&quot;他不喜欢吃香蕉&quot;</span>,</span><br><span class="line">        <span class="string">&quot;他喜欢吃苹果&quot;</span>,</span><br><span class="line">        <span class="string">&quot;她喜欢吃草莓&quot;</span>]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">return</span> [char <span class="keyword">for</span> char <span class="keyword">in</span> text]</span><br><span class="line">   <span class="comment">#分词方式很多，也有很多处理方法，这里为了方便直接取一个字</span></span><br></pre></td></tr></table></figure>
<p>然后需要统计grams的频数，设计函数<code>count_ngrams</code>统计频数，可以自定义n统计，当n=2时，片段为“我喜”，“喜欢”等等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">count_ngrams</span>(<span class="params">corpus,n</span>):</span><br><span class="line">    ngrams_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> text <span class="keyword">in</span> corpus:</span><br><span class="line">        tokens = tokenize(text)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(tokens)-n+<span class="number">1</span>):</span><br><span class="line">            prefix = <span class="string">&#x27;&#x27;</span>.join(tokens[i:i+n-<span class="number">1</span>])</span><br><span class="line">            token = tokens[i+n-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> prefix <span class="keyword">in</span> ngrams_count:</span><br><span class="line">                <span class="keyword">if</span> token <span class="keyword">in</span> ngrams_count[prefix]:</span><br><span class="line">                    ngrams_count[prefix][token]+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    ngrams_count[prefix][token]=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ngrams_count[prefix]=&#123;token:<span class="number">1</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> ngrams_count</span><br><span class="line">bigram_counts = count_ngrams(corpus, <span class="number">2</span>) <span class="comment"># 计算 bigram 词频</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bigram 词频：&quot;</span>) <span class="comment"># 打印 bigram 词频</span></span><br><span class="line"><span class="keyword">for</span> prefix, counts <span class="keyword">in</span> bigram_counts.items():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot;&quot;</span>.join(prefix), <span class="built_in">dict</span>(counts)))</span><br><span class="line"><span class="comment"># 我: &#123;&#x27;喜&#x27;: 2&#125;</span></span><br><span class="line"><span class="comment"># 喜: &#123;&#x27;欢&#x27;: 6&#125;</span></span><br><span class="line"><span class="comment"># 欢: &#123;&#x27;吃&#x27;: 6&#125;</span></span><br><span class="line"><span class="comment"># 吃: &#123;&#x27;苹&#x27;: 2, &#x27;香&#x27;: 2, &#x27;葡&#x27;: 1, &#x27;草&#x27;: 1&#125;</span></span><br><span class="line"><span class="comment"># 苹: &#123;&#x27;果&#x27;: 2&#125;</span></span><br><span class="line"><span class="comment"># 香: &#123;&#x27;蕉&#x27;: 2&#125;</span></span><br><span class="line"><span class="comment"># 她: &#123;&#x27;喜&#x27;: 2&#125;</span></span><br><span class="line"><span class="comment"># 葡: &#123;&#x27;萄&#x27;: 1&#125;</span></span><br><span class="line"><span class="comment"># 他: &#123;&#x27;不&#x27;: 1, &#x27;喜&#x27;: 1&#125;</span></span><br><span class="line"><span class="comment"># 不: &#123;&#x27;喜&#x27;: 1&#125;</span></span><br><span class="line"><span class="comment"># 草: &#123;&#x27;莓&#x27;: 1&#125;</span></span><br></pre></td></tr></table></figure>
<p>当n=3时，片段为“我喜欢”等，前缀为“我喜”：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我喜: &#123;&#x27;欢&#x27;: 2&#125;</span></span><br><span class="line"><span class="comment"># 喜欢: &#123;&#x27;吃&#x27;: 6&#125;</span></span><br><span class="line"><span class="comment"># 欢吃: &#123;&#x27;苹&#x27;: 2, &#x27;香&#x27;: 2, &#x27;葡&#x27;: 1, &#x27;草&#x27;: 1&#125;</span></span><br><span class="line"><span class="comment"># 吃苹: &#123;&#x27;果&#x27;: 2&#125;</span></span><br><span class="line"><span class="comment"># 吃香: &#123;&#x27;蕉&#x27;: 2&#125;</span></span><br><span class="line"><span class="comment"># 她喜: &#123;&#x27;欢&#x27;: 2&#125;</span></span><br><span class="line"><span class="comment"># 吃葡: &#123;&#x27;萄&#x27;: 1&#125;</span></span><br><span class="line"><span class="comment"># 他不: &#123;&#x27;喜&#x27;: 1&#125;</span></span><br><span class="line"><span class="comment"># 不喜: &#123;&#x27;欢&#x27;: 1&#125;</span></span><br><span class="line"><span class="comment"># 他喜: &#123;&#x27;欢&#x27;: 1&#125;</span></span><br><span class="line"><span class="comment"># 吃草: &#123;&#x27;莓&#x27;: 1&#125;</span></span><br></pre></td></tr></table></figure>
<p>根据grams频数计算grams的条件概率，函数为<code>ngram_probabilities</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ngram_probabilities</span>(<span class="params">ngrams_count</span>):</span><br><span class="line">    <span class="keyword">for</span> prefix,tokens <span class="keyword">in</span> ngrams_count.items():</span><br><span class="line">        tokens_count_sum = <span class="built_in">sum</span>(tokens.values())</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens.keys():</span><br><span class="line">            tokens[token] /= tokens_count_sum</span><br><span class="line">    <span class="keyword">return</span> ngrams_count</span><br><span class="line"></span><br><span class="line">bigram_probs = ngram_probabilities(bigram_counts) <span class="comment"># 计算 bigram 出现的概率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nbigram 出现的概率 :&quot;</span>) <span class="comment"># 打印 bigram 概率</span></span><br><span class="line"><span class="keyword">for</span> prefix, probs <span class="keyword">in</span> bigram_probs.items():</span><br><span class="line"> <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot;&quot;</span>.join(prefix), <span class="built_in">dict</span>(probs)))</span><br><span class="line"><span class="comment"># 我: &#123;&#x27;喜&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="comment"># 喜: &#123;&#x27;欢&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="comment"># 欢: &#123;&#x27;吃&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="comment"># 吃: &#123;&#x27;苹&#x27;: 0.3333333333333333, &#x27;香&#x27;: 0.3333333333333333, &#x27;葡&#x27;: 0.16666666666666666, &#x27;草&#x27;: 0.16666666666666666&#125;</span></span><br><span class="line"><span class="comment"># 苹: &#123;&#x27;果&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="comment"># 香: &#123;&#x27;蕉&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="comment"># 她: &#123;&#x27;喜&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="comment"># 葡: &#123;&#x27;萄&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="comment"># 他: &#123;&#x27;不&#x27;: 0.5, &#x27;喜&#x27;: 0.5&#125;</span></span><br><span class="line"><span class="comment"># 不: &#123;&#x27;喜&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="comment"># 草: &#123;&#x27;莓&#x27;: 1.0&#125;</span></span><br></pre></td></tr></table></figure>
<p>最后应用场景是根据部分文本生成接下来的文本，<code>generate_next_token</code>函数可以根据前一个片段生成后一个token，具体方式就是选择条件概率最大的文本输出</p>
<p>需要注意的是文本生成的截止条件，如果生成的最后一个字在词表片段中不存在以它开头的前缀时，就停止，例如如果生成的最后一个字的“果”，上述<code>bigram_probs</code>中没有以“果”为前缀的片段，则终止输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_next_token</span>(<span class="params">prefix,ngrams_probs</span>):</span><br><span class="line">    <span class="keyword">if</span> prefix <span class="keyword">in</span> ngrams_probs:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(ngrams_probs[prefix],key=ngrams_probs[prefix].get)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text</span>(<span class="params">prefix,n</span>):</span><br><span class="line">    ngram_counts = count_ngrams(corpus, n)</span><br><span class="line">    ngrams_probs = ngram_probabilities(ngram_counts)</span><br><span class="line">    <span class="keyword">for</span> prefixs, probs <span class="keyword">in</span> ngrams_probs.items():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot;&quot;</span>.join(prefixs), <span class="built_in">dict</span>(probs)))</span><br><span class="line"></span><br><span class="line">    text = prefix</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        ngrams_prefix = text[-(n-<span class="number">1</span>):]</span><br><span class="line">        next_token = generate_next_token(ngrams_prefix,ngrams_probs)</span><br><span class="line">        <span class="keyword">if</span> next_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            text = text+next_token</span><br><span class="line">    <span class="keyword">return</span> text</span><br></pre></td></tr></table></figure>
<p><strong>缺点：无法捕捉距离较远文本的信息</strong></p>
<h2 id="bag-of-words"><a class="markdownIt-Anchor" href="#bag-of-words"></a> Bag of Words</h2>
<p>词袋模型是一种将文本转换为向量的方式，其只关注词语出现的次数而不关注词语的上下文关系，也就是不关心词语的顺序。</p>
<p>举个例子，对于一个句子<code>i love you very very much</code>，其通过词袋模型编码后的结果可能为<code>[1,1,1,2,1,0,0]</code>,这代表整个词语库共7种词语，这个句子包含了5种词语，词语的频数也有体现。</p>
<p>通常可以用于比较句子之间的相关性</p>
<p>具体实现：<br />
构建一个词语库，统计到共21个词语：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">corpus=[<span class="string">&#x27;我特别特别喜欢看电影&#x27;</span>,<span class="string">&#x27;这部电影真的是很好看的电影&#x27;</span>,<span class="string">&#x27;今天天气真好是难得的好天气&#x27;</span>,<span class="string">&#x27;我今天去看了一部电影&#x27;</span>,<span class="string">&#x27;电影院的电影都很好看&#x27;</span>]</span><br><span class="line">tokens = [<span class="built_in">list</span>(jieba.cut(i)) <span class="keyword">for</span> i <span class="keyword">in</span> corpus]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_words_table</span>(<span class="params">tokens</span>):</span><br><span class="line">    words_dict = &#123;&#125;</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> tokens:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> words_dict:</span><br><span class="line">                words_dict[word] = index</span><br><span class="line">                index+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> words_dict</span><br><span class="line"></span><br><span class="line">words_dict = create_words_table(tokens)</span><br><span class="line"><span class="built_in">print</span>(words_dict)</span><br><span class="line"><span class="comment">#  &#123;&#x27;我&#x27;: 0, &#x27;特别&#x27;: 1, &#x27;喜欢&#x27;: 2, &#x27;看&#x27;: 3, &#x27;电影&#x27;: 4, &#x27;这部&#x27;: 5, &#x27;真的&#x27;: 6, &#x27;是&#x27;: 7, &#x27;很&#x27;: 8, &#x27;好看&#x27;: 9, &#x27;的&#x27;: 10, &#x27;今天天气&#x27;: 11, &#x27;真好&#x27;: 12, &#x27;难得&#x27;: 13, &#x27;好&#x27;: 14, &#x27;天气&#x27;: 15, &#x27;今天&#x27;: 16, &#x27;去&#x27;: 17, &#x27;了&#x27;: 18, &#x27;一部&#x27;: 19, &#x27;电影院&#x27;: 20, &#x27;都&#x27;: 21&#125;               </span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>对每个句子进行向量化，具体方法为统计句子中出现了哪些词语且其频数是多少，在长度为21的向量中进行标注：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_words_bag</span>(<span class="params">words_dict,tokens</span>):</span><br><span class="line">    words_bag = []</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> tokens:</span><br><span class="line">        sentence_vector = [<span class="number">0</span>]*<span class="built_in">len</span>(words_dict)</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">            sentence_vector[words_dict[word]]+=<span class="number">1</span></span><br><span class="line">        words_bag.append(sentence_vector)</span><br><span class="line">    <span class="keyword">return</span> words_bag</span><br><span class="line"></span><br><span class="line">words_bag = create_words_bag(words_dict,tokens)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="built_in">print</span>(np.matrix(words_bag))</span><br><span class="line"><span class="comment"># [[1 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</span></span><br><span class="line"><span class="comment">#  [0 0 0 0 2 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]</span></span><br><span class="line"><span class="comment">#  [0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0]</span></span><br><span class="line"><span class="comment">#  [1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0]</span></span><br><span class="line"><span class="comment">#  [0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1]]</span></span><br></pre></td></tr></table></figure>
<p>计算句子之间的相关性，使用余弦相似度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">vec1,vec2</span>):</span><br><span class="line">    <span class="keyword">return</span> np.dot(vec1,vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">similarity_matrix</span>(<span class="params">words_bag</span>):</span><br><span class="line">    similarity_matrix = np.zeros((<span class="built_in">len</span>(words_bag),<span class="built_in">len</span>(words_bag)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(words_bag)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(words_bag)):</span><br><span class="line">            similarity_matrix[i][j] = cosine_similarity(words_bag[i],words_bag[j])</span><br><span class="line">    <span class="keyword">return</span> similarity_matrix</span><br><span class="line"></span><br><span class="line">similarity_matrix = similarity_matrix(words_bag)</span><br><span class="line"><span class="built_in">print</span>(similarity_matrix)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">fig,ax = plt.subplots()</span><br><span class="line">cax = ax.matshow(similarity_matrix,cmap = plt.cm.Blues)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<!-- ![](https://gitee.com/leeMX111/images_for_markdown/raw/master/Figure_1.png) -->
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/Figure_1.png" height="400px" />
<p><strong>缺点：对于较大的词语库会造成高稀疏表示，且不关注词语的顺序，会损失部分语义信息</strong></p>
<h1 id="chapter2word2vec"><a class="markdownIt-Anchor" href="#chapter2word2vec"></a> Chapter2：Word2Vec</h1>
<p><strong>词语向量化</strong>的一种重要方法，对比与one-hot方法，word2vec可以体现词语之间的相互关系，为后续的语义理解提供了基础。</p>
<p>Word2Vec的基础思想为构造一个<strong>神经网络</strong>，通过一些nlp任务（例如通过周围的词语得到中间词）训练这个神经网络，而我们真正需要的是这个神经网络的<strong>隐藏层</strong>，其可以将输入词语（可以是one-hot编码）映射到一个n维的向量，这个向量是<strong>非稀疏</strong>的，且经过前期的训练，这个向量可以很好的反应这个词语的语义信息。</p>
<p>Word2Vec训练时，一般会有两个NLP任务，既<strong>Skip-Gram</strong>和<strong>CBOW</strong></p>
<ul>
<li><strong>Skip-Gram</strong>：使用中间词预测周边其他词</li>
<li><strong>CBOW</strong>：使用周边其他词预测中间词</li>
</ul>
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/v2-35339b4e3efc29326bad70728e2f469c_1440w.png" height="500px" />
<p>tip：从实现来看，上图中的sum应该改为mean才对</p>
<p>训练完之后，我们并不需要整个模型，而只需要<strong>中间层</strong>的参数作为<strong>词语向量化查询表</strong>，也就是上图中两个方法的中间层。</p>
<h2 id="skip-gram"><a class="markdownIt-Anchor" href="#skip-gram"></a> Skip-Gram</h2>
<p>以Skip-Gram为例，<strong>在实现中并非同时生成周边其他词，而是训练n次，每次生成一个词</strong>，例如对于“我爱你”这句话，“爱”的周边词为“我”和“你”，在训练时则训练两次，分别为<code>“爱”-&gt;“我”</code>和<code>“爱”-&gt;“你”</code>，这也解释了下图中从<code>hidden layer</code>到<code>output layer</code>时是使用一样参数的原因。</p>
<p>最终我们只需要中间层参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>V</mi><mo>×</mo><mi>N</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{V×N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>作为词语向量化表，表示词语库中共有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>个词语，将每个词语向量化为长度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>的向量。</p>
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/联想截图_20250319222737.png" height="700px" />
<p>实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sentences = [<span class="string">&#x27;kate is teacher&#x27;</span>,<span class="string">&#x27;mazong is boss&#x27;</span>,<span class="string">&#x27;niuzong is boss&#x27;</span>,<span class="string">&#x27;xiaobing is student&#x27;</span>,<span class="string">&#x27;xiaoxue is student&#x27;</span>]</span><br><span class="line">tokens = [i.split(<span class="string">&#x27; &#x27;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> sentences]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_words_table</span>(<span class="params">tokens</span>):</span><br><span class="line">    words_dict = &#123;&#125;</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> tokens:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> words_dict:</span><br><span class="line">                words_dict[word] = index</span><br><span class="line">                index+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> words_dict</span><br><span class="line">words_dict = create_words_table(tokens)</span><br><span class="line"><span class="built_in">print</span>(words_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#123;&#x27;kate&#x27;: 0, &#x27;is&#x27;: 1, &#x27;teacher&#x27;: 2, &#x27;mazong&#x27;: 3, &#x27;boss&#x27;: 4, &#x27;niuzong&#x27;: 5, &#x27;xiaobing&#x27;: 6, &#x27;student&#x27;: 7, &#x27;xiaoxue&#x27;: 8&#125;</span></span><br></pre></td></tr></table></figure>
<p>构建skip-gram的数据集，此处的<code>windowsize</code>表示周围文本的长度，当其值为2时，表示中心词只能预测周围距离为1的词语，例如“kate”为中心词时，其周围词只有“is”</p>
<p>得到的数据集为多个数组，每个数组的第一个词为中心词，既输入，第二个词为周围词，既输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_skipgram_dataset</span>(<span class="params">token,windowsize = <span class="number">2</span></span>):</span><br><span class="line">    dataset=[]</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> token:</span><br><span class="line">        <span class="keyword">for</span> word_index,word <span class="keyword">in</span> <span class="built_in">enumerate</span>(sentence):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(-windowsize+<span class="number">1</span>,windowsize):</span><br><span class="line">                <span class="keyword">if</span> i&lt;<span class="number">0</span> <span class="keyword">and</span> word_index+i&gt;=<span class="number">0</span>:</span><br><span class="line">                    dataset.append([word,sentence[word_index+i]])</span><br><span class="line">                <span class="keyword">elif</span> i&gt;<span class="number">0</span> <span class="keyword">and</span> word_index+i&lt;=<span class="built_in">len</span>(sentence)-<span class="number">1</span>:</span><br><span class="line">                    dataset.append([word,sentence[word_index+i]])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line">dataset = create_skipgram_dataset(tokens)</span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br><span class="line"><span class="comment"># [[&#x27;kate&#x27;, &#x27;is&#x27;], [&#x27;is&#x27;, &#x27;kate&#x27;], [&#x27;is&#x27;, &#x27;teacher&#x27;], [&#x27;teacher&#x27;, &#x27;is&#x27;], [&#x27;mazong&#x27;, &#x27;is&#x27;], [&#x27;is&#x27;, &#x27;mazong&#x27;], [&#x27;is&#x27;, &#x27;boss&#x27;],</span></span><br><span class="line"><span class="comment"># [&#x27;boss&#x27;, &#x27;is&#x27;], [&#x27;niuzong&#x27;, &#x27;is&#x27;], [&#x27;is&#x27;, &#x27;niuzong&#x27;], [&#x27;is&#x27;, &#x27;boss&#x27;], [&#x27;boss&#x27;, &#x27;is&#x27;], [&#x27;xiaobing&#x27;, &#x27;is&#x27;], [&#x27;is&#x27;, &#x27;xiaobing&#x27;],</span></span><br><span class="line"><span class="comment"># [&#x27;is&#x27;, &#x27;student&#x27;], [&#x27;student&#x27;, &#x27;is&#x27;], [&#x27;xiaoxue&#x27;, &#x27;is&#x27;], [&#x27;is&#x27;, &#x27;xiaoxue&#x27;], [&#x27;is&#x27;, &#x27;student&#x27;], [&#x27;student&#x27;, &#x27;is&#x27;]]</span></span><br></pre></td></tr></table></figure>
<p>将上面的训练集中的输入变为<strong>one-hot编码</strong>，这样才能输入神经网络进行训练，而输出不需要是因为在计算误差时，使用<code>CrossEntropyLoss</code>函数会自动进行one-hot编码以计算误差值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one_hot_encoding</span>(<span class="params">word,words_dict</span>):</span><br><span class="line">    tensor = torch.zeros(<span class="built_in">len</span>(words_dict))</span><br><span class="line">    tensor[words_dict[word]] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line">    </span><br><span class="line">skip_gram_data = [[one_hot_encoding(context,words_dict),words_dict[output]] <span class="keyword">for</span>[context,output] <span class="keyword">in</span> dataset]</span><br><span class="line"><span class="built_in">print</span>(skip_gram_data)</span><br><span class="line"><span class="comment"># [[tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.]), 1], [tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.]), 0], [tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.]), 2],</span></span><br><span class="line"><span class="comment"># [tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.]), 1], [tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.]), 1], [tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.]), 3], </span></span><br><span class="line"><span class="comment"># [tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.]), 4], [tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.]), 1], [tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.]), 1], </span></span><br><span class="line"><span class="comment"># [tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.]), 5], [tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.]), 4], [tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.]), 1], </span></span><br><span class="line"><span class="comment"># [tensor([0., 0., 0., 0., 0., 0., 1., 0., 0.]), 1], [tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.]), 6], [tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.]), 7], </span></span><br><span class="line"><span class="comment"># [tensor([0., 0., 0., 0., 0., 0., 0., 1., 0.]), 1], [tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]), 1], [tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.]), 8], </span></span><br><span class="line"><span class="comment"># [tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.]), 7], [tensor([0., 0., 0., 0., 0., 0., 0., 1., 0.]), 1]]</span></span><br></pre></td></tr></table></figure>
<p>定义神经网络模型，此处定义了两层Linear层：</p>
<ul>
<li>Linear1：input_2_hidden，输入大小为词表中词语个数，也就是输入词语进行one-hot编码后的长度，输出为自定义的隐藏层大小</li>
<li>Linear2：hidden_2_output，输入为隐藏层大小，输入长度也是one-hot编码后的长度，表示各个词语的输出概率</li>
</ul>
<p>这里不需要定义softmax层，因为误差函数会自动进行softmax：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SkipGram</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, voc_size,embedding_size</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(SkipGram,<span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.input_2_hidden =nn.Linear(voc_size,embedding_size,bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># self.input_2_hidden = nn.Embedding(voc_size, embedding_size)</span></span><br><span class="line">        <span class="variable language_">self</span>.hidden_2_output = nn.Linear(embedding_size,voc_size,bias=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,X</span>):</span><br><span class="line">        hidden = <span class="variable language_">self</span>.input_2_hidden(X)</span><br><span class="line">        output = <span class="variable language_">self</span>.hidden_2_output(hidden)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">skip_gram_model = SkipGram(voc_size=<span class="built_in">len</span>(skip_gram_data[<span class="number">0</span>][<span class="number">0</span>]),embedding_size=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(skip_gram_model)</span><br><span class="line"><span class="comment"># SkipGram(</span></span><br><span class="line"><span class="comment">#   (input_2_hidden): Linear(in_features=9, out_features=2, bias=False)</span></span><br><span class="line"><span class="comment">#   (hidden_2_output): Linear(in_features=2, out_features=9, bias=False)</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure>
<p>这里的<code>input_2_hidden</code>可看成一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">V×N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>的矩阵，输入是一个长度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>的向量，那么实际上这一层做的操作即为矩阵乘法，这个向量是一个one-hot向量，矩阵乘法实际上是对这个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">V×N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>矩阵的查找（选出one-hot向量中为1的元素对应的行）</p>
<p>那么在实现时可以使用<code>nn.Embedding</code>代替线性层，这个层的本质是一个<code>查找表</code>，输入大小不需要改变，在输入时便不需要进行one-hot编码，直接输入词语对应的索引进行查找即可，简化运算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, voc_size,embedding_size</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">super</span>(SkipGram,<span class="variable language_">self</span>).__init__()</span><br><span class="line">    <span class="comment"># self.input_2_hidden =nn.Linear(voc_size,embedding_size,bias=False)</span></span><br><span class="line">    <span class="variable language_">self</span>.input_2_hidden = nn.Embedding(voc_size, embedding_size)</span><br><span class="line">    <span class="variable language_">self</span>.hidden_2_output = nn.Linear(embedding_size,voc_size,bias=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>模型训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">1000</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 定义交叉熵损失函数</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">optimizer = optim.SGD(params=skip_gram_model.parameters(),lr = lr)</span><br><span class="line">loss_values = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    loss_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> [one_hot_input,target] <span class="keyword">in</span> skip_gram_data:</span><br><span class="line">        X = one_hot_input.<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>) </span><br><span class="line">        <span class="comment"># tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])</span></span><br><span class="line">        y_true = torch.tensor([target], dtype=torch.long)</span><br><span class="line">        <span class="comment"># tensor([8])</span></span><br><span class="line">        y_pred = skip_gram_model(X)</span><br><span class="line">        <span class="comment"># tensor([[-0.1776, -0.1084, 0.0309, 0.0138, 0.2688, -0.0034, -0.2324, 0.1325,</span></span><br><span class="line">        <span class="comment">#          0.1417]], grad_fn= &lt; MmBackward0 &gt;)</span></span><br><span class="line">        loss = criterion(y_pred,y_true)</span><br><span class="line">        loss_sum +=loss.item()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>: <span class="comment"># 输出每 100 轮的损失，并记录损失</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss_sum/<span class="built_in">len</span>(skip_gram_data)&#125;</span>&quot;</span>)</span><br><span class="line">      loss_values.append(loss_sum / <span class="built_in">len</span>(skip_gram_data))</span><br><span class="line">      </span><br><span class="line"><span class="comment">#使用nn.Embedding的训练过程，直接输入索引即可</span></span><br><span class="line"><span class="comment">#for epoch in range(epochs):</span></span><br><span class="line"><span class="comment">#     loss_sum = 0</span></span><br><span class="line"><span class="comment">#     for [center_word,target] in dataset:</span></span><br><span class="line"><span class="comment">#         X = torch.tensor(words_dict[center_word],dtype=torch.long).unsqueeze(0)</span></span><br><span class="line"><span class="comment">#         y_true = torch.tensor([words_dict[target]], dtype=torch.long) # 将周围词转换为索引值</span></span><br><span class="line"><span class="comment">#         y_pred = skip_gram_model(X)</span></span><br><span class="line"><span class="comment">#         loss = criterion(y_pred,y_true)</span></span><br><span class="line"><span class="comment">#         loss_sum +=loss.item()</span></span><br><span class="line"><span class="comment">#         optimizer.zero_grad()</span></span><br><span class="line"><span class="comment">#         loss.backward()</span></span><br><span class="line"><span class="comment">#         optimizer.step()</span></span><br><span class="line"><span class="comment">#     if (epoch+1) % 100 == 0: # 输出每 100 轮的损失，并记录损失</span></span><br><span class="line"><span class="comment">#       print(f&quot;Epoch: &#123;epoch+1&#125;, Loss: &#123;loss_sum/len(skip_gram_data)&#125;&quot;)</span></span><br><span class="line"><span class="comment">#       loss_values.append(loss_sum / len(skip_gram_data))</span></span><br><span class="line">   </span><br></pre></td></tr></table></figure>
<!-- ![](https://gitee.com/leeMX111/images_for_markdown/raw/master/skip_gram.png) -->
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/skip_gram.png" height="450px" />
<p>训练完之后，我们需要的是隐藏层的参数，即<code>skip_gram_model.input_2_hidden.weight</code>，这是一个9×2的矩阵，表示将9个词语变为了长度为2的向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 导入 matplotlib</span></span><br><span class="line"><span class="comment"># 绘制二维词向量图</span></span><br><span class="line">plt.rcParams[<span class="string">&quot;font.family&quot;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># 用来设定字体样式</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># 用来设定无衬线字体样式</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span> <span class="comment"># 用来正常显示负号</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, epochs//<span class="number">100</span> + <span class="number">1</span>), loss_values) <span class="comment"># 绘图</span></span><br><span class="line">plt.title(<span class="string">&#x27; 训练损失曲线 &#x27;</span>) <span class="comment"># 图题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27; 轮次 &#x27;</span>) <span class="comment"># X 轴 Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27; 损失 &#x27;</span>) <span class="comment"># Y 轴 Label</span></span><br><span class="line">plt.show() <span class="comment"># 显示图</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(skip_gram_model.input_2_hidden.weight)</span><br><span class="line"><span class="comment"># tensor([[-0.4476,  0.6655, -0.7532, -0.6657, -1.0122, -0.4196, -0.6324, -0.5355,</span></span><br><span class="line"><span class="comment">#          -0.4030],</span></span><br><span class="line"><span class="comment">#         [ 1.0320, -0.5229,  0.8602,  0.9287,  1.0592,  0.9380,  0.9937,  1.3663,</span></span><br><span class="line"><span class="comment">#           0.9899]], requires_grad=True)</span></span><br></pre></td></tr></table></figure>
<p>可以将每个词语的向量表示汇出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words_dict:</span><br><span class="line">    <span class="built_in">print</span>(word)</span><br><span class="line">    <span class="built_in">print</span>(skip_gram_model.input_2_hidden.weight[:,words_dict[word]].detach().numpy())</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots() </span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words_dict:</span><br><span class="line">    vec = skip_gram_model.input_2_hidden.weight[:,words_dict[word]].detach().numpy()</span><br><span class="line">    ax.scatter(vec[<span class="number">0</span>], vec[<span class="number">1</span>]) <span class="comment"># 在图中绘制嵌入向量的点</span></span><br><span class="line">    ax.annotate(word, (vec[<span class="number">0</span>], vec[<span class="number">1</span>]), fontsize=<span class="number">12</span>) <span class="comment"># 点旁添加单词标签</span></span><br><span class="line">plt.title(<span class="string">&#x27; 二维词嵌入 &#x27;</span>) <span class="comment"># 图题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27; 向量维度 1&#x27;</span>) <span class="comment"># X 轴 Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27; 向量维度 2&#x27;</span>) <span class="comment"># Y 轴 Label</span></span><br><span class="line">plt.show() <span class="comment"># 显示图</span></span><br></pre></td></tr></table></figure>
<!-- ![](https://gitee.com/leeMX111/images_for_markdown/raw/master/skip_gram2.png) -->
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/skip_gram2.png" height="450px" />
<h2 id="cbow"><a class="markdownIt-Anchor" href="#cbow"></a> CBOW</h2>
<p>CBOW是用周围词预测中间词，这里需要注意的是，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>个中间词是同时输入的，那么可以把输入矩阵看做<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mrow><mi>C</mi><mo>×</mo><mi>V</mi></mrow></msub></mrow><annotation encoding="application/x-tex">I_{C×V}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>，隐藏层矩阵为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>V</mi><mo>×</mo><mi>N</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{V×N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>，则输出大小为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">C×N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>，这里需要做一次<strong>平均操作</strong>，使得输出大小变为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">1×N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>以输入后续的线性层。</p>
<!-- ![](https://gitee.com/leeMX111/images_for_markdown/raw/master/1231.png) -->
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/1231.png" height="700px" />
<p>实现过程与skip_gram类似，只需要进行部分调整：<br />
在生成数据集时，需要实现多对一的数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_CBOW_dataset</span>(<span class="params">token, windowsize=<span class="number">2</span></span>):</span><br><span class="line">    dataset = []</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> token:</span><br><span class="line">        <span class="keyword">for</span> word_index, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(sentence):</span><br><span class="line">            context = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(-windowsize, windowsize+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> (i &lt; <span class="number">0</span> <span class="keyword">and</span> word_index + i &gt;= <span class="number">0</span>) <span class="keyword">or</span> (i &gt; <span class="number">0</span> <span class="keyword">and</span> word_index + i &lt;= <span class="built_in">len</span>(sentence) - <span class="number">1</span>):</span><br><span class="line">                    context.append(sentence[word_index + i])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">            dataset.append([word,context])</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = create_CBOW_dataset(tokens)</span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br><span class="line"><span class="comment"># [[&#x27;kate&#x27;, [&#x27;is&#x27;, &#x27;teacher&#x27;]], [&#x27;is&#x27;, [&#x27;kate&#x27;, &#x27;teacher&#x27;]], [&#x27;teacher&#x27;, [&#x27;kate&#x27;, &#x27;is&#x27;]], [&#x27;mazong&#x27;, [&#x27;is&#x27;, &#x27;boss&#x27;]], [&#x27;is&#x27;, [&#x27;mazong&#x27;, &#x27;boss&#x27;]], </span></span><br><span class="line"><span class="comment">#  [&#x27;boss&#x27;, [&#x27;mazong&#x27;, &#x27;is&#x27;]], [&#x27;niuzong&#x27;, [&#x27;is&#x27;, &#x27;boss&#x27;]], [&#x27;is&#x27;, [&#x27;niuzong&#x27;, &#x27;boss&#x27;]], [&#x27;boss&#x27;, [&#x27;niuzong&#x27;, &#x27;is&#x27;]], [&#x27;xiaobing&#x27;, [&#x27;is&#x27;, &#x27;student&#x27;]], </span></span><br><span class="line"><span class="comment">#  [&#x27;is&#x27;, [&#x27;xiaobing&#x27;, &#x27;student&#x27;]], [&#x27;student&#x27;, [&#x27;xiaobing&#x27;, &#x27;is&#x27;]], [&#x27;xiaoxue&#x27;, [&#x27;is&#x27;, &#x27;student&#x27;]], [&#x27;is&#x27;, [&#x27;xiaoxue&#x27;, &#x27;student&#x27;]], [&#x27;student&#x27;, [&#x27;xiaoxue&#x27;, &#x27;is&#x27;]]]</span></span><br></pre></td></tr></table></figure>
<p>将数据集进行one-hot编码，且使用<code>torch.stack</code>将多个输入进行合并，与<code>torch.cat</code>的区别在于<code>torch.stack</code>会新增一个维度来进行拼接，这使得它在构建具有批次维度等场景下非常有用，比如在深度学习中构建批次数据时，将多个样本张量堆叠起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_CBOW_data</span>(<span class="params">dataset</span>):</span><br><span class="line">    CBOW_data = []</span><br><span class="line">    <span class="keyword">for</span> [center_word,context] <span class="keyword">in</span> dataset:</span><br><span class="line">        context_one_hot = torch.stack([one_hot_encoding(word,words_dict) <span class="keyword">for</span> word <span class="keyword">in</span> context]).<span class="built_in">float</span>()</span><br><span class="line">        CBOW_data.append([torch.tensor(words_dict[center_word],dtype=torch.long),context_one_hot])</span><br><span class="line">    <span class="keyword">return</span> CBOW_data</span><br><span class="line"><span class="comment"># print(skip_gram_data)</span></span><br><span class="line">CBOW_data = create_CBOW_data(dataset)</span><br><span class="line"><span class="comment"># [[tensor(0), tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.],[0., 0., 1., 0., 0., 0., 0., 0., 0.]])], </span></span><br><span class="line"><span class="comment">#  [tensor(1), tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],[0., 0., 1., 0., 0., 0., 0., 0., 0.]])],</span></span><br></pre></td></tr></table></figure>
<p>定义网络结构，<code>input_2_hidden</code>层将多个输入同时计算，得到<code>2×embedding_size</code>的结果，然后中间加入了一个<code>mean</code>层，将输出变为1维，值得注意的是，网络的输入输出大小没变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CBOW</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, voc_size, embedding_size</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(CBOW, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.input_2_hidden = nn.Linear(voc_size, embedding_size, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.hidden_2_output = nn.Linear(embedding_size, voc_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        embedding = <span class="variable language_">self</span>.input_2_hidden(X)</span><br><span class="line">        <span class="comment"># tensor([[-0.2415, 0.2611],</span></span><br><span class="line">        <span class="comment">#         [0.2320, -0.3655]], grad_fn= &lt; MmBackward0 &gt;)</span></span><br><span class="line">        hidden = torch.mean(embedding,dim=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># tensor([-0.0047, -0.0522], grad_fn= &lt; MeanBackward1 &gt;)</span></span><br><span class="line">        output = <span class="variable language_">self</span>.hidden_2_output(hidden.unsqueeze(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p><strong>Word2Vec的局限性</strong></p>
<ul>
<li>词向量是固定的，无法处理“一词多义”的情况</li>
<li>无法处理未知词汇</li>
</ul>
<h1 id="chapter3nplm模型"><a class="markdownIt-Anchor" href="#chapter3nplm模型"></a> Chapter3：NPLM模型</h1>
<p>在NPLM（Neural Probabilistic Language Model）模型中</p>
<p>在</p>
<!-- ![](https://gitee.com/leeMX111/images_for_markdown/raw/master/nplm.png) -->
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/nplm.png" height="700px" />
<p><strong>因此，神经概率语言模型</strong>依然是一个概率语言模型，它是通过<strong>神经网络</strong>来计算<strong>概率语言模型</strong>中的每个参数。</p>
<p>相比于N-gram语言模型，<strong>神经概率语言模型</strong>有以下优点：</p>
<ol>
<li><strong>单词之间的相似性可以通过词向量来体现</strong>(相比神经语言模型本身，作为其副产品的词向量反而是更大的惊喜)</li>
<li><strong>自带平滑处理</strong></li>
</ol>
<blockquote>
<p>在某种程度上，可以说Word2Vec和NPLM在一些方面有相似之处，但它们在设计和应用上仍有一些显著的区别。以下是它们的一些相似点和差异：</p>
<p><strong>相似点</strong>：</p>
<ol>
<li><strong>基于神经网络</strong>：Word2Vec和NPLM都是基于神经网络的模型，用于学习词向量和处理自然语言文本。</li>
<li><strong>词嵌入</strong>：两者都旨在将单词映射到连续向量空间中，以便捕捉单词之间的语义关系。</li>
</ol>
<p><strong>差异点</strong>：</p>
<ol>
<li><strong>预测任务</strong>：Word2Vec的预测任务主要是通过上下文单词预测目标单词（Skip-gram）或通过目标单词预测上下文单词（CBOW），而NPLM是一种神经网络语言模型，主要任务是预测下一个单词出现的概率。</li>
<li><strong>上下文考虑</strong>：NPLM在训练时考虑了前面n-1个单词的上下文信息，以便更好地捕捉长距离依赖关系，而Word2Vec主要关注词与词之间的语义关系，对于长距离依赖的处理不如NPLM。</li>
<li><strong>应用领域</strong>：由于任务和设计的差异，Word2Vec通常用于词向量学习、词义相似度计算等任务，而NPLM更适用于语言建模等需要考虑长距离依赖的任务。</li>
</ol>
</blockquote>
<p>实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentences = [<span class="string">&quot;我 非常 喜欢 玩具&quot;</span>, <span class="string">&quot;我 爱 爸爸&quot;</span>, <span class="string">&quot;我 讨厌 挨打&quot;</span>]</span><br><span class="line">words_list = <span class="built_in">list</span>(<span class="built_in">set</span>(<span class="string">&quot; &quot;</span>.join(sentences).split()))</span><br><span class="line">words_dict =  &#123;word:index <span class="keyword">for</span> index,word <span class="keyword">in</span> <span class="built_in">enumerate</span>(words_list)&#125;</span><br><span class="line"><span class="built_in">print</span>(words_dict)</span><br><span class="line"><span class="comment"># &#123;&#x27;我&#x27;: 0, &#x27;喜欢&#x27;: 1, &#x27;爱&#x27;: 2, &#x27;爸爸&#x27;: 3, &#x27;讨厌&#x27;: 4, &#x27;挨打&#x27;: 5, &#x27;玩具&#x27;: 6&#125;</span></span><br><span class="line">idx_to_word = &#123;idx: word <span class="keyword">for</span> idx, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(words_dict)&#125;</span><br></pre></td></tr></table></figure>
<p>构建训练集，设置<code>make_batch</code>生成一个batch训练集，在这里一个batch包含两份数据，<code>n_step</code>表示一次性输入模型的token数量，在这里设置为2，也就是说用前面2个token预测下一个token</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="comment"># 导入 PyTorch 库</span></span><br><span class="line"><span class="keyword">import</span> random <span class="comment"># 导入 random 库</span></span><br><span class="line">batch_size = <span class="number">2</span> <span class="comment"># 每批数据的大小</span></span><br><span class="line">n_step = <span class="number">2</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_batch</span>(<span class="params">n_step</span>):</span><br><span class="line">    input_batch = []  <span class="comment"># 定义输入批处理列表</span></span><br><span class="line">    target_batch = []  <span class="comment"># 定义目标批处理列表</span></span><br><span class="line">    selected_sentences = random.sample(sentences, batch_size) <span class="comment"># 随机选择句子</span></span><br><span class="line">    <span class="keyword">for</span> sen <span class="keyword">in</span> selected_sentences:  <span class="comment"># 遍历每个句子</span></span><br><span class="line">        word = sen.split()  <span class="comment"># 用空格将句子分隔成多个词</span></span><br><span class="line">        <span class="comment"># 将除最后一个词以外的前面n_step个词的索引作为输入</span></span><br><span class="line">        <span class="built_in">input</span> = [words_dict[n] <span class="keyword">for</span> n <span class="keyword">in</span> word[-n_step-<span class="number">1</span>:-<span class="number">1</span>]]  <span class="comment"># 创建输入数据</span></span><br><span class="line">        <span class="comment"># 将最后一个词的索引作为目标</span></span><br><span class="line">        target = words_dict[word[-<span class="number">1</span>]]  <span class="comment"># 创建目标数据</span></span><br><span class="line">        input_batch.append(<span class="built_in">input</span>)  <span class="comment"># 将输入添加到输入批处理列表</span></span><br><span class="line">        target_batch.append(target)  <span class="comment"># 将目标添加到目标批处理列表</span></span><br><span class="line">    input_batch = torch.LongTensor(input_batch) <span class="comment"># 将输入数据转换为张量</span></span><br><span class="line">    target_batch = torch.LongTensor(target_batch) <span class="comment"># 将目标数据转换为张量</span></span><br><span class="line">    <span class="keyword">return</span> input_batch, target_batch  <span class="comment"># 返回输入批处理和目标批处理数据</span></span><br><span class="line">input_batch, target_batch = make_batch(n_step) <span class="comment"># 生成批处理数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 输入批处理数据：&quot;</span>,input_batch)  <span class="comment"># 打印输入批处理数据</span></span><br><span class="line"><span class="comment"># 将输入批处理数据中的每个索引值转换为对应的原始词</span></span><br><span class="line">input_words = []</span><br><span class="line"><span class="keyword">for</span> input_idx <span class="keyword">in</span> input_batch:</span><br><span class="line">    input_words.append([idx_to_word[idx.item()] <span class="keyword">for</span> idx <span class="keyword">in</span> input_idx])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 输入批处理数据对应的原始词：&quot;</span>,input_words)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 目标批处理数据：&quot;</span>,target_batch) <span class="comment"># 打印目标批处理数据</span></span><br><span class="line"><span class="comment"># 将目标批处理数据中的每个索引值转换为对应的原始词</span></span><br><span class="line">target_words = [idx_to_word[idx.item()] <span class="keyword">for</span> idx <span class="keyword">in</span> target_batch]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 目标批处理数据对应的原始词：&quot;</span>,target_words)</span><br><span class="line"><span class="comment"># #</span></span><br><span class="line"><span class="comment"># 输入批处理数据： tensor([[1, 5],</span></span><br><span class="line"><span class="comment">#                         [6, 0]])</span></span><br><span class="line"><span class="comment"># 输入批处理数据对应的原始词： [[&#x27;我&#x27;, &#x27;爱&#x27;], [&#x27;非常&#x27;, &#x27;喜欢&#x27;]]</span></span><br><span class="line"><span class="comment"># 目标批处理数据： tensor([7, 3])</span></span><br><span class="line"><span class="comment"># 目标批处理数据对应的原始词： [&#x27;爸爸&#x27;, &#x27;玩具&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>构建模型，这里的重点是第一个线性层的输入大小为<code>n_step * embedding_size</code>,也就是将<code>n_step</code>个输入进行<code>embedding</code>后拼接起来再输入线性层（区别于CBOW，其方法为多个输入编码后取平均）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn <span class="comment"># 导入神经网络模块</span></span><br><span class="line"><span class="comment"># 定义神经概率语言模型（NPLM）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NPLM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NPLM, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.C = nn.Embedding(voc_size, embedding_size) <span class="comment"># 定义一个词嵌入层</span></span><br><span class="line">        <span class="comment"># 第一个线性层，其输入大小为 n_step * embedding_size，输出大小为 n_hidden</span></span><br><span class="line">        <span class="variable language_">self</span>.linear1 = nn.Linear(n_step * embedding_size, n_hidden)</span><br><span class="line">        <span class="comment"># 第二个线性层，其输入大小为 n_hidden，输出大小为 voc_size，即词汇表大小</span></span><br><span class="line">        <span class="variable language_">self</span>.linear2 = nn.Linear(n_hidden, voc_size)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):  <span class="comment"># 定义前向传播过程</span></span><br><span class="line">        <span class="comment"># 输入数据 X 张量的形状为 [batch_size, n_step]</span></span><br><span class="line">        X = <span class="variable language_">self</span>.C(X)  <span class="comment"># 将 X 通过词嵌入层，形状变为 [batch_size, n_step, embedding_size]</span></span><br><span class="line">        X = X.view(-<span class="number">1</span>, n_step * embedding_size) <span class="comment"># 形状变为 [batch_size, n_step * embedding_size]</span></span><br><span class="line">        <span class="comment"># 通过第一个线性层并应用 ReLU 激活函数</span></span><br><span class="line">        hidden = torch.tanh(<span class="variable language_">self</span>.linear1(X)) <span class="comment"># hidden 张量形状为 [batch_size, n_hidden]</span></span><br><span class="line">        <span class="comment"># 通过第二个线性层得到输出</span></span><br><span class="line">        output = <span class="variable language_">self</span>.linear2(hidden) <span class="comment"># output 形状为 [batch_size, voc_size]</span></span><br><span class="line">        <span class="keyword">return</span> output <span class="comment"># 返回输出结果</span></span><br><span class="line"><span class="comment">#</span></span><br></pre></td></tr></table></figure>
<p>模型训练及预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">n_hidden = <span class="number">2</span> <span class="comment"># 隐藏层大小</span></span><br><span class="line">embedding_size = <span class="number">2</span> <span class="comment"># 词嵌入大小</span></span><br><span class="line">voc_size = <span class="built_in">len</span>(words_dict)</span><br><span class="line">model = NPLM() <span class="comment"># 创建神经概率语言模型实例</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; NPLM 模型结构：&#x27;</span>, model) <span class="comment"># 打印模型的结构</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim <span class="comment"># 导入优化器模块</span></span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment"># 定义损失函数为交叉熵损失</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.1</span>) <span class="comment"># 定义优化器为 Adam，学习率为 0.1</span></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5000</span>): <span class="comment"># 设置训练迭代次数</span></span><br><span class="line">   optimizer.zero_grad() <span class="comment"># 清除优化器的梯度</span></span><br><span class="line">   input_batch, target_batch = make_batch(n_step) <span class="comment"># 创建输入和目标批处理数据</span></span><br><span class="line">   output = model(input_batch) <span class="comment"># 将输入数据传入模型，得到输出结果</span></span><br><span class="line">   loss = criterion(output, target_batch) <span class="comment"># 计算损失值</span></span><br><span class="line">   <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">1000</span> == <span class="number">0</span>: <span class="comment"># 每 1000 次迭代，打印损失值</span></span><br><span class="line">     <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (epoch + <span class="number">1</span>), <span class="string">&#x27;cost =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line">   loss.backward() <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">   optimizer.step() <span class="comment"># 更新模型参数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line">input_strs = [[<span class="string">&#x27;我&#x27;</span>, <span class="string">&#x27;讨厌&#x27;</span>], [<span class="string">&#x27;我&#x27;</span>, <span class="string">&#x27;喜欢&#x27;</span>]]  <span class="comment"># 需要预测的输入序列</span></span><br><span class="line"><span class="comment"># 将输入序列转换为对应的索引</span></span><br><span class="line">input_indices = [[words_dict[word] <span class="keyword">for</span> word <span class="keyword">in</span> seq] <span class="keyword">for</span> seq <span class="keyword">in</span> input_strs]</span><br><span class="line"><span class="comment"># 将输入序列的索引转换为张量</span></span><br><span class="line">input_batch = torch.LongTensor(input_indices)</span><br><span class="line"><span class="comment"># 对输入序列进行预测，取输出中概率最大的类别</span></span><br><span class="line">predict = model(input_batch).data.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 将预测结果的索引转换为对应的词</span></span><br><span class="line">predict_strs = [idx_to_word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> predict.squeeze()]</span><br><span class="line"><span class="keyword">for</span> input_seq, pred <span class="keyword">in</span> <span class="built_in">zip</span>(input_strs, predict_strs):</span><br><span class="line">   <span class="built_in">print</span>(input_seq, <span class="string">&#x27;-&gt;&#x27;</span>, pred)  <span class="comment"># 打印输入序列和预测结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [&#x27;我&#x27;, &#x27;讨厌&#x27;] -&gt; 挨打</span></span><br><span class="line"><span class="comment"># [&#x27;我&#x27;, &#x27;喜欢&#x27;] -&gt; 爸爸</span></span><br></pre></td></tr></table></figure>
<h1 id="chapter4seq2seq"><a class="markdownIt-Anchor" href="#chapter4seq2seq"></a> Chapter4：Seq2Seq</h1>
<p>部分引用：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/147310766">https://zhuanlan.zhihu.com/p/147310766</a></p>
<p>Seq2Seq本意为<code>序列—&gt;序列</code>的一种模型，解决的是一些序列转换的问题，例如机器翻译等等，基本思想是<strong>将输入序列编码为一些向量表示，然后再通过解码获奖这些信息转换为输出序列</strong></p>
<p>Seq2Seq一般包含两个部分：</p>
<ul>
<li>Encoder：将输入序列进行编码，映射到一个向量空间中，一般会采用<code>embedding</code>+<code>rnn</code>(或<code>lstm</code>等)，输入有两个：输入序列和初始化的<code>hidden</code></li>
<li>Decoder: 接收编码器的最后的<code>hidden</code>，并将其解码为需要的序列。解码器也有输入序列，在训练时和预测时有不同：</li>
</ul>
<p>在预测时，将编码器的<code>hidden</code>当成解码器的初始隐藏层，并在第一个时间步输入一个<strong>开始信号</strong>，一般为<code>&lt;sos&gt;</code>，然后将上一时刻的输出作为下一时刻的输入，这很好理解，根据上一时刻说了什么推断下一时刻要说什么很合理。</p>
<p><img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/v2-6c73bb4f24b93d8a640fea0ef60d1919_1440w.jpg" alt="" /></p>
<p>但是在训练时不能像测试时一样，在一开始时，模型是混乱的，利用模型的输出，将上一时刻的输出作为下一时刻的输入是没有意义的，模型的进步会非常缓慢，所以需要<strong>教师强制（Teacher Forcing)</strong> 机制</p>
<p>训练时，解码器的输入和期望输出基本一致，但是错开一个时间步。教师强制是一种Seq2Seq在训练时的监督方法，decoder在运行是一步一步输出，可以看成一个生成模型，教师强制指的是在训练时，对其每一步都基于正确的引导，使得其能快速的更新参数</p>
<p>例如在一次训练中，解码器期望输出为<code>I LOVE YOU &lt;eos&gt;</code>,那么其输入为<code>&lt;sos&gt; I LOVE YOU</code>,在第一个时间步，解码器输入<code>&lt;sos&gt;</code>，其期望输出为<code>I</code>，在第二个时间步，输入为<code>I</code>(尽管在第一个时间步的实际输出可能不是<code>I</code>)，期望输出为<code>LOVE</code>,以此类推，<strong>就好像每一步都有一个老师拿着上一时刻的正确答案引导你下一时刻做出正确的选择。</strong></p>
<p><img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/222.jpg" alt="" /></p>
<p><strong>具体实现</strong></p>
<p>先准备数据，每一个数据包括<strong>编码器输入</strong>，<strong>解码器输入</strong>和<strong>期望解码器输出</strong>，这里模拟一个中文翻译英文的场景：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">sentences = [</span><br><span class="line">    [<span class="string">&#x27;咖哥 喜欢 小冰&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; KaGe likes XiaoBing&#x27;</span>, <span class="string">&#x27;KaGe likes XiaoBing &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;我 爱 学习 人工智能&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; I love studying AI&#x27;</span>, <span class="string">&#x27;I love studying AI &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;深度学习 改变 世界&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; DL changed the world&#x27;</span>, <span class="string">&#x27;DL changed the world &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;自然 语言 处理 很 强大&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; NLP is so powerful&#x27;</span>, <span class="string">&#x27;NLP is so powerful &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;神经网络 非常 复杂&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; Neural-Nets are complex&#x27;</span>, <span class="string">&#x27;Neural-Nets are complex &lt;eos&gt;&#x27;</span>]]</span><br><span class="line">word_list_cn, word_list_en = [], []  <span class="comment"># 初始化中英文词汇表</span></span><br><span class="line"><span class="comment"># 遍历每一个句子并将单词添加到词汇表中</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> sentences:</span><br><span class="line">    word_list_cn.extend(s[<span class="number">0</span>].split())</span><br><span class="line">    word_list_en.extend(s[<span class="number">1</span>].split())</span><br><span class="line">    word_list_en.extend(s[<span class="number">2</span>].split())</span><br><span class="line"><span class="comment"># 去重，得到没有重复单词的词汇表</span></span><br><span class="line">word_list_cn = <span class="built_in">list</span>(<span class="built_in">set</span>(word_list_cn))</span><br><span class="line">word_list_en = <span class="built_in">list</span>(<span class="built_in">set</span>(word_list_en))</span><br><span class="line"><span class="comment"># 构建单词到索引的映射</span></span><br><span class="line">word2idx_cn = &#123;w: i <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_cn)&#125;</span><br><span class="line">word2idx_en = &#123;w: i <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_en)&#125;</span><br><span class="line"><span class="comment"># 构建索引到单词的映射</span></span><br><span class="line">idx2word_cn = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_cn)&#125;</span><br><span class="line">idx2word_en = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_en)&#125;</span><br><span class="line"><span class="comment"># 计算词汇表的大小</span></span><br><span class="line">voc_size_cn = <span class="built_in">len</span>(word_list_cn)</span><br><span class="line">voc_size_en = <span class="built_in">len</span>(word_list_en)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 句子数量：&quot;</span>, <span class="built_in">len</span>(sentences)) <span class="comment"># 打印句子数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 中文词汇表大小：&quot;</span>, voc_size_cn) <span class="comment"># 打印中文词汇表大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 英文词汇表大小：&quot;</span>, voc_size_en) <span class="comment"># 打印英文词汇表大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 中文词汇到索引的字典：&quot;</span>, word2idx_cn) <span class="comment"># 打印中文词汇到索引的字典</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 英文词汇到索引的字典：&quot;</span>, word2idx_en) <span class="comment"># 打印英文词汇到索引的字典</span></span><br><span class="line"><span class="comment"># 句子数量： 5</span></span><br><span class="line"><span class="comment"># 中文词汇表大小： 18</span></span><br><span class="line"><span class="comment"># 英文词汇表大小： 20</span></span><br><span class="line"><span class="comment"># 中文词汇到索引的字典： &#123;&#x27;人工智能&#x27;: 0, &#x27;语言&#x27;: 1, &#x27;深度学习&#x27;: 2, &#x27;强大&#x27;: 3, &#x27;很&#x27;: 4, &#x27;复杂&#x27;: 5, &#x27;喜欢&#x27;: 6, &#x27;改变&#x27;: 7,</span></span><br><span class="line"><span class="comment">#                        &#x27;处理&#x27;: 8, &#x27;自然&#x27;: 9, &#x27;小冰&#x27;: 10, &#x27;神经网络&#x27;: 11, &#x27;学习&#x27;: 12, &#x27;我&#x27;: 13, &#x27;咖哥&#x27;: 14, &#x27;爱&#x27;: 15,</span></span><br><span class="line"><span class="comment">#                        &#x27;世界&#x27;: 16, &#x27;非常&#x27;: 17&#125;</span></span><br><span class="line"><span class="comment"># 英文词汇到索引的字典： &#123;&#x27;I&#x27;: 0, &#x27;&lt;eos&gt;&#x27;: 1, &#x27;are&#x27;: 2, &#x27;powerful&#x27;: 3, &#x27;changed&#x27;: 4, &#x27;AI&#x27;: 5, &#x27;Neural-Nets&#x27;: 6, &#x27;NLP&#x27;: 7,</span></span><br><span class="line"><span class="comment">#                        &#x27;&lt;sos&gt;&#x27;: 8, &#x27;XiaoBing&#x27;: 9, &#x27;KaGe&#x27;: 10, &#x27;studying&#x27;: 11, &#x27;the&#x27;: 12, &#x27;likes&#x27;: 13, &#x27;love&#x27;: 14,</span></span><br><span class="line"><span class="comment">#                        &#x27;is&#x27;: 15, &#x27;DL&#x27;: 16, &#x27;complex&#x27;: 17, &#x27;world&#x27;: 18, &#x27;so&#x27;: 19&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentences</span>):</span><br><span class="line">    sentence = random.choice(sentences)</span><br><span class="line">    encoder_input = torch.LongTensor([word2idx_cn[word]<span class="keyword">for</span> word <span class="keyword">in</span> sentence[<span class="number">0</span>].split()]).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    decoder_input = torch.LongTensor([word2idx_en[word]<span class="keyword">for</span> word <span class="keyword">in</span> sentence[<span class="number">1</span>].split()]).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    encoder_output = torch.LongTensor([word2idx_en[word]<span class="keyword">for</span> word <span class="keyword">in</span> sentence[<span class="number">2</span>].split()]).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> encoder_input,decoder_input,encoder_output</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(make_data(sentences))</span><br><span class="line"><span class="comment"># (tensor([[16,  8, 12]]), tensor([[ 3,  0, 12,  8, 17]]), tensor([[ 0, 12,  8, 17, 14]]))</span></span><br></pre></td></tr></table></figure>
<p>构建<strong>encoder</strong>和<strong>decoder</strong></p>
<ul>
<li>encoder:
<ul>
<li>输入大小为输入中文词库的大小，在这里是18，<code>hidden_size</code>人为定义，为128,输出大小也为128</li>
<li>主要包含一层<code>embedding</code>，将输入词语映射到向量中，然后带入<code>rnn</code>层进行编码</li>
</ul>
</li>
<li>decoder
<ul>
<li><code>embedding</code>将解码器输入部分进行编码，输出大小为输出英文词库的大小，这里为20</li>
<li>将<code>embedding</code>输出和从解码器过来的<code>hidden</code>输入rnn</li>
<li>输出接一个线性层，线性层输出大小也为输出英文词库的大小，这里为20，代表每个英文单词的概率</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.input_size = input_size</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(<span class="variable language_">self</span>.input_size, <span class="variable language_">self</span>.hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, encoder_input, hidden</span>):</span><br><span class="line">        embedding = <span class="variable language_">self</span>.embedding(encoder_input)</span><br><span class="line">        output, hidden = <span class="variable language_">self</span>.rnn(embedding, hidden)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(output_size, <span class="variable language_">self</span>.hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, decoder_input, hidden</span>):</span><br><span class="line">        embedding = <span class="variable language_">self</span>.embedding(decoder_input)</span><br><span class="line">        output, hidden = <span class="variable language_">self</span>.rnn(embedding, hidden)</span><br><span class="line">        output = <span class="variable language_">self</span>.linear(output)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">voc_size = <span class="built_in">len</span>(word_list_en)</span><br><span class="line">hidden_size = <span class="number">128</span></span><br><span class="line">encoder = Encoder(<span class="built_in">len</span>(word_list_cn), hidden_size)</span><br><span class="line">decoder = Decoder(hidden_size, <span class="built_in">len</span>(word_list_en))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(encoder, decoder)</span><br><span class="line"><span class="comment"># Encoder(</span></span><br><span class="line"><span class="comment">#   (embedding): Embedding(18, 128)</span></span><br><span class="line"><span class="comment">#   (rnn): RNN(128, 128, batch_first=True)</span></span><br><span class="line"><span class="comment"># ) Decoder(</span></span><br><span class="line"><span class="comment">#   (embedding): Embedding(20, 128)</span></span><br><span class="line"><span class="comment">#   (rnn): RNN(128, 128, batch_first=True)</span></span><br><span class="line"><span class="comment">#   (linear): Linear(in_features=128, out_features=20, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure>
<p><strong>构建Seq2Seq模型</strong></p>
<ul>
<li><code>forward</code>函数用于训练</li>
<li><code>predict</code>函数用于测试，在《GPT图解》中没有这个函数，取而代之的是输入<code>&lt;sos&gt;&lt;sos&gt;&lt;sos&gt;...&lt;eos&gt;</code>,这样每一次解码器的输入都是<code>&lt;eos&gt;</code>，效果是比较差的。
<ul>
<li>正确的做法应该是将解码器每一步的输出当场下一步的输入</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2Seq</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,encoder,decoder</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2Seq, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,encoder_input,hidden,decoder_input</span>):</span><br><span class="line">        encoder_output,hidden = <span class="variable language_">self</span>.encoder(encoder_input,hidden)</span><br><span class="line">        decoder_output,_ = <span class="variable language_">self</span>.decoder(decoder_input,hidden)</span><br><span class="line">        <span class="keyword">return</span> decoder_output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, input_seq, start_token, max_length,end_token</span>):</span><br><span class="line">        batch_size = input_seq.size(<span class="number">0</span>)</span><br><span class="line">        hidden = torch.zeros(<span class="number">1</span>, batch_size, <span class="variable language_">self</span>.decoder.hidden_size)  <span class="comment"># 初始化隐藏状态</span></span><br><span class="line">        encoder_output, hidden = <span class="variable language_">self</span>.encoder(input_seq, hidden)</span><br><span class="line">        decoder_input = torch.tensor([start_token] * batch_size).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 解码器的第一回合的输入还是start_token，也就是&lt;sos&gt;</span></span><br><span class="line"></span><br><span class="line">        output_seq = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_length):</span><br><span class="line">            <span class="comment"># 解码器前向传播</span></span><br><span class="line">            decoder_output, hidden = <span class="variable language_">self</span>.decoder(decoder_input, hidden)</span><br><span class="line">            <span class="comment"># 取出输出中概率最大的词的序号</span></span><br><span class="line">            decoder_input = decoder_output.data.<span class="built_in">max</span>(<span class="number">2</span>,keepdim=<span class="literal">True</span>)[<span class="number">1</span>].squeeze(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 当前的输入作为下一步的输入</span></span><br><span class="line">            output_word = <span class="built_in">int</span>(decoder_input[<span class="number">0</span>][<span class="number">0</span>].detach())</span><br><span class="line">            output_seq.append(output_word)</span><br><span class="line">            <span class="comment"># 如果输出是&lt;eos&gt;则结束预测</span></span><br><span class="line">            <span class="keyword">if</span> output_word == end_token:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> output_seq</span><br></pre></td></tr></table></figure>
<p><strong>训练和预测</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_seq2seq</span>(<span class="params">model,sentences,epochs,optimizer,loss_func</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        encoder_input,decoder_input,encoder_output = make_data(sentences)</span><br><span class="line">        hidden = torch.zeros(<span class="number">1</span>,encoder_input.size(<span class="number">0</span>),hidden_size)<span class="comment">#torch.Size([1, 1, 128])</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(encoder_input,hidden,decoder_input)</span><br><span class="line">        loss = loss_func(output.view(-<span class="number">1</span>,output.size(<span class="number">2</span>)),encoder_output.squeeze(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(epoch,loss)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(),lr=<span class="number">0.001</span>)</span><br><span class="line">loss_func = nn.CrossEntropyLoss()</span><br><span class="line">train_seq2seq(model,sentences,<span class="number">200</span>,optimizer,loss_func)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_seq2seq</span>(<span class="params">model,test_input,word2index_cn = word2idx_cn,word2index_en = word2idx_en</span>):</span><br><span class="line">    encoder_input = torch.LongTensor([word2index_cn[i] <span class="keyword">for</span> i <span class="keyword">in</span> test_input.split()]).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    hidden = torch.zeros(<span class="number">1</span>, encoder_input.size(<span class="number">0</span>), hidden_size)  <span class="comment"># torch.Size([1, 1, 128])</span></span><br><span class="line">    start_token = word2index_en[<span class="string">&#x27;&lt;sos&gt;&#x27;</span>]</span><br><span class="line">    end_token = word2index_en[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(start_token)</span><br><span class="line">    predict  = model.predict(encoder_input,start_token,<span class="number">10</span>,end_token=end_token)</span><br><span class="line"></span><br><span class="line">    predict = [word_list_en[i] <span class="keyword">for</span> i <span class="keyword">in</span> predict]</span><br><span class="line">    <span class="built_in">print</span>(predict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test_seq2seq(model,<span class="string">&#x27;深度学习 很 强大&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Seq2Seq的局限性</strong></p>
<ul>
<li>编码器将其最后一个状态输出交由解码器进行解码，这要求编码器最后一个状态中包含所有信息，这其实是非常困难的，尤其是输入序列较长的时候，可能会存在信息丢失问题和梯度消失问题</li>
<li>编码时序列被编码成了固定长度的向量，解码过程中模型难以关注到序列的重要信息。</li>
</ul>
<h1 id="chapter5注意力机制"><a class="markdownIt-Anchor" href="#chapter5注意力机制"></a> Chapter5：注意力机制</h1>
<h2 id="点积注意力dot-product-attention"><a class="markdownIt-Anchor" href="#点积注意力dot-product-attention"></a> 点积注意力（Dot-Product Attention）</h2>
<p>点积注意力的公式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">⋅</mo><msup><mi>K</mi><mi>T</mi></msup><mo stretchy="false">)</mo><mo separator="true">⋅</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">out = softmax(Q·K^T)·V
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p>
<p>为了便于理解，这里不展开Q，K，V的描述，先从两个向量解释：</p>
<ul>
<li>对于张量<code>x1</code>(<em>batch_size, sep_len1, feature_dim</em>)和<code>x2</code>(<em>batch_size, sep_len2, feature_dim</em>)</li>
<li><code>x1</code>与<code>x2</code>（转置）进行<strong>点积</strong>，得到初始权重<code>raw_weights</code>，大小为(<em>batch_size, sep_len1, sep_len2</em>)</li>
<li>使用<code>softmax</code>对其<strong>行</strong>进行归一化，得到归一化后注意力权重<code>atten_weights</code>，大小不变</li>
<li>最后跟x2进行加权求和，也就是相乘，得到<code>atten_out</code>(<em>batch_size, sep_len1, feature_dim</em>)，这就是x1对于x2的<strong>点积注意力</strong></li>
</ul>
<p>第2步中的点积实际上是提取<code>x1</code>和<code>x2</code>不同元素之间的相似度，可以想象<code>x1</code>是“衣服感兴趣向量”，例如代表（质量、品牌、美观），值为(0.8,0.1,0.1)，表示其最需要质量。<code>x2</code>为“衣服实际状态向量”，由于两个向量<em>feature_dim</em>是一样的，其也代表（质量、品牌、美观），值为（98,1,1），这件衣服的重点在于质量，那么<code>x1</code>与<code>x2</code>点积结果就会很大。</p>
<p>在<code>raw_weights</code>(<em>batch_size, sep_len1, sep_len2</em>)中，每个元素表示<code>x1</code>中的每个元素对于<code>x2</code>中的每个元素的<strong>相似程度</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x1 = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">x2 = torch.randn(<span class="number">2</span>,<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">raw_weights = torch.bmm(x1,x2.transpose(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment">#(2,3,5)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>torch.bmm</code>表示批量矩阵乘法（Batch Matrix - Multiplication），它主要用于处理小批次（batch）的矩阵乘法运算场景，其输入需要为三维，而<code>torch.matmul</code>也是矩阵乘法，但是其更加灵活，可以处理2维，但是为了代码严谨性，在确定为批量矩阵乘法的情况下，使用<code>torch.bmm</code>可以提高代码可读性。</p>
</blockquote>
<p>第3步，softmax进行归一化，意义不变，只是将相似程度变成了类似概率值形式，例如下面第一行是 <em>[0.7248, 0.1541, 0.0420, 0.0030, 0.0761]</em>，表示<code>x1</code>的第一个元素对<code>x2</code>第一个元素关注度最高，有0.7248，对<code>x2</code>第二个元素关注度只有0.1541</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">atten_weights = F.softmax(raw_weights,dim=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tensor([[[0.7248, 0.1541, 0.0420, 0.0030, 0.0761],</span></span><br><span class="line"><span class="comment">#          [0.1541, 0.0578, 0.6464, 0.0147, 0.1270],</span></span><br><span class="line"><span class="comment">#          [0.4476, 0.0523, 0.0898, 0.3739, 0.0364]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[0.6825, 0.2509, 0.0154, 0.0353, 0.0159],</span></span><br><span class="line"><span class="comment">#          [0.0075, 0.2422, 0.6167, 0.0582, 0.0755],</span></span><br><span class="line"><span class="comment">#          [0.1848, 0.0666, 0.5354, 0.1908, 0.0223]]])</span></span><br></pre></td></tr></table></figure>
<p>第4步本质上是根据关注度，或者说权重，提取<code>x2</code>中的关键信息，因为<strong>注意力机制的目的就是格局x1中各个位置的关注程度提取x2中的关键信息</strong>，还是那个衣服的例子，假设x1的第一个元素对于x2三个元素的权重分布为0.8,0.1,0.1,而x2三个元素在“质量”这个特征上的值为100,1,1,那么x1第一个元素关于x2点积注意力中关于“质量”部分的值为80.2，<strong>这是包含了x1和x2所有信息的结果。</strong></p>
<p>这样的意义在于，out中的词被编码之后的信息，就不再仅仅包含自身或只学习了周围几个词的信息，而是整合了整个序列的全部。</p>
<p><strong>其实本质上来说，注意力机制的目的是根据<code>x1</code>的各个位置的关注程度来提取<code>x2</code>中的关键信息</strong></p>
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/pic.png" alt="pic" style=" height: 300px !important;">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">atten_out = torch.bmm(atten_weights,x2) <span class="comment">#(2,3,4)</span></span><br><span class="line"><span class="comment"># tensor([[[ 2.8867,  0.5762, -0.1491,  0.6604],</span></span><br><span class="line"><span class="comment">#          [ 0.1754,  0.7851,  0.5922,  1.0507],</span></span><br><span class="line"><span class="comment">#          [ 1.0783, -0.1699, -0.1212,  0.4201]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[-0.0468,  0.1708,  1.3725,  0.0546],</span></span><br><span class="line"><span class="comment">#          [ 0.6665,  1.8111,  0.8774,  0.6866],</span></span><br><span class="line"><span class="comment">#          [ 0.7801,  1.3610,  0.9799,  0.5164]]]) torch.Size([2, 3, 4])</span></span><br></pre></td></tr></table></figure>
<h2 id="缩放点积注意力scaled-dot-product-attention"><a class="markdownIt-Anchor" href="#缩放点积注意力scaled-dot-product-attention"></a> 缩放点积注意力（Scaled Dot-Product Attention）</h2>
<p>缩放点积注意力公式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><mo separator="true">⋅</mo><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo stretchy="false">)</mo><mo separator="true">⋅</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">out = softmax(\frac{Q·K^T}{\sqrt{d}})·V
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.448331em;vertical-align:-0.93em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183309999999999em;"><span style="top:-2.17778em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.93222em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">d</span></span></span><span style="top:-2.89222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.10777999999999999em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p>
<p><strong>与点积注意力的最大差别是，在第2步之前和在第3步之后，将点积结果除以一个缩放因子，一般是输入特征维度的平方根。</strong></p>
<blockquote>
<p>因为许多时候特征维度很大的时候，点积结果会很大，除以缩放因子<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mi>d</mi></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.10777999999999999em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.93222em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">d</span></span></span><span style="top:-2.89222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.10777999999999999em;"><span></span></span></span></span></span></span></span></span>（其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">d</span></span></span></span>是输入向量的维度）的目的是为了缓解上述问题。通过缩放注意力分数，使得 Softmax 函数的输入值不会因为维度过高而出现过大的差异，减轻了梯度消失的问题。</p>
</blockquote>
<p><strong>softmax 反正会将结果归一化，为什么还需要除以缩放因子呢</strong></p>
<blockquote>
<ol>
<li><strong>Softmax 函数的特点和潜在问题</strong><br />
Softmax 函数的公式为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Softmax</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Softmax}(x_{i}) = \frac{e^{x_{i}}}{\sum_{j}e^{x_{j}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.588492em;vertical-align:-0.677512em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.91098em;"><span style="top:-2.6447149999999997em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.14964714285714287em;"><span style="top:-2.1785614285714283em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.46032428571428574em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.19516666666666668em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7789785714285715em;"><span style="top:-2.9714357142857146em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.65952em;"></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5091600000000001em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7385428571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.65952em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.31472em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.677512em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，它将输入的数值向量转换为一个概率分布向量，其中每个元素都在 0 到 1 之间，且所有元素之和为 1。<br />
当输入的数值向量中元素之间的差异较大时，例如在点积注意力机制中，如果查询向量和键向量的维度较高，点积的结果（未缩放的注意力分数）可能会出现较大的值。假设未缩放的注意力分数为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy="false">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">x = [x_{1}, x_{2}, \cdots, x_{n}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>，当其中某个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>很大时，经过 Softmax 计算后，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup></mrow><annotation encoding="application/x-tex">e^{x_{i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>会在分母<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow><annotation encoding="application/x-tex">\sum_{j}e^{x_{j}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.185818em;vertical-align:-0.43581800000000004em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>中占主导地位。<br />
这会导致 Softmax 函数的输出概率分布出现极端情况，大部分概率集中在最大值对应的位置，其他位置的概率接近于 0。在这种情况下，在反向传播过程中，梯度会变得非常小（梯度消失现象），使得模型难以有效地学习到不同位置之间的关系。</li>
<li><strong>缩放因子的作用</strong><br />
除以缩放因子<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.18278000000000005em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85722em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.18278000000000005em;"><span></span></span></span></span></span></span></span></span>（其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_{k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是键向量的维度）的目的是为了缓解上述问题。通过缩放注意力分数，使得 Softmax 函数的输入值不会因为维度过高而出现过大的差异。<br />
例如，在高维空间中，点积的结果可能会随着维度的增加而增大。假设未缩放的注意力分数与维度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_{k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>成线性关系，当除以<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_{k}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.18278000000000005em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85722em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.18278000000000005em;"><span></span></span></span></span></span></span></span></span>后，能够将注意力分数的大小控制在一个相对合理的范围内，避免 Softmax 函数的输出过于极端。<br />
这样，在反向传播过程中，梯度能够更有效地传播，模型可以更好地学习到每个位置的信息对最终结果的贡献，尤其是在处理长序列和高维向量的场景下，这有助于提高模型的性能和训练效率。</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x1 = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">x2 = torch.randn(<span class="number">2</span>,<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">raw_weights = torch.bmm(x1,x2.transpose(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment">#(2,3,5)</span></span><br><span class="line"></span><br><span class="line">scaling_factor = x1.size(-<span class="number">1</span>)**<span class="number">0.5</span><span class="comment">#2</span></span><br><span class="line"></span><br><span class="line">atten_weights = F.softmax(raw_weights/scaling_factor,dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">atten_out = torch.bmm(atten_weights,x2) <span class="comment">#(2,3,4)</span></span><br></pre></td></tr></table></figure>
<h2 id="编码器-解码器注意力"><a class="markdownIt-Anchor" href="#编码器-解码器注意力"></a> 编码器-解码器注意力</h2>
<p>将注意力机制运用到编码器-解码器架构中，上述文中的<code>x1</code>和<code>x2</code>分别对应<strong>解码器</strong>和<strong>编码器</strong>：</p>
<ul>
<li><code>x1</code>：对应<strong>解码器</strong>的各个时间步的隐藏状态。</li>
<li><code>x2</code>：对应<strong>编码器</strong>的各个时间步的隐藏状态。</li>
</ul>
<p><strong>大概步骤为：</strong></p>
<ul>
<li>得到将编码器的输出<code>encoder_output</code>（这里可以表征为解码器每一个时间步的状态，大小为<code>(batch_size, seq_len, encoder_out_size)</code>）</li>
<li>将这个输出和解码器的每一个时间步的<code>rnn</code>的输出<code>decoder_rnn_output</code>进行<strong>注意力</strong>计算，得到<code>attention_output</code></li>
<li>最后将<code>attention_output</code>和<code>decoder_rnn_output</code>拼接起来输入线性层，得到最终输出。</li>
</ul>
<p><strong>下面是对上述方法的实现：</strong></p>
<p>首先是定义<code>Attenton</code>方法，实现了之前介绍的注意力机制</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attenion</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Attenion,<span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,encoder_context,decoder_context</span>):</span><br><span class="line">        raw_weights = torch.bmm(decoder_context, encoder_context.transpose(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># </span></span><br><span class="line">        atten_weights = F.softmax(raw_weights, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        atten_out = torch.bmm(atten_weights, encoder_context)  </span><br><span class="line">        <span class="keyword">return</span> atten_out,atten_weights</span><br></pre></td></tr></table></figure>
<p>重构<code>Decoder</code>部分，主要的变化是</p>
<ul>
<li>增加了<code>attention</code>部分，将<code>rnn</code>的输出和解码器的输出作为其输入</li>
<li>将<code>attention</code>的输出和<code>rnn</code>输出拼接起来输入线性层</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderWithAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderWithAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(output_size, <span class="variable language_">self</span>.hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.attention = Attenion()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">2</span> * hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, decoder_input, hidden, encoder_output</span>):</span><br><span class="line">        embedding = <span class="variable language_">self</span>.embedding(decoder_input)</span><br><span class="line">        rnn_output, hidden = <span class="variable language_">self</span>.rnn(embedding, hidden)</span><br><span class="line">        attention_output, attention_weights = <span class="variable language_">self</span>.attention(encoder_output,rnn_output)</span><br><span class="line">        decoder_output = <span class="variable language_">self</span>.linear(torch.cat((rnn_output,attention_output),dim=-<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> decoder_output, hidden ,  attention_weights</span><br></pre></td></tr></table></figure>
<p>其余实现部分与前文类似，此处不赘述</p>
<p>在大部分的关于注意力的文章中都会对于<strong>Q，K，V</strong>进行描述，以展开注意力的介绍，回忆缩放点积注意力的公式：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><mo separator="true">⋅</mo><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo stretchy="false">)</mo><mo separator="true">⋅</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">out = softmax(\frac{Q·K^T}{\sqrt{d}})·V
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.448331em;vertical-align:-0.93em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183309999999999em;"><span style="top:-2.17778em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.93222em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">d</span></span></span><span style="top:-2.89222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.10777999999999999em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p>
<ul>
<li><strong>Q：Query</strong>，查询</li>
<li><strong>K：Key</strong>，键</li>
<li><strong>V：Value</strong>，值</li>
</ul>
<p>这里通过一个通俗的例子说明这个过程的意义，假如我们需要去图书馆看书。<strong>Q</strong>表示我们需要的书的清单，<strong>K</strong>表示图书馆的书的编号，<strong>V</strong>表示书的具体内容。首先我们会根据我们的清单和编号去确定我需要的书的编号，然后根据这个结果去找书，最终拿到需要的书的结果。</p>
<p>而对于<strong>编码器解码器</strong>的过程，<strong>Q，K，V</strong>的对应如下所示：</p>
<ul>
<li>编码器的隐藏状态：<strong>K，V</strong></li>
<li>解码器的隐藏状态：<strong>Q</strong></li>
</ul>
<p>本质上，在Seq2Seq中运用Attention的意义就是，<strong>可以得到在当前解码器的输入下，它对编码器的哪些信息更感兴趣，最后根据结果提取这个感兴趣的内容。</strong> 这样的好处是，能使得解码器的输出再任意时刻不再单一依赖于编码器的最后隐藏层，且也可以过滤到许多无效信息。</p>
<p>例如对于一个翻译任务：<code>我爱你--&gt;I Love You</code>，某一时刻解码器的输入是<code>I</code>,那么<strong>Q，K，V</strong>的通俗意义如下：</p>
<ul>
<li><strong>Q</strong>：<code>I</code>对应的一些表征</li>
<li><strong>K</strong>：在解码器的输出中，对于<code>我爱你</code>的表征</li>
<li><strong>V</strong>：同K，虽然跟K一样，但是其意义不一样</li>
</ul>
<p>那么在这个过程中，Q和K点积会得到当前情况下（输入为<code>I</code>），解码器对<code>我爱你</code>的表征哪些比较感兴趣（也许是<code>我</code>和<code>爱</code>），这里得到的是一个感兴趣的概率，然后再乘以V，最终得到感兴趣的内容。</p>
<h2 id="多头自注意力multi-head-attention"><a class="markdownIt-Anchor" href="#多头自注意力multi-head-attention"></a> 多头自注意力（Multi-head Attention）</h2>
<p><strong>自注意力：</strong></p>
<p>在之前的做法中，Q，K，V向量可能是不同的来源，<strong>而自注意力则是表示对同一个输入进行不同的线性变换，得到<em>Q，K，V</em>向量，然后再应用缩放点积注意力即可</strong></p>
<p>而多头自注意力是一种扩展形式，<strong>可以帮助模型从不同的表示子空间捕获输入数据的特征</strong>，主要做法是：</p>
<ul>
<li><em>Q，K，V</em>分别进行多次线性变化，从而获得不同的head</li>
<li>进行缩放点积注意力</li>
<li>将不同的head的注意力结果拼接起来输入线性层</li>
</ul>
<p><img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/attention2.png" alt="" /></p>
<p><strong>下面实现一个简单的多头自注意力：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyexpat <span class="keyword">import</span> features</span><br><span class="line"></span><br><span class="line">x = torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">num_heads = <span class="number">4</span></span><br><span class="line">head_dim = x.size(-<span class="number">1</span>)//num_heads</span><br><span class="line"><span class="comment"># 计算每个头对应的维度大小，这里假设原始特征维度能被头的数量整除</span></span><br><span class="line"></span><br><span class="line">linear_layers_q = [torch.nn.Linear(x.size(-<span class="number">1</span>), head_dim) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_heads)]</span><br><span class="line">linear_layers_k = [torch.nn.Linear(x.size(-<span class="number">1</span>), head_dim) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_heads)]</span><br><span class="line">linear_layers_v = [torch.nn.Linear(x.size(-<span class="number">1</span>), head_dim) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_heads)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成Q、K、V，每个头都有独立的线性层进行转换</span></span><br><span class="line">Qs = [linear_layer_q(x) <span class="keyword">for</span> linear_layer_q <span class="keyword">in</span> linear_layers_q]</span><br><span class="line">Ks = [linear_layer_k(x) <span class="keyword">for</span> linear_layer_k <span class="keyword">in</span> linear_layers_k]</span><br><span class="line">Vs = [linear_layer_v(x) <span class="keyword">for</span> linear_layer_v <span class="keyword">in</span> linear_layers_v]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将每个头的Q、K、V分别堆叠起来，形成新的维度 (batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line">Q = torch.stack(Qs, dim=<span class="number">1</span>)</span><br><span class="line">K = torch.stack(Ks, dim=<span class="number">1</span>)</span><br><span class="line">V = torch.stack(Vs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完成缩放点积注意力运算</span></span><br><span class="line">raw_weights = torch.matmul(Q,K.transpose(-<span class="number">2</span>,-<span class="number">1</span>))</span><br><span class="line">scale_factor = K.size(-<span class="number">1</span>) ** <span class="number">0.5</span></span><br><span class="line">scale_weights = raw_weights / scale_factor</span><br><span class="line"><span class="built_in">print</span>(scale_weights.size())</span><br><span class="line"><span class="comment"># torch.Size([2, 4, 3, 3]),batch_size, num_heads, seq_len, seq_len</span></span><br><span class="line"></span><br><span class="line">attention_weights = F.softmax(scale_weights, dim=-<span class="number">1</span>)</span><br><span class="line">attention_output = torch.matmul(attention_weights, V)</span><br><span class="line"><span class="built_in">print</span>(attention_output.size())</span><br><span class="line"><span class="comment"># torch.Size([2, 4, 3, 32]) batch_size, num_heads, seq_len, head_dim</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并多头输出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">combine_heads</span>(<span class="params">data</span>):</span><br><span class="line">    batch_size, num_heads,seq_len, head_dim  = data.size()</span><br><span class="line">    feature_dim = num_heads*head_dim</span><br><span class="line">    output = data.transpose(<span class="number">1</span>,<span class="number">2</span>).contiguous().view(batch_size,seq_len,feature_dim)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">attention_output = combine_heads(attention_output)</span><br><span class="line">linear_out = torch.nn.Linear(x.size(-<span class="number">1</span>),<span class="number">64</span>)</span><br><span class="line">attention_output = linear_out(attention_output)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_output.size())</span><br><span class="line"><span class="comment"># torch.Size([2, 3, 64])</span></span><br></pre></td></tr></table></figure>
<h1 id="chapter6transformer"><a class="markdownIt-Anchor" href="#chapter6transformer"></a> Chapter6：Transformer</h1>
<h2 id="transformer结构分析"><a class="markdownIt-Anchor" href="#transformer结构分析"></a> Transformer结构分析</h2>
<p>Transformer的主要结构为：</p>
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/transformer.png" alt="pic" style=" height: 800px !important;">
<p>下面配合代码对Transformer的每个组成部分进行说明，在开始前需要对一些参数进行设置：</p>
<ul>
<li><code>d_q，d_k，d_v</code>：<strong>Q，K，V</strong>张量的维度，其中<code>d_q=d_k</code></li>
<li><code>batch_size</code>：每次训练的批数据大小</li>
<li><code>dim_embedding</code>：词的编码长度</li>
<li><code>num_heads</code>：多头注意力中“头”的数量</li>
<li><code>n_layer</code>：encoder或者decoder中的层数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">d_k = <span class="number">64</span> </span><br><span class="line">d_v = <span class="number">64</span></span><br><span class="line">d_q = <span class="number">64</span> <span class="comment">#必须跟d_k一样</span></span><br><span class="line">batch_size = <span class="number">3</span></span><br><span class="line">dim_embedding = <span class="number">512</span> </span><br><span class="line">num_heads = <span class="number">8</span></span><br><span class="line">n_layer = <span class="number">6</span></span><br></pre></td></tr></table></figure>
<h3 id="注意力掩码"><a class="markdownIt-Anchor" href="#注意力掩码"></a> 注意力掩码</h3>
<p>在进行说明之前，还需要详细说明transformer中的<strong>掩码机制</strong>，具体来说分为两种：</p>
<ul>
<li><strong>填充注意力掩码</strong>（Padding Attention Mask）：当处理的序列长度不一样时，需要对短的序列进行填充，使所有序列的长度一样，这样就可以批处理了。但是填充的部分往往是没有意义的，<strong>这个时候需要将填充的部分进行掩码，具体做法是把这些位置的注意力权重设置为极小值，在softmax后这些权重趋近于0，</strong> 就可以避免这部分内容的影响。
<ul>
<li>填充注意力掩码在编码器部分和解码器部分都有运用</li>
</ul>
</li>
<li><strong>后续注意力掩码</strong>（Subsequent Attention Mask）：在自回归任务时，模型需要逐步的输出序列，<strong>为了避免在输出当前步的序列时看到未来的信息，这里需要将对应未来的信息掩码。具体的做法跟上面的类似，也是将当前位置之后的位置的注意力权重设置为极小值。</strong>
<ul>
<li>后续注意力掩码主要用在解码器的第一个多头注意力部分</li>
</ul>
</li>
</ul>
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/transformer2.png" alt="pic" style=" height: 400px !important;">
<p><strong>现在分别实现两种掩码方式：</strong></p>
<ul>
<li><strong>填充注意力掩码</strong>
<ul>
<li>输入是最原始的<strong>未编码</strong>的序列，<code>seq_q</code>和<code>seq_k</code>大小都是<code>batch_size, len_q</code>,不同批次（句子）之间的序列长度不一样，所以可能需要填充无意义的内容</li>
<li>掩码张量需要和注意力权重大小一样，大小为<code>batch_size,len_q,len_k</code>，直接复制即可</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attention_pad_mask</span>(<span class="params">seq_q,seq_k</span>):</span><br><span class="line">    <span class="built_in">print</span>(seq_q)</span><br><span class="line">    <span class="comment"># tensor([[4, 5, 6, 7, 0],</span></span><br><span class="line">    <span class="comment">#         [14, 15, 16, 0, 0],</span></span><br><span class="line">    <span class="comment">#         [1, 2, 3, 0, 0]])</span></span><br><span class="line">    batch_size, len_q = seq_q.size()</span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    pad_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>) <span class="comment">#batch_size, 1 ,len_k</span></span><br><span class="line">    pad_mask = pad_mask.expand(batch_size,len_q,len_k) <span class="comment">#batch_size,len_q,len_k</span></span><br><span class="line">    <span class="built_in">print</span>(pad_mask)</span><br><span class="line">    <span class="comment"># tensor([[[False, False, False, False, True],</span></span><br><span class="line">    <span class="comment">#          [False, False, False, False, True],</span></span><br><span class="line">    <span class="comment">#          [False, False, False, False, True],</span></span><br><span class="line">    <span class="comment">#          [False, False, False, False, True],</span></span><br><span class="line">    <span class="comment">#          [False, False, False, False, True]], ... ]</span></span><br><span class="line">    <span class="keyword">return</span> pad_mask</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>后续注意力掩码</strong>
<ul>
<li><code>np.triu</code>构建一个大小为<code>batch_size,len_q,len_k</code>的上三角矩阵，且往右平移一位</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attention_subsequent_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    batch_size,seq_len = seq.size()</span><br><span class="line">    subsequent_mask = np.triu(np.ones((batch_size,seq_len,seq_len)),k=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(subsequent_mask)</span><br><span class="line">    <span class="comment"># [[[0. 1. 1. 1. 1.]</span></span><br><span class="line">    <span class="comment">#   [0. 0. 1. 1. 1.]</span></span><br><span class="line">    <span class="comment">#  [0. 0. 0. 1. 1.]</span></span><br><span class="line">    <span class="comment"># [0. 0. 0. 0. 1.]</span></span><br><span class="line">    <span class="comment"># [0. 0. 0. 0. 0.]]...]</span></span><br><span class="line">    subsequent_mask = torch.from_numpy(subsequent_mask).byte()</span><br><span class="line">    <span class="keyword">return</span> subsequent_mask</span><br></pre></td></tr></table></figure>
<h3 id="多头自注意力残差连接归一化"><a class="markdownIt-Anchor" href="#多头自注意力残差连接归一化"></a> 多头自注意力&amp;残差连接&amp;归一化</h3>
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/transformer3.png" alt="pic" style=" height: 300px !important;">
<p><strong>首先实现多头自注意机制</strong></p>
<p>多头自注意力是实现tarnsformer的基础，我们首先需要实现<strong>缩放点积注意力</strong>，其原理在前文中已经阐述，这里不再赘述。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">     <span class="built_in">super</span>(ScaledDotProductAttention,<span class="variable language_">self</span>).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,Q,K,V,attention_mask</span>):</span><br><span class="line">        raw_weights = torch.matmul(Q,K.transpose(-<span class="number">1</span>,-<span class="number">2</span>))/np.sqrt(d_k)</span><br><span class="line">        raw_weights.masked_fill(attention_mask,-<span class="number">1e9</span>)</span><br><span class="line">        weights = F.softmax(raw_weights,dim=-<span class="number">1</span>)</span><br><span class="line">        attention_output = torch.matmul(weights,V)</span><br><span class="line">        <span class="keyword">return</span> attention_output,weights</span><br></pre></td></tr></table></figure>
<p>这里的重点是<strong>掩码机制</strong>，通过<code>masked_fill</code>函数完成，函数功能为：</p>
<blockquote>
<p><code>tensor.masked_fill(mask, value)</code><br />
其中：</p>
<ul>
<li><code>tensor</code> 是要进行操作的 <code>PyTorch</code> 张量。</li>
<li><code>mask</code> 是一个布尔类型（<code>torch.bool</code>）的张量，其形状需要和 <code>tensor</code> 的形状或者能够广播（broadcast）到与 <code>tensor</code> 相同的形状。这个掩码张量用于指定 <code>tensor</code> 中哪些元素需要被填充，在 <code>mask</code> 中对应位置为 <code>True</code> 的元素所在的 <code>tensor</code> 中的位置就是要被填充的位置。</li>
<li><code>value</code> 是用于填充的具体值，其数据类型需要和 <code>tensor</code> 中元素的数据类型相匹配（或者能够进行相应的类型转换）。</li>
</ul>
</blockquote>
<p>接下来以此为基础实现<strong>多头自注意力机制</strong>：<br />
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/attention2.png" alt="" /></p>
<ul>
<li>在实践中，上图指代的<strong>Q，K，V</strong>，都是一样的，都是<strong>输入序列的编码</strong>；</li>
<li>多头注意力中，<strong>Q，K，V</strong>应该分别由<code>num_heads</code>个线性层得到。在实践中没有定义那么多线性层，而是由一个输出大小为<code>d_q(or d_k,d_v) * num_heads</code>的线性层一样，通过后期的形状变化，等价于<code>num_heads</code>个线性层的效果；</li>
<li>经过线性层后的<strong>Q，K，V</strong>数据形状为<code>batch_size,num_heads,seq_len,d_v(or d_k,d_v)</code></li>
<li><code>attention_mask</code>需要跟注意力权重大小一样，这里就是加了一个<code>num_head</code>维度，因为每个头的输入的序列其实是一样的，pad填充也是一样的，这里直接复制<code>num_head</code>即可</li>
<li>在完成点积注意力的计算之后，通过形状变化就可以实现上图中的<code>cancat</code>的效果</li>
</ul>
<p><strong>残差连接和归一化</strong>的做法是在多头自注意力的最终的线性层的下一层，与自注意的输入进行<strong>加和</strong>，一起通入归一化层，也就是<code>layer norm</code></p>
<ul>
<li><strong>残差连接的意义主要是在网络层变大的情况下</strong>，避免模型准确度不增反降的情况。在代码中的实现方式也很简单，直接<code>+</code>进行加和就行（注意这里不是拼接，所以这里<code>output</code>和<code>residual</code>大小一样）</li>
<li><code>layer norm</code>是针对一个序列的所有特征进行归一化，区别于<code>batch norm</code>，在这里直接用<code>nn.LayerNorm</code>即可</li>
<li>先进行残差加和再<code>layer norm</code>，和先<code>layer norm</code>再残差加和两种方式都可以，适应的场景不一样</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention,<span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 用一个线性层实现num_heads个线性层的效果</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_Q = nn.Linear(dim_embedding, d_q * num_heads)</span><br><span class="line">        <span class="variable language_">self</span>.linear_K = nn.Linear(dim_embedding, d_k * num_heads)</span><br><span class="line">        <span class="variable language_">self</span>.linear_V = nn.Linear(dim_embedding, d_v * num_heads)</span><br><span class="line">        <span class="variable language_">self</span>.linear_out = nn.Linear(d_v * num_heads, dim_embedding)</span><br><span class="line">        <span class="variable language_">self</span>.layer_norm = nn.LayerNorm(dim_embedding)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,Q,K,V,attention_mask</span>):</span><br><span class="line">        residual, batch_size = Q,Q.size(<span class="number">0</span>)  <span class="comment">#这里Q,K,V的内容是一样的，所以残差可以使用Q保存</span></span><br><span class="line">        q_s = <span class="variable language_">self</span>.linear_Q(Q).view(batch_size,-<span class="number">1</span>,num_heads,d_q).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        k_s = <span class="variable language_">self</span>.linear_K(K).view(batch_size,-<span class="number">1</span>,num_heads,d_k).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        v_s = <span class="variable language_">self</span>.linear_V(V).view(batch_size,-<span class="number">1</span>,num_heads,d_v).transpose(<span class="number">1</span>,<span class="number">2</span>)<span class="comment"># batch_size,num_heads,seq_len,d_v</span></span><br><span class="line">        attention_mask = attention_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>,num_heads,<span class="number">1</span>,<span class="number">1</span>)<span class="comment"># batch_size, num_heads, seq_len_q ,seq_len_k</span></span><br><span class="line"></span><br><span class="line">        attention_output, weights = ScaledDotProductAttention()(q_s,k_s,v_s,attention_mask)<span class="comment"># batch_size,num_heads,seq_len,d_v</span></span><br><span class="line">        <span class="comment"># cancat</span></span><br><span class="line">        attention_output = attention_output.transpose(<span class="number">1</span>,<span class="number">2</span>).contiguous().view(batch_size,-<span class="number">1</span>,num_heads*d_v) <span class="comment"># batch_size,seq_len,num_heads*d_v</span></span><br><span class="line">        output = <span class="variable language_">self</span>.linear_out(attention_output)</span><br><span class="line">        output = <span class="variable language_">self</span>.layer_norm(output + residual)<span class="comment"># batch_size,seq_len,dim_embedding</span></span><br><span class="line">        <span class="keyword">return</span> output,weights</span><br></pre></td></tr></table></figure>
<h3 id="前馈神经网络"><a class="markdownIt-Anchor" href="#前馈神经网络"></a> 前馈神经网络</h3>
<p><strong>Position-wise Feed Forward</strong>主要分布在编码器和解码器的每一层注意力层之后，主要组成部分是一个简单的<strong>两层线性层，线性层中间用一个激活函数（一般是ReLU）连接</strong></p>
<p>一般来说第一层线性层会将增加输入的维度，然后通过激活函数，在接入第二个线性层，然后把维度降到原始的大小，这样的好处是<strong>有助于模型学习到更复杂的特征表示，让模型能够学习到输入和输出之间的非线性关系。</strong></p>
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/transformer4.png" alt="pic" style=" height: 200px !important;">
<ul>
<li>跟上面的方法类似，这里也实现了残差连接和归一化处理</li>
<li>这里的<code>Position-wise</code>的理解是，<strong>神经网络是独立的处理输入序列的每个位置的</strong>，对于一个序列的不同的词语都是应用相同的神经网络，代码中定义线性层的大小为<code>dim_embedding</code>可以说明这一点，这样的好处有两点：
<ul>
<li><strong>参数共享与效率提升</strong>：其实本质是参数共享，减少了训练的时间</li>
<li><strong>位置无关性和泛化能力增强</strong>：这种设计体现了位置无关性。因为每个位置使用相同的神经网络，模型不会对序列中的某个特定位置产生偏向。在处理不同长度的序列时，具有更好的泛化能力。有助于模型学习到序列的全局特征，而不是被局部位置信息所干扰。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_ff=<span class="number">2048</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionFeedForward, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.ffn1 = nn.Linear(dim_embedding, d_ff)</span><br><span class="line">        <span class="variable language_">self</span>.ffn2 = nn.Linear(d_ff, dim_embedding)</span><br><span class="line">        <span class="variable language_">self</span>.layer_norm = nn.LayerNorm(dim_embedding)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        residual = inputs</span><br><span class="line">        output = nn.ReLU()(<span class="variable language_">self</span>.ffn1(inputs))</span><br><span class="line">        output = <span class="variable language_">self</span>.ffn2(output)</span><br><span class="line">        output = <span class="variable language_">self</span>.layer_norm(output + residual)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="位置编码"><a class="markdownIt-Anchor" href="#位置编码"></a> 位置编码</h3>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})
\\PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.03853em;vertical-align:-0.3551999999999999em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mord">/</span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.03853em;vertical-align:-0.3551999999999999em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mord">/</span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_sin_enc_table</span>(<span class="params">seq_len, dim_embedding</span>):</span><br><span class="line">    position_table = np.zeros((seq_len, dim_embedding))</span><br><span class="line">    <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(dim_embedding):</span><br><span class="line">            i = j // <span class="number">2</span></span><br><span class="line">            angle = pos / (np.power(<span class="number">10000</span>, (<span class="number">2</span> * i) / dim_embedding))</span><br><span class="line">            position_table[pos, i] = angle</span><br><span class="line"></span><br><span class="line">    position_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(position_table[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">    position_table[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(position_table[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">return</span> torch.FloatTensor(position_table)</span><br></pre></td></tr></table></figure>
<h3 id="encoder"><a class="markdownIt-Anchor" href="#encoder"></a> Encoder</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.muti_head_attention = MultiHeadAttention()</span><br><span class="line">        <span class="variable language_">self</span>.ffn = PositionFeedForward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, embedding_output, encoder_mask</span>):</span><br><span class="line">        encoder_attention_output, weights = <span class="variable language_">self</span>.muti_head_attention(embedding_output, embedding_output,</span><br><span class="line">                                                                     embedding_output, encoder_mask)</span><br><span class="line">        encoder_output = <span class="variable language_">self</span>.ffn(encoder_attention_output)</span><br><span class="line">        <span class="keyword">return</span> encoder_output, weights</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, corpus</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding_layer = nn.Embedding(<span class="built_in">len</span>(corpus.src_vocab), dim_embedding)</span><br><span class="line">        <span class="variable language_">self</span>.position_embedding = nn.Embedding.from_pretrained(get_sin_enc_table(corpus.src_len + <span class="number">1</span>, dim_embedding),</span><br><span class="line">                                                               freeze=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.encoder_layers = nn.ModuleList(EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layer))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, encoder_input</span>):</span><br><span class="line">        pos_indices = torch.arange(<span class="number">1</span>, encoder_input.size(<span class="number">1</span>) + <span class="number">1</span>).unsqueeze(<span class="number">0</span>).to(encoder_input)  <span class="comment"># 1,source_len</span></span><br><span class="line">        encoder_output = <span class="variable language_">self</span>.embedding_layer(encoder_input) + <span class="variable language_">self</span>.position_embedding(pos_indices)</span><br><span class="line">        encoder_mask = get_attention_pad_mask(encoder_input, encoder_input)</span><br><span class="line">        encoder_weights = []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.encoder_layers:</span><br><span class="line">            encoder_output, weights = layer(encoder_output, encoder_mask)</span><br><span class="line">            encoder_weights.append(weights)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> encoder_output, encoder_weights</span><br></pre></td></tr></table></figure>
<h3 id="decoder"><a class="markdownIt-Anchor" href="#decoder"></a> Decoder</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.muti_self_attention = MultiHeadAttention()</span><br><span class="line">        <span class="variable language_">self</span>.muti_seq2seq_attention = MultiHeadAttention()</span><br><span class="line">        <span class="variable language_">self</span>.ffn = PositionFeedForward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, encoder_output, embedding_output, self_mask, seq2seq_mask</span>):</span><br><span class="line">        decoder_output, self_weights = <span class="variable language_">self</span>.muti_self_attention(embedding_output, embedding_output, embedding_output,</span><br><span class="line">                                                                self_mask)</span><br><span class="line">        decoder_output, seq2seq_weights = <span class="variable language_">self</span>.muti_seq2seq_attention(decoder_output, encoder_output, encoder_output,</span><br><span class="line">                                                                      seq2seq_mask)</span><br><span class="line">        decoder_output = <span class="variable language_">self</span>.ffn(decoder_output)</span><br><span class="line">        <span class="keyword">return</span> decoder_output, self_weights, seq2seq_weights</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, corpus</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding_layer = nn.Embedding(<span class="built_in">len</span>(corpus.tgt_vocab), dim_embedding)</span><br><span class="line">        <span class="variable language_">self</span>.position_embedding = nn.Embedding.from_pretrained(get_sin_enc_table(corpus.src_len + <span class="number">1</span>, dim_embedding),</span><br><span class="line">                                                               freeze=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.decoder_layers = nn.ModuleList(DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layer))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, decoder_input, encoder_input, encoder_output</span>):</span><br><span class="line">        pos_indices = torch.arange(<span class="number">1</span>, decoder_input.size(<span class="number">1</span>) + <span class="number">1</span>).unsqueeze(<span class="number">0</span>).to(decoder_input)  <span class="comment"># 1,source_len</span></span><br><span class="line">        embedding_output = <span class="variable language_">self</span>.embedding_layer(decoder_input) + <span class="variable language_">self</span>.position_embedding(pos_indices)</span><br><span class="line">        decoder_output = embedding_output</span><br><span class="line"></span><br><span class="line">        decoder_self_pad_mask = get_attention_pad_mask(decoder_input, decoder_input)</span><br><span class="line">        decoder_seq2seq_pad_mask = get_attention_pad_mask(decoder_input, encoder_input)</span><br><span class="line">        decoder_self_subsequent_mask = get_attention_subsequent_mask(decoder_input)</span><br><span class="line"></span><br><span class="line">        decoder_self_mask = torch.gt((decoder_self_pad_mask + decoder_self_subsequent_mask), <span class="number">0</span>)</span><br><span class="line">        decoder_self_weights = []</span><br><span class="line">        decoder_seq2seq_weights = []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.decoder_layers:</span><br><span class="line">            decoder_output, self_weights, seq2seq_weights = layer(encoder_output, decoder_output, decoder_self_mask,</span><br><span class="line">                                                                  decoder_seq2seq_pad_mask)</span><br><span class="line">            decoder_self_weights.append(self_weights)</span><br><span class="line">            decoder_seq2seq_weights.append(seq2seq_weights)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decoder_output, decoder_self_weights, decoder_seq2seq_weights</span><br></pre></td></tr></table></figure>
<h3 id="transformer"><a class="markdownIt-Anchor" href="#transformer"></a> Transformer</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, corpus</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = Encoder(corpus)</span><br><span class="line">        <span class="variable language_">self</span>.decoder = Decoder(corpus)</span><br><span class="line">        <span class="variable language_">self</span>.projection = nn.Linear(dim_embedding, <span class="built_in">len</span>(corpus.tgt_vocab), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, encoder_input, decoder_input</span>):</span><br><span class="line">        encoder_output, encoder_weights = <span class="variable language_">self</span>.encoder(encoder_input)</span><br><span class="line">        decoder_output, decoder_self_weights, decoder_seq2seq_weights = <span class="variable language_">self</span>.decoder(decoder_input, encoder_input,</span><br><span class="line">                                                                                     encoder_output)</span><br><span class="line">        decoder_logits = <span class="variable language_">self</span>.projection(decoder_output)</span><br><span class="line">        <span class="keyword">return</span> decoder_logits, encoder_weights, decoder_self_weights, decoder_seq2seq_weights</span><br></pre></td></tr></table></figure>
<h1 id="chapter7gpt"><a class="markdownIt-Anchor" href="#chapter7gpt"></a> Chapter7：GPT</h1>
<p>语言模型从内部原理可以大致分为一支是 <strong>[自编码语言模型]</strong>（<strong>Autoencoder Language Model</strong>），<strong>[自回归语言模型]（AutoregressiveLanguage Model）</strong></p>
<ul>
<li><strong>自编码语言模型</strong>：通俗来说是从</li>
</ul>
<img src="https://gitee.com/leeMX111/images_for_markdown/raw/master/4-Figure1-2.png" alt="pic" style=" height: 400px !important;">
<p>GPT作为自回归语言模型，与Transformer最大的差异在于，<strong>其可以看成只采用了Decoder部分，且不包含编码器-解码器自注意力，任意时刻的输入都只能看到之前时间的信息</strong>（Transformer也在Decoder部分增加了因果掩码，但是由于编码器的参与，其实也是可以看到所有时间的信息的）。GPT单层的组成主要由下面几个部分组成：</p>
<ul>
<li><strong>词语编码</strong>：词语编码跟Transformer一样</li>
<li><strong>位置编码</strong>：GPT 主要采用了一种相对简单的位置编码方式，称为<strong>绝对位置嵌入（Absolute Positional Embedding）</strong>。其实实现方式就是一个相同的<code>embedding</code>层，输入是绝对位置</li>
<li><strong>掩码多头注意力</strong>：掩码和Transformer的<code>Decoder</code>层类似，包含<strong>填充掩码</strong>和<strong>因果掩码</strong></li>
<li><strong>残差连接&amp;归一化&amp;FFN</strong>：跟Transformer一样</li>
</ul>
<p><strong>总体来说，GPT的结构是Transformer的解码器删掉了Seq2Seq自注意力部分</strong></p>
<h2 id="gpt搭建"><a class="markdownIt-Anchor" href="#gpt搭建"></a> GPT搭建</h2>
<p><strong>下面实现GPT的单层：</strong><br />
主要包含4层，注意此处与transfomer的解码器之间的区别：</p>
<ul>
<li>掩码多头注意力层</li>
<li>layer norm层</li>
<li>全连接层</li>
<li>layer norm层<br />
在本书的代码中，此处的网络层都是采用chapter 6中的基础网络层，但是这些基础网络层其实已经内嵌了残差连接和归一化等操作，此处直接调用可能与原本的GBT结构不一样（不过对结果影响不大）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">dim_embedding = <span class="number">512</span></span><br><span class="line">n_layer = <span class="number">6</span></span><br><span class="line">batch_size = <span class="number">3</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer,<span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.multi_head_attention = MultiHeadAttention()</span><br><span class="line">        <span class="variable language_">self</span>.ffn = PositionFeedForward()</span><br><span class="line">        <span class="variable language_">self</span>.layer_norm_1 = nn.LayerNorm(dim_embedding)</span><br><span class="line">        <span class="variable language_">self</span>.layer_norm_2 = nn.LayerNorm(dim_embedding)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,decoder_input,mask</span>):</span><br><span class="line">        attention_output,_ = <span class="variable language_">self</span>.multi_head_attention(decoder_input,decoder_input,decoder_input,mask)</span><br><span class="line">        norm_1_output = <span class="variable language_">self</span>.layer_norm_1(attention_output + decoder_input)</span><br><span class="line">        ffn_output = <span class="variable language_">self</span>.ffn(norm_1_output)</span><br><span class="line">        norm_2_output = <span class="variable language_">self</span>.layer_norm_2(ffn_output + norm_1_output)</span><br><span class="line">        <span class="keyword">return</span> norm_2_output</span><br></pre></td></tr></table></figure>
<p><strong>根据单层实现GPT的解码器结构</strong><br />
主要添加了前缀的词嵌入层，并定义了<code>n_layer</code>个解码器单层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,corpus</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder,<span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding_layer = nn.Embedding(<span class="built_in">len</span>(corpus.vocab), dim_embedding)</span><br><span class="line">        <span class="variable language_">self</span>.position_embedding = nn.Embedding(<span class="built_in">len</span>(corpus.vocab), dim_embedding)</span><br><span class="line">        <span class="variable language_">self</span>.decoder_layers = nn.ModuleList(DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layer))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,decoder_input</span>):</span><br><span class="line">        pos_indices = torch.arange(<span class="number">1</span>, decoder_input.size(<span class="number">1</span>) + <span class="number">1</span>).unsqueeze(<span class="number">0</span>).to(device)  <span class="comment"># 1,source_len</span></span><br><span class="line">        a = <span class="variable language_">self</span>.embedding_layer(decoder_input)</span><br><span class="line">        b = <span class="variable language_">self</span>.position_embedding(pos_indices)</span><br><span class="line">        embedding_output = a + b</span><br><span class="line">        decoder_output = embedding_output</span><br><span class="line"></span><br><span class="line">        decoder_self_subsequent_mask = get_attention_subsequent_mask(decoder_input).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.decoder_layers:</span><br><span class="line">            decoder_output = layer(decoder_output,decoder_self_subsequent_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decoder_output</span><br></pre></td></tr></table></figure>
<p><strong>最终定义完整的GPT结构</strong><br />
添加输出的线性层，其输出大小为词表的大小，表示下一个词的概率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,corpus</span>):</span><br><span class="line">        <span class="built_in">super</span>(GPT,<span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.decoder = Decoder(corpus)</span><br><span class="line">        <span class="variable language_">self</span>.projection = nn.Linear(dim_embedding, <span class="built_in">len</span>(corpus.vocab), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">        decoder_input = <span class="variable language_">self</span>.decoder(inputs)</span><br><span class="line">        logits = <span class="variable language_">self</span>.projection(decoder_input)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<h2 id="数据准备"><a class="markdownIt-Anchor" href="#数据准备"></a> 数据准备</h2>
<h2 id="集束搜索"><a class="markdownIt-Anchor" href="#集束搜索"></a> 集束搜索</h2>
<h1 id="chapter8chatgpt基于强化学习"><a class="markdownIt-Anchor" href="#chapter8chatgpt基于强化学习"></a> Chapter8：ChatGPT基于强化学习</h1>
<h1 id="chapter8chatgpt基于强化学习-2"><a class="markdownIt-Anchor" href="#chapter8chatgpt基于强化学习-2"></a> Chapter8：ChatGPT基于强化学习</h1>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://duqi666.github.io">Mercik</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://duqi666.github.io/2025/04/16/Diagrammatize_GPT/">https://duqi666.github.io/2025/04/16/Diagrammatize_GPT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0/">学习</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></div><div class="post-share"><div class="social-share" data-image="/imgs/GPT2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/04/20/%E7%AE%97%E6%B3%95/" title="一些算法"><img class="cover" src="/imgs/algorithm.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">一些算法</div></div><div class="info-2"><div class="info-item-1">  排序算法 https://zhuanlan.zhihu.com/p/567134257  冒泡排序 它重复地走访过要排序的元素列，依次比较两个相邻的元素，如果顺序（如从大到小、首字母从Z到A）错误就把他们交换过来。走访元素的工作是重复地进行，直到没有相邻元素需要交换，也就是说该元素列已经排序完成。   比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素做同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。   1234567def BubbleSort(mylist:list):    for z in range(len(mylist)):        #最后的z个已经排好序，不需要再排序了        for i in range(len(mylist) - z-1):            if mylist[i] &gt; mylist[i + 1]:                mylist[i...</div></div></div></a><a class="pagination-related" href="/2025/04/15/hello-world/" title="Hello World"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Hello World</div></div><div class="info-2"><div class="info-item-1">Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.  Quick Start  Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing  Run server 1$ hexo server More info: Server  Generate static files 1$ hexo generate More info: Generating  Deploy to remote sites 1$ hexo deploy More info: Deployment </div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/10/25/Flash-Attention/" title="Flash-Attention算法"><img class="cover" src="/imgs/Flash-Attention.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-25</div><div class="info-item-2">Flash-Attention算法</div></div><div class="info-2"><div class="info-item-1">  前言 学Flash-Attention算法的主要契机是最近在工作中遇到了进行长文本训练时的OOM（Out of Memory）问题，在训练时使用Flash-Attention是尝试过的有效的方法之一，所以就想学一下这个算法的原理。 Paper: https://arxiv.org/pdf/2205.14135 参考：https://www.zhihu.com/question/611236756/answer/3132304304  背景 传统的transformer算法中，对于self-attention的计算是一个比较费时费空间的做法，其存储复杂度是输入文本长度的平方复杂度，所以当输入序列变得很长时训练速度会变得比较慢，且存在OOM的风险，当前许多研究工作着重于减小self-attention的计算复杂度以加快训练速度，减小显存占用。 FlashAttention主要解决Transformer计算速度慢和存储占用高的问题。但与绝大多数Efficient Transformer把改进方法集中在降低模型的FLOPS（floating point operations per ...</div></div></div></a><a class="pagination-related" href="/2025/09/01/PPO/" title="强化学习PPO算法"><img class="cover" src="/imgs/Reinforcement_Learning.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-01</div><div class="info-item-2">强化学习PPO算法</div></div><div class="info-2"><div class="info-item-1"> PPO算法详解：从策略梯度到近端策略优化  目录  策略梯度算法基础 策略梯度定理 REINFORCE算法 Actor-Critic方法 TRPO算法 PPO算法 PPO的数学推导 PPO算法实现 总结   策略梯度算法基础  强化学习基本概念 在强化学习中，我们有一个智能体(Agent)与环境(Environment)交互。在每个时间步ttt：  状态(State): st∈Ss_t \in \mathcal{S}st​∈S 动作(Action): at∈Aa_t \in \mathcal{A}at​∈A 奖励(Reward): rt∈Rr_t \in \mathbb{R}rt​∈R 策略(Policy): π(a∣s)\pi(a|s)π(a∣s) - 在状态sss下选择动作aaa的概率   目标函数 我们的目标是最大化期望累积奖励： J(θ)=Eτ∼πθ[∑t=0∞γtrt]J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right] J(θ)=Eτ...</div></div></div></a><a class="pagination-related" href="/2025/10/27/ZeRO3/" title="DeepSpeed ZeRO技术"><img class="cover" src="/imgs/Flash-Attention.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-27</div><div class="info-item-2">DeepSpeed ZeRO技术</div></div><div class="info-2"><div class="info-item-1">  前言 参考：https://www.bilibili.com/video/BV1mm42137X8/?spm_id_from=333.337.search-card.all.click&amp;vd_source=47a89074dbbb4a7c1909efaecd754e32 ψ\psiψ </div></div></div></a><a class="pagination-related" href="/2025/08/24/Reinforcement_Learning/" title="《深度学习4-强化学习》学习笔记"><img class="cover" src="/imgs/Reinforcement_Learning.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-24</div><div class="info-item-2">《深度学习4-强化学习》学习笔记</div></div><div class="info-2"><div class="info-item-1">  Chapter1：老虎机问题  一般问题 问题描述： 假如有n个老虎机，每个老虎机有一定的得分概率分布，如果你可以玩很多次，每次选择一个老虎机玩，如何做可以使得自己的得分最高？ 数学表示：  奖励：RRR，即老虎机的返回值 行动：aaa，即对老虎机的选择 行动价值：q(a)=E[R∣A]q(a)=\mathbb{E}[R|A]q(a)=E[R∣A]，采取某个行动所得到奖励的期望值，q(a)q(a)q(a)表示真实的期望值，Q(a)Q(a)Q(a)表示估计的期望值  问题的本质是对老虎机概率分布的估计，即估计每台老虎机的QQQ值，简单来说是对老虎机输出的数学期望进行估计，根据大数定律，可以使用输出的平均值估计其数学期望，老虎机输出的平均值计算公式如下所示，随着n增加，其估计理论上也是越来越接近真实数学期望 Qn=R1+R2+R3+...+Rnn=Qn−1+Rn−Qn−1nQ_n=\frac{R_1+R_2+R_3+...+R_n}{n}=Q_{n-1}+\frac{R_n-Q_{n-1}}{n} Qn​=nR1​+R2​+R3​+...+Rn​​=Qn−1​+nRn​−Qn−1...</div></div></div></a><a class="pagination-related" href="/2025/05/05/Reinforcement_Learning_part2/" title="《深度学习4-强化学习》学习笔记-2"><img class="cover" src="/imgs/RL2.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-05</div><div class="info-item-2">《深度学习4-强化学习》学习笔记-2</div></div><div class="info-2"><div class="info-item-1">  Chapter7：神经网络和Q学习 在前面的方法中，我们都是用一个表格形式存储Q函数，但是复杂情况下，状态数量太多，无法将其存储为表格形式，用更加紧凑的函数近似Q函数是一种处理方法，其中最有效的便是神经网络 机器学习和深度学习的基础知识此处省略 将求Q函数的过程抽象为神经网络，有两种形式：  输入状态sss和行动aaa，输出Q函数值Q(s,a)Q(s,a)Q(s,a) 输入状态sss，输入所有行动的Q函数列表  首先我们需要知道神经网络的输入输出，以及其误差函数的设计，这个地方书上写的比较模糊，书上采用了下面这个公式说明，也就是Q学习的迭代公式 Qπ′(St,At)=Qπ(St,At)+α{Rt+γmax⁡a′Qπ(St+1,a′)−Qπ(St,At)}Q_{\pi}^{&#x27;}(S_t,A_t) = Q_{\pi}(S_t,A_t) + \alpha \bigg\{ R_t + \gamma \max_{a^{&#x27;}}Q_{\pi}(S_{t+1},a^{&#x27;})-Q_{\pi}(S_t,A_t)\bigg\} Qπ′​(St​,At​)=Qπ​(St...</div></div></div></a><a class="pagination-related" href="/2025/04/20/%E7%AE%97%E6%B3%95/" title="一些算法"><img class="cover" src="/imgs/algorithm.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-20</div><div class="info-item-2">一些算法</div></div><div class="info-2"><div class="info-item-1">  排序算法 https://zhuanlan.zhihu.com/p/567134257  冒泡排序 它重复地走访过要排序的元素列，依次比较两个相邻的元素，如果顺序（如从大到小、首字母从Z到A）错误就把他们交换过来。走访元素的工作是重复地进行，直到没有相邻元素需要交换，也就是说该元素列已经排序完成。   比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素做同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。   1234567def BubbleSort(mylist:list):    for z in range(len(mylist)):        #最后的z个已经排好序，不需要再排序了        for i in range(len(mylist) - z-1):            if mylist[i] &gt; mylist[i + 1]:                mylist[i...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comments</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/imgs/I.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Mercik</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Duqi666"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#chapter1n-grams-bag-of-words"><span class="toc-number">1.</span> <span class="toc-text"> Chapter1：N-grams &amp; Bag-of-words</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#n-grams%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.</span> <span class="toc-text"> N-grams模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bag-of-words"><span class="toc-number">1.2.</span> <span class="toc-text"> Bag of Words</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#chapter2word2vec"><span class="toc-number">2.</span> <span class="toc-text"> Chapter2：Word2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#skip-gram"><span class="toc-number">2.1.</span> <span class="toc-text"> Skip-Gram</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cbow"><span class="toc-number">2.2.</span> <span class="toc-text"> CBOW</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#chapter3nplm%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text"> Chapter3：NPLM模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#chapter4seq2seq"><span class="toc-number">4.</span> <span class="toc-text"> Chapter4：Seq2Seq</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#chapter5%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">5.</span> <span class="toc-text"> Chapter5：注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9Bdot-product-attention"><span class="toc-number">5.1.</span> <span class="toc-text"> 点积注意力（Dot-Product Attention）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9Bscaled-dot-product-attention"><span class="toc-number">5.2.</span> <span class="toc-text"> 缩放点积注意力（Scaled Dot-Product Attention）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">5.3.</span> <span class="toc-text"> 编码器-解码器注意力</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9Bmulti-head-attention"><span class="toc-number">5.4.</span> <span class="toc-text"> 多头自注意力（Multi-head Attention）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#chapter6transformer"><span class="toc-number">6.</span> <span class="toc-text"> Chapter6：Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90"><span class="toc-number">6.1.</span> <span class="toc-text"> Transformer结构分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8E%A9%E7%A0%81"><span class="toc-number">6.1.1.</span> <span class="toc-text"> 注意力掩码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">6.1.2.</span> <span class="toc-text"> 多头自注意力&amp;残差连接&amp;归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.1.3.</span> <span class="toc-text"> 前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">6.1.4.</span> <span class="toc-text"> 位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder"><span class="toc-number">6.1.5.</span> <span class="toc-text"> Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoder"><span class="toc-number">6.1.6.</span> <span class="toc-text"> Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer"><span class="toc-number">6.1.7.</span> <span class="toc-text"> Transformer</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#chapter7gpt"><span class="toc-number">7.</span> <span class="toc-text"> Chapter7：GPT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#gpt%E6%90%AD%E5%BB%BA"><span class="toc-number">7.1.</span> <span class="toc-text"> GPT搭建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-number">7.2.</span> <span class="toc-text"> 数据准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%9D%9F%E6%90%9C%E7%B4%A2"><span class="toc-number">7.3.</span> <span class="toc-text"> 集束搜索</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#chapter8chatgpt%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">8.</span> <span class="toc-text"> Chapter8：ChatGPT基于强化学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#chapter8chatgpt%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-2"><span class="toc-number">9.</span> <span class="toc-text"> Chapter8：ChatGPT基于强化学习</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/10/27/ZeRO3/" title="DeepSpeed ZeRO技术"><img src="/imgs/Flash-Attention.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DeepSpeed ZeRO技术"/></a><div class="content"><a class="title" href="/2025/10/27/ZeRO3/" title="DeepSpeed ZeRO技术">DeepSpeed ZeRO技术</a><time datetime="2025-10-27T14:18:54.766Z" title="Created 2025-10-27 22:18:54">2025-10-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/25/Flash-Attention/" title="Flash-Attention算法"><img src="/imgs/Flash-Attention.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flash-Attention算法"/></a><div class="content"><a class="title" href="/2025/10/25/Flash-Attention/" title="Flash-Attention算法">Flash-Attention算法</a><time datetime="2025-10-25T08:14:40.626Z" title="Created 2025-10-25 16:14:40">2025-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/01/PPO/" title="强化学习PPO算法"><img src="/imgs/Reinforcement_Learning.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="强化学习PPO算法"/></a><div class="content"><a class="title" href="/2025/09/01/PPO/" title="强化学习PPO算法">强化学习PPO算法</a><time datetime="2025-09-01T13:09:55.171Z" title="Created 2025-09-01 21:09:55">2025-09-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/24/Reinforcement_Learning/" title="《深度学习4-强化学习》学习笔记"><img src="/imgs/Reinforcement_Learning.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="《深度学习4-强化学习》学习笔记"/></a><div class="content"><a class="title" href="/2025/08/24/Reinforcement_Learning/" title="《深度学习4-强化学习》学习笔记">《深度学习4-强化学习》学习笔记</a><time datetime="2025-08-24T07:13:43.417Z" title="Created 2025-08-24 15:13:43">2025-08-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/05/Reinforcement_Learning_part2/" title="《深度学习4-强化学习》学习笔记-2"><img src="/imgs/RL2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="《深度学习4-强化学习》学习笔记-2"/></a><div class="content"><a class="title" href="/2025/05/05/Reinforcement_Learning_part2/" title="《深度学习4-强化学习》学习笔记-2">《深度学习4-强化学习》学习笔记-2</a><time datetime="2025-05-05T06:46:28.344Z" title="Created 2025-05-05 14:46:28">2025-05-05</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By Mercik</span></div><div class="footer_custom_text">I wish you to become your own sun, no need to rely on who's light.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll to Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const initValine = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyValine = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const valineConfig = {
      el: '#vcomment',
      appId: 'uu6tSxmBoCndulFYKGKobx2L-MdYXbMMI',
      appKey: 'jBho9VeFfJwBsfbRDlg0zM6B',
      avatar: '',
      serverURLs: 'https://uu6tsxmb.api.lncldglobal.com',
      emojiMaps: "",
      visitor: false,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || window.location.pathname
    }

    new Valine(valineConfig)
  }

  const loadValine = async (el, path) => {
    if (typeof Valine === 'function') {
      initValine(el, path)
    } else {
      await btf.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js')
      initValine(el, path)
    }
  }

  if (isShuoshuo) {
    'Valine' === 'Valine'
      ? window.shuoshuoComment = { loadComment: loadValine }
      : window.loadOtherComment = loadValine
    return
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><script src="/js/custom.js"></script><script src="/js/mode_change.js"></script><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/js/cat.js"></script><script type="text/javascript" src="https://unpkg.zhimg.com/jquery@latest/dist/jquery.min.js"></script><script async data-pjax src="/js/txmap.js"></script><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://duqi666.github.io/categories/大模型/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 大模型LLM (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://duqi666.github.io/categories/算法/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 算法 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="https://duqi666.github.io/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: var(--btn-bg)}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/10/27/ZeRO3/" alt=""><img width="48" height="48" src="/imgs/Flash-Attention.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-10-27</span><a class="blog-slider__title" href="2025/10/27/ZeRO3/" alt="">DeepSpeed ZeRO技术</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/10/27/ZeRO3/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/10/25/Flash-Attention/" alt=""><img width="48" height="48" src="/imgs/Flash-Attention.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-10-25</span><a class="blog-slider__title" href="2025/10/25/Flash-Attention/" alt="">Flash-Attention算法</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/10/25/Flash-Attention/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/05/05/Reinforcement_Learning_part2/" alt=""><img width="48" height="48" src="/imgs/RL2.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-05-05</span><a class="blog-slider__title" href="2025/05/05/Reinforcement_Learning_part2/" alt="">《深度学习4-强化学习》学习笔记-2</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/05/05/Reinforcement_Learning_part2/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/09/01/PPO/" alt=""><img width="48" height="48" src="/imgs/Reinforcement_Learning.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-09-01</span><a class="blog-slider__title" href="2025/09/01/PPO/" alt="">强化学习PPO算法</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/09/01/PPO/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/08/24/Reinforcement_Learning/" alt=""><img width="48" height="48" src="/imgs/Reinforcement_Learning.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-08-24</span><a class="blog-slider__title" href="2025/08/24/Reinforcement_Learning/" alt="">《深度学习4-强化学习》学习笔记</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/08/24/Reinforcement_Learning/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/04/20/算法/" alt=""><img width="48" height="48" src="/imgs/algorithm.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-20</span><a class="blog-slider__title" href="2025/04/20/算法/" alt="">一些算法</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/04/20/算法/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/04/16/Diagrammatize_GPT/" alt=""><img width="48" height="48" src="/imgs/GPT2.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-16</span><a class="blog-slider__title" href="2025/04/16/Diagrammatize_GPT/" alt="">《GPT图解》学习笔记</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/04/16/Diagrammatize_GPT/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1s');
    arr[i].setAttribute('data-wow-delay', '0s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1s');
    arr[i].setAttribute('data-wow-delay', '0s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('post-class');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '0.5s');
    arr[i].setAttribute('data-wow-delay', '0s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('archive-class');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1s');
    arr[i].setAttribute('data-wow-delay', '0s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('page-class');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1s');
    arr[i].setAttribute('data-wow-delay', '0s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><!-- hexo injector body_end end --></body></html>