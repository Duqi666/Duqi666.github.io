<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>《深度学习4-强化学习》学习笔记 | Merick'Blog</title><meta name="author" content="Mercik"><meta name="copyright" content="Mercik"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Chapter1：老虎机问题一般问题问题描述： 假如有n个老虎机，每个老虎机有一定的得分概率分布，如果你可以玩很多次，每次选择一个老虎机玩，如何做可以使得自己的得分最高？ 数学表示：  奖励：$R$，即老虎机的返回值 行动：$a$，即对老虎机的选择 行动价值：$q(a)&#x3D;\mathbb{E}[R|A]$，采取某个行动所得到奖励的期望值，$q(a)$表示真实的期望值，$Q(a)$表示估计的期望值">
<meta property="og:type" content="article">
<meta property="og:title" content="《深度学习4-强化学习》学习笔记">
<meta property="og:url" content="https://duqi666.github.io/2025/08/24/Reinforcement_Learning/index.html">
<meta property="og:site_name" content="Merick&#39;Blog">
<meta property="og:description" content="Chapter1：老虎机问题一般问题问题描述： 假如有n个老虎机，每个老虎机有一定的得分概率分布，如果你可以玩很多次，每次选择一个老虎机玩，如何做可以使得自己的得分最高？ 数学表示：  奖励：$R$，即老虎机的返回值 行动：$a$，即对老虎机的选择 行动价值：$q(a)&#x3D;\mathbb{E}[R|A]$，采取某个行动所得到奖励的期望值，$q(a)$表示真实的期望值，$Q(a)$表示估计的期望值">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://duqi666.github.io/imgs/Reinforcement_Learning.png">
<meta property="article:published_time" content="2025-08-24T07:13:43.417Z">
<meta property="article:modified_time" content="2025-08-24T13:55:43.383Z">
<meta property="article:author" content="Mercik">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://duqi666.github.io/imgs/Reinforcement_Learning.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "《深度学习4-强化学习》学习笔记",
  "url": "https://duqi666.github.io/2025/08/24/Reinforcement_Learning/",
  "image": "https://duqi666.github.io/imgs/Reinforcement_Learning.png",
  "datePublished": "2025-08-24T07:13:43.417Z",
  "dateModified": "2025-08-24T13:55:43.383Z",
  "author": [
    {
      "@type": "Person",
      "name": "Mercik",
      "url": "https://duqi666.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://duqi666.github.io/2025/08/24/Reinforcement_Learning/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '《深度学习4-强化学习》学习笔记',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/universe.css"><link rel="stylesheet" href="/css/category.css"><link rel="stylesheet" href="/css/cat.css"><link rel="stylesheet" href="/css/neon_lamp.css"><link rel="stylesheet" href="/css/discuss.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      if ($loadingBox.classList.contains('loaded')) return
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()

  if (document.readyState === 'complete') {
    preloader.endLoading()
  } else {
    window.addEventListener('load', preloader.endLoading)
    document.addEventListener('DOMContentLoaded', preloader.endLoading)
    // Add timeout protection: force end after 7 seconds
    setTimeout(preloader.endLoading, 7000)
  }

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/imgs/I.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 模式</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="javascript:void(0)"><i class="fa-fw fa-regular fa-moon"></i><span> dark</span></a></li><li><a class="site-page child" href="javascript:void(1)"><i class="fa-fw fa-regular fa-sun"></i><span> light</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/travel/"><i class="fa-fw fas fa-video"></i><span> 足迹</span></a></div><div class="menus_item"><a class="site-page" href="/charts/"><i class="fa-fw fas fa-video"></i><span> 图表</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/imgs/Reinforcement_Learning.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Merick'Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">《深度学习4-强化学习》学习笔记</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 模式</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="javascript:void(0)"><i class="fa-fw fa-regular fa-moon"></i><span> dark</span></a></li><li><a class="site-page child" href="javascript:void(1)"><i class="fa-fw fa-regular fa-sun"></i><span> light</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/travel/"><i class="fa-fw fas fa-video"></i><span> 足迹</span></a></div><div class="menus_item"><a class="site-page" href="/charts/"><i class="fa-fw fas fa-video"></i><span> 图表</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">《深度学习4-强化学习》学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-08-24T07:13:43.417Z" title="Created 2025-08-24 15:13:43">2025-08-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-08-24T13:55:43.383Z" title="Updated 2025-08-24 21:55:43">2025-08-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">10.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>46mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><meta name="referrer" content="no-referrer" />

<h1 id="Chapter1：老虎机问题"><a href="#Chapter1：老虎机问题" class="headerlink" title="Chapter1：老虎机问题"></a>Chapter1：老虎机问题</h1><h2 id="一般问题"><a href="#一般问题" class="headerlink" title="一般问题"></a>一般问题</h2><p><strong>问题描述：</strong> 假如有<em>n</em>个老虎机，每个老虎机有一定的得分概率分布，如果你可以玩很多次，每次选择一个老虎机玩，如何做可以使得自己的得分最高？</p>
<p><strong>数学表示：</strong></p>
<ul>
<li>奖励：$R$，即老虎机的返回值</li>
<li>行动：$a$，即对老虎机的选择</li>
<li>行动价值：$q(a)=\mathbb{E}[R|A]$，采取某个行动所得到奖励的期望值，$q(a)$表示真实的期望值，$Q(a)$表示估计的期望值</li>
</ul>
<p>问题的本质是<strong>对老虎机概率分布的估计</strong>，即估计每台老虎机的$Q$值，简单来说是对老虎机输出的数学期望进行估计，根据<strong>大数定律</strong>，可以使用输出的<strong>平均值</strong>估计其数学期望，老虎机输出的平均值计算公式如下所示，随着n增加，其估计理论上也是越来越接近真实数学期望</p>
<script type="math/tex; mode=display">
Q_n=\frac{R_1+R_2+R_3+...+R_n}{n}=Q_{n-1}+\frac{R_n-Q_{n-1}}{n}</script><p><strong>Bandit类实现：</strong></p>
<ul>
<li><code>rate</code>代表输出概率列表，列表的索引是输出的值，例如[0.2,0.4,0.4]，则其0.2的概率输出0，以此类推，<code>rand</code>控制概率列表的长度</li>
<li><code>value</code>代表输出的<strong>数学期望</strong></li>
<li><code>play</code>函数表示玩一次老虎机，按照概率随机输出</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">11</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bandit</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        rand = np.random.rand(<span class="number">10</span>)</span><br><span class="line">        sum_rand = <span class="built_in">sum</span>(rand)</span><br><span class="line">        <span class="variable language_">self</span>.rate = [i/sum_rand <span class="keyword">for</span> i <span class="keyword">in</span> rand]</span><br><span class="line">        <span class="variable language_">self</span>.value = <span class="built_in">sum</span>([index*i <span class="keyword">for</span> i,index <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.rate)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">play</span>(<span class="params">self</span>):</span><br><span class="line">        rand = np.random.rand()</span><br><span class="line">        <span class="keyword">for</span> index,value <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.rate):</span><br><span class="line">            rand-=value</span><br><span class="line">            <span class="keyword">if</span> rand&lt;<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> index</span><br></pre></td></tr></table></figure>
<p><strong>Agent类实现：</strong></p>
<ul>
<li><code>Q_list</code>表示对老虎机输出数学期望的估计</li>
<li><code>n_list</code>记录玩老虎机的次数</li>
<li><code>epsilon</code>表示探索的概率，如果每次都是选择Q值最高的老虎机，则会陷入局部最优。比如最开始Q值都是0，现在随机选择一个老虎机，其Q值会变的大于0，如果每次选择Q最高的老虎机，则会一直选择这个老虎机，导致无法获得最优解。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Agent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,epsilon,action_size</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Q_list = np.zeros(action_size)</span><br><span class="line">        <span class="variable language_">self</span>.n_list = np.zeros(action_size)</span><br><span class="line">        <span class="variable language_">self</span>.epsilon = epsilon</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self,action,reward</span>):</span><br><span class="line">        <span class="variable language_">self</span>.n_list[action]+=<span class="number">1</span></span><br><span class="line">        <span class="variable language_">self</span>.Q_list[action]+=(reward-<span class="variable language_">self</span>.Q_list[action])/<span class="variable language_">self</span>.n_list[action]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decide_action</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> np.random.rand()&lt;<span class="variable language_">self</span>.epsilon:</span><br><span class="line">            <span class="keyword">return</span> np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(<span class="variable language_">self</span>.Q_list))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> np.argmax(<span class="variable language_">self</span>.Q_list)</span><br></pre></td></tr></table></figure>
<p><strong>实践操作：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">steps = <span class="number">1000</span></span><br><span class="line">num_of_bandit = <span class="number">10</span></span><br><span class="line">bandits = [Bandit() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_of_bandit)]</span><br><span class="line">bandits_values = [bandit.value <span class="keyword">for</span> bandit <span class="keyword">in</span> bandits]</span><br><span class="line">best_choice = np.argmax(bandits_values)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">agent = Agent(<span class="number">0.1</span>,num_of_bandit)</span><br><span class="line">win_rate = []</span><br><span class="line">rewards = []</span><br><span class="line"></span><br><span class="line">total_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">    action = agent.decide_action()</span><br><span class="line">    reward = bandits[action].play()</span><br><span class="line">    agent.update(action,reward)</span><br><span class="line">    total_reward+=reward</span><br><span class="line">    rewards.append(total_reward)</span><br><span class="line">    win_rate.append(total_reward/(n+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(total_reward)</span><br><span class="line"><span class="built_in">print</span>(bandits_values)</span><br><span class="line">plt.plot(win_rate)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>理论上其得分的平均值会慢慢趋近于数学期望最高的老虎机的数学期望：</p>
<!-- <p align="center"><img src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/d0c2f8c306ac444788b44c1db7bb8497~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg54us5oap:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMzg1NTMwNjg3OTkzMzU4MiJ9&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1747028962&x-orig-sign=z3ywlzLtPcySv7gpK8Q%2B2XOHbz0%3D" alt="image.png" width="70%"></p> -->
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202505051427942.png" height="400px" /></p>
<h2 id="非稳态问题"><a href="#非稳态问题" class="headerlink" title="非稳态问题"></a>非稳态问题</h2><p>上个例子中，老虎机的概率分布不会变化，属于<strong>稳态问题</strong>，而真实场景中，环境的状态会发生改变，也就是老虎机的概率分布会变化，这叫<strong>非稳态问题</strong></p>
<p>对于非稳态问题，Q值的更新方式变化为：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
Q_n&=Q_{n-1}+\alpha(R_n-Q_{n-1})
\\
&= \alpha R_n+\alpha (1-\alpha)R_{n-1}+...\alpha (1-\alpha)^{n-1} R_1+\alpha (1-\alpha)^nQ_0
\end{aligned}</script><p>在稳态问题中，使用平均值对$Q_n$进行更新，体现为所有时刻的$R$对其权值都是一样的，都是$1/n$,而在非稳态问题中，我们希望最近的$R$对$Q_n$的影响更大，所以使用$\alpha$将之前的$R$的权重变小，这种权重指数级减少的计算，称为<strong>指数移动平均</strong>。</p>
<p><strong>NonStatBandit类实现：</strong></p>
<ul>
<li>使用<code>change_rate</code>在每一次调用<code>play</code>时，给老虎机的概率分布增加噪声</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NonStatBandit</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        rand = np.random.rand(<span class="number">10</span>)</span><br><span class="line">        sum_rand = <span class="built_in">sum</span>(rand)</span><br><span class="line">        <span class="variable language_">self</span>.rate = [i/sum_rand <span class="keyword">for</span> i <span class="keyword">in</span> rand]</span><br><span class="line">        <span class="variable language_">self</span>.value = <span class="built_in">sum</span>([index*i <span class="keyword">for</span> i,index <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.rate)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">play</span>(<span class="params">self</span>):</span><br><span class="line">        rand = np.random.rand()</span><br><span class="line"></span><br><span class="line">        change_rate = np.random.randn(<span class="number">10</span>)</span><br><span class="line">        avg_change_rate = np.average(change_rate)</span><br><span class="line">        change_rate = [<span class="number">0.1</span>*(i-avg_change_rate)  <span class="keyword">for</span> i <span class="keyword">in</span> change_rate]</span><br><span class="line">        <span class="variable language_">self</span>.rate = [<span class="variable language_">self</span>.rate[i]+change_rate[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.rate))]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> index,value <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.rate):</span><br><span class="line">            rand-=value</span><br><span class="line">            <span class="keyword">if</span> rand&lt;<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> index</span><br></pre></td></tr></table></figure>
<p><strong>AlphaAgent类实现：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AlphaAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,epsilon,action_size,alpha</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Q_list = np.zeros(action_size)</span><br><span class="line">        <span class="variable language_">self</span>.epsilon = epsilon</span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self,action,reward</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Q_list[action]+=(reward-<span class="variable language_">self</span>.Q_list[action])*<span class="variable language_">self</span>.alpha</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decide_action</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> np.random.rand()&lt;<span class="variable language_">self</span>.epsilon:</span><br><span class="line">            <span class="keyword">return</span> np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(<span class="variable language_">self</span>.Q_list))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> np.argmax(<span class="variable language_">self</span>.Q_list)</span><br></pre></td></tr></table></figure>
<p><strong>两种方法对比：</strong></p>
<p>这里每种方法都进行多次实验，次数为<code>runs</code>，每次实验的步数为<code>steps</code>,统计不同实验下，每一个<code>step</code>的收益平均值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">steps = <span class="number">1000</span></span><br><span class="line">runs = <span class="number">200</span></span><br><span class="line">all_rates = np.zeros((runs,steps))</span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> <span class="built_in">range</span>(runs):</span><br><span class="line">    steps = <span class="number">1000</span></span><br><span class="line">    num_of_bandit = <span class="number">10</span></span><br><span class="line">    <span class="comment"># bandits = [Bandit() for i in range(num_of_bandit)]</span></span><br><span class="line">    bandits = [NonStatBandit() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_of_bandit)]</span><br><span class="line">    agent = Agent(<span class="number">0.1</span>,num_of_bandit)</span><br><span class="line">    win_rate = []</span><br><span class="line">    rewards = []</span><br><span class="line">    total_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">        action = agent.decide_action()</span><br><span class="line">        reward = bandits[action].play()</span><br><span class="line">        agent.update(action,reward)</span><br><span class="line">        total_reward+=reward</span><br><span class="line">        rewards.append(total_reward)</span><br><span class="line">        win_rate.append(total_reward/(n+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    all_rates[run] = win_rate</span><br><span class="line"></span><br><span class="line">avg_rates = np.average(all_rates,axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(avg_rates)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">all_rates_nonstat = np.zeros((runs,steps))</span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> <span class="built_in">range</span>(runs):</span><br><span class="line">    steps = <span class="number">1000</span></span><br><span class="line">    num_of_bandit = <span class="number">10</span></span><br><span class="line">    bandits = [NonStatBandit() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_of_bandit)]</span><br><span class="line">    agent = AlphaAgent(<span class="number">0.1</span>,num_of_bandit,<span class="number">0.8</span>)</span><br><span class="line">    win_rate = []</span><br><span class="line">    rewards = []</span><br><span class="line">    total_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">        action = agent.decide_action()</span><br><span class="line">        reward = bandits[action].play()</span><br><span class="line">        agent.update(action,reward)</span><br><span class="line">        total_reward+=reward</span><br><span class="line">        rewards.append(total_reward)</span><br><span class="line">        win_rate.append(total_reward/(n+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    all_rates_nonstat[run] = win_rate</span><br><span class="line"></span><br><span class="line">avg_rates_nonsta = np.average(all_rates_nonstat,axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(avg_rates,label =<span class="string">&#x27;sample average&#x27;</span>)</span><br><span class="line">plt.plot(avg_rates_nonsta,label =<span class="string">&#x27;alpha const update&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>可以看到，使用移动平均的效果更好：</p>
<!-- <p align="center"><img src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/7c4b05a9a7f04678b804c25f28a829c8~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg54us5oap:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMzg1NTMwNjg3OTkzMzU4MiJ9&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1747028962&x-orig-sign=oKoeqmYndkmGhQr4L4v83sx%2BPCs%3D" alt="image.png" width="70%"></p> -->
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202505051444520.png" height="400px" /></p>
<h1 id="Chapter2：马尔可夫决策过程"><a href="#Chapter2：马尔可夫决策过程" class="headerlink" title="Chapter2：马尔可夫决策过程"></a>Chapter2：马尔可夫决策过程</h1><p>本章讨论的是<strong>environment</strong>随<strong>Agent</strong>的<strong>Action</strong>而发生变化的问题</p>
<h2 id="数学表达"><a href="#数学表达" class="headerlink" title="数学表达"></a>数学表达</h2><ul>
<li><strong>状态迁移函数：</strong> 给定当前状态$s$和行动$a$，输出下一个状态$s^{‘}$的函数，既$s^{‘}=f(s,a)$</li>
<li><strong>状态迁移概率：</strong> 给定当前状态$s$和行动$a$，输出下一个状态$s^{‘}$的概率，既$P(s^{‘}|s,a)$，因为即使采取了行动，下个状态不是确定的，比如当确定向左移动，但是因为打滑之类的问题而没有发生移动<ul>
<li>状态迁移需要上一时刻的信息，既$s^{‘}$只收到状态$s$和行动$a$的影响，这是<strong>马尔可夫性</strong>的体现</li>
</ul>
</li>
<li><strong>奖励函数：</strong> $r(s,a,s^{‘})$，在下面的例子中，$r(s,a,s^{‘})=r(s^{‘})$<ul>
<li>奖励函数不是确定的，比如在$s^{‘}$时，0.8的几率被攻击，奖励为-10等</li>
</ul>
</li>
<li><strong>策略：</strong> 只基于<strong>当前状态</strong>对下一步行动的确定，使用$\pi(a|s)$表示在$s$状态下采取$a$行动的概率</li>
</ul>
<h2 id="MDP的目标"><a href="#MDP的目标" class="headerlink" title="MDP的目标"></a>MDP的目标</h2><ul>
<li><strong>回合制任务：</strong> 有结束的任务，例如下围棋</li>
<li><strong>连续性任务：</strong> 没有结束的任务，例如仓库存储</li>
</ul>
<h2 id="收益（return）"><a href="#收益（return）" class="headerlink" title="收益（return）"></a>收益（return）</h2><p>收益是指在给定策略时，某个状态下，未来时间内的奖励的总和，公式表示为：</p>
<script type="math/tex; mode=display">
G_t=R_t+\gamma R_{t+1} + \gamma ^2 R_{t+2}+\gamma ^3 R_{t+3}...</script><p>$\gamma$被称为<code>折现率（discount rate）</code>，具体原因有两个：</p>
<ul>
<li>防止在连续性任务重，收益无限大的情况</li>
<li>收益实质上是对未来收益的一种预测，显然距离$t$越远的预测是越不可信的，且这种设置使得收益更关注当前的收益，这是一种<strong>贪心</strong>的体现</li>
</ul>
<p>策略或状态变化在一些问题中是随机的，所以我们需要使用<strong>数学期望</strong>描述收益，称为<strong>状态价值函数</strong>：</p>
<script type="math/tex; mode=display">
\nu _{\pi}(s) = \mathbb{E}_{\pi}[G_t|S_t = s]</script><p><strong>策略</strong>$\pi$和<strong>初始状态</strong>$s$是需要给定的，然后计算这个条件下未来的收益的数学期望，显然我们需要找到使得这个期望最大的策略$\pi$</p>
<p><strong>最优策略：</strong> 如果策略$\pi ^{‘}$在任意的$s$下，$\nu _{\pi^{‘}}(s)$都是最大的，那么$\pi ^{‘}$为最优策略。<strong>可以证明在MDP问题中，一定存在一个最优策略</strong></p>
<p>这里举一个例子，只存在两个状态，当吃到苹果时+1，苹果会无限刷新，撞到墙-1：</p>
<!-- <p align="center"><img src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/08174fc2ba604fd38c05daa43a963df1~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg54us5oap:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMzg1NTMwNjg3OTkzMzU4MiJ9&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1747028962&x-orig-sign=ZWLLSjIDJqlTbovhQKvJVG%2FYoGY%3D" alt="Screenshot_20241202_144609_com.newskyer.draw.jpg" width="50%"></p> -->
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202505051807716.png" height="260px" /></p>
<p>策略一共有四种，值得注意的是，<strong>这个例子中状态转移和策略都是确定的</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>$s=L1$</th>
<th>$s=L2$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\pi_1(s)$</td>
<td>Right</td>
<td>Right</td>
</tr>
<tr>
<td>$\pi_2(s)$</td>
<td>Right</td>
<td>Left</td>
</tr>
<tr>
<td>$\pi_3(s)$</td>
<td>Left</td>
<td>Right</td>
</tr>
<tr>
<td>$\pi_4(s)$</td>
<td>Left</td>
<td>Left</td>
</tr>
</tbody>
</table>
</div>
<p>随便算一个,假设$\gamma=0.9$</p>
<script type="math/tex; mode=display">
\nu_{\pi_1}(s=L1) = 1+0.9×(-1)+0.9^2×(-1)...=-8</script><p>计算所有策略，很显然$\pi_2(s)$是最优策略</p>
<h1 id="Chapter3：贝尔曼方程"><a href="#Chapter3：贝尔曼方程" class="headerlink" title="Chapter3：贝尔曼方程"></a>Chapter3：贝尔曼方程</h1><p>上面的例子中，状态转移和策略都是确定的，当面临随机性时，我们不能使用上面的方法计算，而需要使用<strong>贝尔曼方程</strong>。</p>
<h2 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h2><p>回忆收益公式：</p>
<script type="math/tex; mode=display">
G_t=R_t+\gamma R_{t+1} + \gamma ^2 R_{t+2}+\gamma ^3 R_{t+3}... = R_t +\gamma G_{t+1}</script><p>那么状态价值函数可以写为：</p>
<script type="math/tex; mode=display">
\nu _{\pi}(s) = \mathbb{E}_{\pi}[G_t|S_t = s] = \mathbb{E}_{\pi}[R_t|S_t = s]+\gamma \mathbb{E}_{\pi}[G_{t+1}|S_t = s]</script><p>其中</p>
<script type="math/tex; mode=display">
\mathbb{E}_{\pi}[R_t|S_t = s] = \sum_{a} \sum_{s^{'}} \pi(a|s)p(s^{'}|s,a)r(s,a,s^{'})
\\
\mathbb{E}_{\pi}[G_{t+1}|S_t = s] = \sum_{a} \sum_{s^{'}}\pi(a|s)p(s^{'}|s,a)\mathbb{E}_{\pi}[G_{t+1}|S_{t+1} = s]</script><ul>
<li>第一个部分：本质上就是求出每一种可能性的概率及其奖励，然后加和</li>
<li>第二个部分：确定初始时刻状态，下一时刻开始的收益，应该等于确认下一时刻状态，下一时刻收益×初始时刻到下一时刻状态的概率2</li>
</ul>
<p>那么<strong>贝尔曼方程</strong>为：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\nu _{\pi}(s) &= \mathbb{E}_{\pi}[R_t|S_t = s]+\gamma \mathbb{E}_{\pi}[G_{t+1}|S_t = s]
\\&=\sum_{a} \sum_{s^{'}} \pi(a|s)p(s^{'}|s,a)r(s,a,s^{'})+\gamma \sum_{a} \sum_{s^{'}}\pi(a|s)p(s^{'}|s,a)\nu _{\pi}(s^{'})
\\&=\sum_{a} \sum_{s^{'}}\pi(a|s)p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{\pi}s^{'}\}
\end{aligned}</script><p><strong>贝尔曼方程表示的是“当前状态$s$的价值函数$\nu<em>{\pi}(s)$”和“下一状态$s^{‘}$的价值函数$\nu</em>{\pi}(s^{‘})$”之间的关系，其最大的意义在于将无限的计算转换为有限的联立方程。</strong></p>
<p>对于上面那个例子，假设策略为0.5概率向左移动，0.5向右移动，采用贝尔曼方程求解，这里由于状态转移是固定的，那么当$s^{‘}=f(s,a)$时，贝尔曼方程可以写为：</p>
<script type="math/tex; mode=display">
\nu _{\pi}(s)=\sum_{a} \pi(a|s)\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'})\}</script><p>这里行动只有往左或者往右，那么其还可写成：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\nu _{\pi}(s)=&\pi(a=Left|s)\{r(s,a=Left,s^{'})+\gamma \nu _{\pi}(s^{'}\}
\\&+\pi(a=Right|s)\{r(s,a=Right,s^{'})+\gamma \nu _{\pi}(s^{'})\}
\end{aligned}</script><p>那么：</p>
<script type="math/tex; mode=display">
\nu _{\pi}(L1)=0.5\{-1+0.9\nu _{\pi}(L1)\}+0.5\{1+0.9\nu _{\pi}(L2)\}
\\
\nu _{\pi}(L2)=0.5\{0+0.9\nu _{\pi}(L1)\}+0.5\{-1+0.9\nu _{\pi}(L2)\}</script><p>求解可得，$\nu <em>{\pi}(L1)=-2.25, \nu </em>{\pi}(L2)=-2.75$</p>
<h2 id="行动价值函数（Q函数）"><a href="#行动价值函数（Q函数）" class="headerlink" title="行动价值函数（Q函数）"></a>行动价值函数（Q函数）</h2><p>状态价值函数需要两个条件，即<strong>策略</strong>和<strong>状态</strong>，在这个基础上再考虑一个条件，也就是第一次的<strong>行动$a$</strong>，就构成了<strong>Q函数：</strong></p>
<script type="math/tex; mode=display">
q_{\pi}(s,a) = \mathbb{E}_{\pi}[G_{t+1}|S_t = s,A_t = a]</script><p>Q函数中行动的选择不受到策略的影响，是自己选择的，后面的行动就是按照策略来了，那么显然Q函数和状态价值函数有如下关系：</p>
<script type="math/tex; mode=display">
\nu _{\pi}(s) = \sum_{a}\pi(a|s)q_{\pi}(s,a)</script><p>按照同样的分析，可以获得Q函数的表达式：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
q_{\pi}(s,a)&=\mathbb{E}_{\pi}[R_t|S_t = s,A_t = a]+\gamma \mathbb{E}_{\pi}[G_{t+1}|S_t = s,A_t = a]
\\&=\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'})\}
\end{aligned}</script><p>那么，<strong>使用Q函数的贝尔曼方程</strong>为：</p>
<script type="math/tex; mode=display">
q_{\pi}(s,a)=\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \sum_{a^{'}}\pi(a^{'}|s^{'})q_{\pi}(s^{'},a^{'})\}</script><h2 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h2><p>回忆贝尔曼方程：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\nu _{\pi}(s) =\sum_{a} \pi(a|s)\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'})\}
\end{aligned}</script><p>最优策略应该使得$\sum<em>{s^{‘}}p(s^{‘}|s,a){r(s,a,s^{‘})+\gamma \nu </em>{\pi}(s^{‘}}$最大（因为最优策略是确定的，那么$\pi(a|s)$部分可以省略），于是<strong>贝尔曼最优方程为：</strong></p>
<script type="math/tex; mode=display">
\begin{aligned} 
\nu _{*}(s) =\max_a \sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{*}(s^{'})\}
\end{aligned}</script><p>同理，<strong>Q函数的贝尔曼最优方程</strong>为：</p>
<script type="math/tex; mode=display">

q_{*}(s,a)=\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \max_{a^{'}} q_{*}(s^{'},a^{'})\}</script><p><strong>最优策略</strong>应该是可以使得价值函数最大的策略，用数学语言描述就是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi_{*}(s) &= \argmax_{a}\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{*}(s^{'})\}
\\&=\argmax_{a}q_{*}(s,a)
\end{aligned}</script><p><strong>最优收益的获取：</strong></p>
<p>还是之前的例子，由于状态转移是确定的，那么最优贝尔曼方程可以写成：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\nu _{*}(s) =\max_a\{r(s,a,s^{'})+\gamma \nu _{*}(s^{'})\}
\end{aligned}</script><p>于是：</p>
<script type="math/tex; mode=display">
\nu_{*}(L1)=max\{-1+0.9\nu_{*}(L1),1+0.9\nu_{*}(L2)\}\\
\nu_{*}(L2)=max\{-1+0.9\nu_{*}(L2),1+0.9\nu_{*}(L1)\}</script><p>求解可得，$\nu<em>{*}(L1)=5.26,\nu</em>{*}(L2)=4.73$</p>
<p><strong>最优策略的获取：</strong></p>
<p>最优策略$\pi_{*}(s)$表示为：</p>
<script type="math/tex; mode=display">
\pi_{*}(s) = \argmax_{a}(\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'})\})</script><p>在这个例子中，当$s=L1$时，如果其行动是往左走，则：</p>
<script type="math/tex; mode=display">
\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'})\}=-1+0.9×5.26=3.734</script><p>如果往右走，则：</p>
<script type="math/tex; mode=display">
\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'})\}=1+0.9×4.73=5.257</script><p>显然，$\pi<em>{*}(L1)=Right$，同理$\pi</em>{*}(L2)=Left$</p>
<h2 id="总计"><a href="#总计" class="headerlink" title="总计"></a>总计</h2><p><strong>【贝尔曼方程】</strong></p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\nu _{\pi}(s) =\sum_{a} \sum_{s^{'}}\pi(a|s)p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'}\}
\\&q_{\pi}(s,a)=\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \sum_{a^{'}}\pi(a^{'}|s^{'})q_{\pi}(s^{'},a^{'})\}
\end{aligned}</script><p><strong>【贝尔曼最优方程】</strong></p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\nu _{*}(s) =\max_a \sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{*}(s^{'})\}
\\&q_{*}(s,a)=\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \max_{a^{'}} q_{*}(s^{'},a^{'})\}
\end{aligned}</script><p><strong>【最优策略】</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi_{*}(s) &= \argmax_{a}\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{*}(s^{'})\}
\\&=\argmax_{a}q_{*}(s,a)
\end{aligned}</script><h1 id="Chapter4：动态规划法"><a href="#Chapter4：动态规划法" class="headerlink" title="Chapter4：动态规划法"></a>Chapter4：动态规划法</h1><p>按照上面的做法，得到最优贝尔曼方程的解需要解一个联立方程，在上述的例子中环境只有两个，所有这个联立方程可求，当环境变复杂，问题变得多变，解方程的计算量<strong>指数级上升</strong>，所以我们需要用<strong>动态规划</strong>的方法计算<strong>最优贝尔曼方程</strong>或<strong>最优策略</strong>。</p>
<p>在强化学习中通常设计两项任务：</p>
<ul>
<li><strong>策略评估：</strong> 求给定策略$\pi$的价值函数$\nu _{\pi}(s)$</li>
<li><strong>策略控制：</strong> 控制策略并将其调整为最优策略</li>
</ul>
<h2 id="迭代策略评估"><a href="#迭代策略评估" class="headerlink" title="迭代策略评估"></a>迭代策略评估</h2><p>回忆贝尔曼方程：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\nu _{\pi}(s) =\sum_{a} \pi(a|s)\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'})\}
\end{aligned}</script><p>使用DP进行策略评估的思路是从贝尔曼方程衍生出来的，思路是将贝尔曼方程变形为“<strong>更新式</strong>”，表达为：</p>
<script type="math/tex; mode=display">
V_{k+1}(s)=\sum_{a} \sum_{s^{'}}\pi(a|s)p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma V _{k}(s^{'})\}</script><p>主要特点是用<strong>下一个状态的当前迭代的价值函数</strong>$V <em>{k}(s^{‘})$来更新<strong>当前状态的下一个迭代的价值函数</strong>$V</em>{k+1}(s)$</p>
<p>例如对于上面那个例子，一开始的初始价值函数为$V_0(L1)$和$V_0(L2)$，这个$0$代表第0次迭代，然后用上面那个公式计算$V_1(L1)$和$V_2(L2)$，以此类推。</p>
<blockquote>
<p><strong>如何证明这个迭代式的有效性？(来自豆包)</strong><br><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202505052042849.png" height="1200px" /></p>
</blockquote>
<p>在上面的例子中，不存在状态转移概率，所有可以简写为：</p>
<script type="math/tex; mode=display">
V_{k+1}(s)=\sum_{a}\pi(a|s)\{r(s,a,s^{'})+\gamma V _{k}(s^{'})\}</script><p>对上面的例子进行迭代评估实验：</p>
<ul>
<li>这里使用<code>delta</code>表征上下两次迭代的改变量，当改变量小于阈值时，停止迭代</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">V = &#123;<span class="string">&#x27;l1&#x27;</span>:<span class="number">0</span>,<span class="string">&#x27;l2&#x27;</span>:<span class="number">0</span>&#125;</span><br><span class="line">new_V = V.copy()</span><br><span class="line">n = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    new_V[<span class="string">&#x27;l1&#x27;</span>] = <span class="number">0.5</span>*(-<span class="number">1</span>+<span class="number">0.9</span>*V[<span class="string">&#x27;l1&#x27;</span>])+<span class="number">0.5</span>*(<span class="number">1</span>+<span class="number">0.9</span>*V[<span class="string">&#x27;l2&#x27;</span>])</span><br><span class="line">    new_V[<span class="string">&#x27;l2&#x27;</span>] = <span class="number">0.5</span> * (<span class="number">0</span> + <span class="number">0.9</span> * V[<span class="string">&#x27;l1&#x27;</span>]) + <span class="number">0.5</span> * (-<span class="number">1</span> + <span class="number">0.9</span> * V[<span class="string">&#x27;l2&#x27;</span>])</span><br><span class="line">    delta = <span class="built_in">max</span>(<span class="built_in">abs</span>(new_V[<span class="string">&#x27;l1&#x27;</span>]-V[<span class="string">&#x27;l1&#x27;</span>]),<span class="built_in">abs</span>(new_V[<span class="string">&#x27;l2&#x27;</span>]-V[<span class="string">&#x27;l2&#x27;</span>]))</span><br><span class="line">    V = new_V.copy()</span><br><span class="line">    n+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> delta&lt;<span class="number">0.0001</span>:</span><br><span class="line">        <span class="built_in">print</span>(n)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(V)</span><br><span class="line"><span class="comment"># 77</span></span><br><span class="line"><span class="comment"># &#123;&#x27;l1&#x27;: -2.249167525908671, &#x27;l2&#x27;: -2.749167525908671&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="迭代策略评估的其他方式-覆盖方式"><a href="#迭代策略评估的其他方式-覆盖方式" class="headerlink" title="迭代策略评估的其他方式-覆盖方式"></a>迭代策略评估的其他方式-覆盖方式</h2><ul>
<li><strong>上述方式</strong>：上面的方法是全部计算出新迭代的所有状态的价值函数，再进行下一轮迭代</li>
<li><strong>覆盖方式</strong>：在计算出新的某个状态的价值函数后，直接将其用于计算其他状态的价值函数。<ul>
<li>例如我们先利用$V_0(L1)$和$V_0(L2)$计算出了$V_1(L1)$，然后利用$V_1(L1)$和$V_0(L2)$计算$V_1(L2)$，以此类推。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">V = &#123;<span class="string">&#x27;l1&#x27;</span>:<span class="number">0</span>,<span class="string">&#x27;l2&#x27;</span>:<span class="number">0</span>&#125;</span><br><span class="line">n = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    t = <span class="number">0.5</span>*(-<span class="number">1</span>+<span class="number">0.9</span>*V[<span class="string">&#x27;l1&#x27;</span>])+<span class="number">0.5</span>*(<span class="number">1</span>+<span class="number">0.9</span>*V[<span class="string">&#x27;l2&#x27;</span>])</span><br><span class="line">    delta = <span class="built_in">abs</span>(t-V[<span class="string">&#x27;l1&#x27;</span>])</span><br><span class="line">    V[<span class="string">&#x27;l1&#x27;</span>] = t</span><br><span class="line"></span><br><span class="line">    t = <span class="number">0.5</span> * (<span class="number">0</span> + <span class="number">0.9</span> * V[<span class="string">&#x27;l1&#x27;</span>]) + <span class="number">0.5</span> * (-<span class="number">1</span> + <span class="number">0.9</span> * V[<span class="string">&#x27;l2&#x27;</span>])</span><br><span class="line">    delta = <span class="built_in">max</span>(<span class="built_in">abs</span>(t - V[<span class="string">&#x27;l2&#x27;</span>]),delta)</span><br><span class="line">    V[<span class="string">&#x27;l2&#x27;</span>]=t</span><br><span class="line"></span><br><span class="line">    n+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> delta&lt;<span class="number">0.0001</span>:</span><br><span class="line">        <span class="built_in">print</span>(n)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(V)</span><br><span class="line"><span class="comment"># 61</span></span><br><span class="line"><span class="comment"># &#123;&#x27;l1&#x27;: -2.2493782177156936, &#x27;l2&#x27;: -2.7494201578106514&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="GridWorld"><a href="#GridWorld" class="headerlink" title="GridWorld"></a>GridWorld</h2><p>在之前的例子中，我们使用了一个只有两个状态的问题，在这一章中使用一个更为复杂的场景，包含一个奖励点，一个惩罚点和一个无法移动点：</p>
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202508241405517.png" height="300px" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GridWorld</span>:  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.RewardMap = [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line">                          [<span class="number">0</span>,<span class="literal">None</span>,<span class="number">0</span>,-<span class="number">1</span>],</span><br><span class="line">                          [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]]</span><br><span class="line">        <span class="variable language_">self</span>.Actions = &#123;<span class="string">&#x27;up&#x27;</span>:[-<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                       <span class="string">&#x27;down&#x27;</span>:[<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                       <span class="string">&#x27;left&#x27;</span>:[<span class="number">0</span>,-<span class="number">1</span>],</span><br><span class="line">                       <span class="string">&#x27;right&#x27;</span>:[<span class="number">0</span>,<span class="number">1</span>]&#125;</span><br><span class="line">        <span class="variable language_">self</span>.GoalState = [<span class="number">0</span>,<span class="number">3</span>]</span><br><span class="line">        <span class="variable language_">self</span>.StartState = [<span class="number">2</span>,<span class="number">0</span>]</span><br><span class="line">        <span class="variable language_">self</span>.WallState = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Width = <span class="built_in">len</span>(<span class="variable language_">self</span>.RewardMap[<span class="number">0</span>])</span><br><span class="line">        <span class="variable language_">self</span>.Height = <span class="built_in">len</span>(<span class="variable language_">self</span>.RewardMap)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">NextState</span>(<span class="params">self,State,Action</span>):</span><br><span class="line">        Move = <span class="variable language_">self</span>.Actions[Action]</span><br><span class="line">        NextState = [State[<span class="number">0</span>] + Move[<span class="number">0</span>],State[<span class="number">1</span>] + Move[<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">if</span> NextState[<span class="number">0</span>]&lt;<span class="number">0</span> <span class="keyword">or</span> NextState[<span class="number">1</span>]&lt;<span class="number">0</span> <span class="keyword">or</span> NextState[<span class="number">0</span>]&gt;=<span class="variable language_">self</span>.Height <span class="keyword">or</span> NextState[<span class="number">1</span>]&gt;=<span class="variable language_">self</span>.Width:</span><br><span class="line">            NextState = State</span><br><span class="line">        <span class="keyword">elif</span> NextState == <span class="variable language_">self</span>.WallState:</span><br><span class="line">            NextState = State</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> NextState</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">GetReward</span>(<span class="params">self,NextState</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.RewardMap[NextState[<span class="number">0</span>]][NextState[<span class="number">1</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面使用<strong>迭代策略评估</strong>来估计在<strong>随机策略</strong>下的价值函数值，回忆一次价值函数的迭代公式：</p>
<script type="math/tex; mode=display">
V_{k+1}(s)=\sum_{a}\pi(a|s)\{r(s,a,s^{'})+\gamma V _{k}(s^{'})\}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">DPOneStep</span>(<span class="params">pi,V,env,gamma=<span class="number">0.9</span></span>):</span><br><span class="line">    <span class="keyword">for</span> Row <span class="keyword">in</span> <span class="built_in">range</span>(env.Height):</span><br><span class="line">        <span class="keyword">for</span> Col <span class="keyword">in</span> <span class="built_in">range</span>(env.Width):</span><br><span class="line">            State = (Row,Col)</span><br><span class="line">            <span class="keyword">if</span> State == env.GoalState:</span><br><span class="line">                V[State] = <span class="number">0</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            NewV = <span class="number">0</span></span><br><span class="line">            ActionProbs = pi[State]</span><br><span class="line">            <span class="keyword">for</span> Action , ActionProb <span class="keyword">in</span> ActionProbs.items():</span><br><span class="line">                NextState = env.NextState(State,Action)</span><br><span class="line">                Reward = env.GetReward(NextState)</span><br><span class="line">                NewV += ActionProb*(Reward+gamma*V[NextState])</span><br><span class="line"></span><br><span class="line">            V[State] = NewV</span><br><span class="line">    <span class="keyword">return</span> V</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">PolicyEval</span>(<span class="params">pi,V,env,gamma=<span class="number">0.9</span>,threshold=<span class="number">0.0001</span></span>):</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        OldV = V.copy()</span><br><span class="line">        V = DPOneStep(pi,V,env,gamma=gamma)</span><br><span class="line"></span><br><span class="line">        Delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> Key,Value <span class="keyword">in</span> V.items():</span><br><span class="line">            T = <span class="built_in">abs</span>(Value-OldV[Key])</span><br><span class="line">            <span class="keyword">if</span> T&gt;Delta:</span><br><span class="line">                Delta=T</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> Delta&lt;threshold:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> V</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202508242130530.png" width="50%" /></p>
<h2 id="策略迭代法"><a href="#策略迭代法" class="headerlink" title="策略迭代法"></a>策略迭代法</h2><p>最优策略$\pi_{*}(s)$表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi_{*}(s) &= \argmax_{a}\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{*}(s^{'})\}
\\&=\argmax_{a}q_{*}(s,a)
\end{aligned}</script><p>同理，也可以以此为基础提出最优策略的迭代公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi^{'}(s) &= \argmax_{a}\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'})\}
\end{aligned}</script><p>其中$\nu _{\pi}$ 表示当前策略下的价值函数，$\pi^{‘}(s)$表示下一次迭代的策略。</p>
<blockquote>
<p>至此，我们得到了两个迭代公式：</p>
<ul>
<li><strong>价值函数迭代公式：</strong> 给定<strong>策略</strong>即可求出当前策略的<strong>价值函数</strong></li>
<li><strong>最优策略迭代公式：</strong> 给定<strong>价值函数</strong>即可迭代更新<strong>策略</strong></li>
</ul>
<p>可以观察到上述两个公式是互相运用的，基于这个前提，策略迭代的基本思路是<strong>重复评估和改进</strong>，具体的说是用$\pi_0$评估得到$V_0$，然后再用$V_0$更新策略得到$\pi_1$，以此类推。</p>
</blockquote>
<p><strong>实施策略迭代法：</strong></p>
<p>在上面的例子中不存在状态转移概率，所以策略的迭代公式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi^{'}(s) &= \argmax_{a}\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'})\}
\end{aligned}</script><ul>
<li><code>GreedyPolicy</code>函数实现了<strong>最优策略迭代公式</strong>，主要思路是计算出每个$Action$的${r(s,a,s^{‘})+\gamma \nu _{\pi}(s^{‘})}$，然后存储到<code>ActionScore</code>中，最后得到最大值的$Action$</li>
<li><code>PolicyIter</code>函数实现了<strong>重复评估和改进</strong>，迭代结束的标准是策略收敛，当前策略即是<strong>最优策略</strong>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">GreedyPolicy</span>(<span class="params">env,V,gamma = <span class="number">0.9</span></span>):</span><br><span class="line">    pi = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> Row <span class="keyword">in</span> <span class="built_in">range</span>(env.Height):</span><br><span class="line">        <span class="keyword">for</span> Col <span class="keyword">in</span> <span class="built_in">range</span>(env.Width):</span><br><span class="line">            State = [Row,Col]</span><br><span class="line">            ActionScore = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> Action,StateChange <span class="keyword">in</span> env.Actions.items():</span><br><span class="line">                NextState = env.NextState(State,Action)</span><br><span class="line">                Reward = env.GetReward(NextState)</span><br><span class="line">                ActionScore[<span class="built_in">str</span>(Action)]=Reward+gamma*V[<span class="built_in">str</span>(NextState)]</span><br><span class="line"></span><br><span class="line">            MaxScore = -<span class="number">1</span> <span class="comment">#确保BestAction有值</span></span><br><span class="line">            <span class="keyword">for</span> Action,Score <span class="keyword">in</span> ActionScore.items():</span><br><span class="line">                <span class="keyword">if</span> Score&gt;MaxScore:</span><br><span class="line">                    MaxScore = Score</span><br><span class="line">                    BestAction = Action</span><br><span class="line">            pi[<span class="built_in">str</span>(State)] = &#123;<span class="string">&#x27;left&#x27;</span>:<span class="number">0</span>,<span class="string">&#x27;right&#x27;</span>:<span class="number">0</span>,<span class="string">&#x27;up&#x27;</span>:<span class="number">0</span>,<span class="string">&#x27;down&#x27;</span>:<span class="number">0</span>&#125;</span><br><span class="line">            pi[<span class="built_in">str</span>(State)][BestAction] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> pi</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">PolicyIter</span>(<span class="params">pi,V,env,gamma=<span class="number">0.9</span>,threshold=<span class="number">0.001</span></span>):</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        V = PolicyEval(pi,V,env,gamma, threshold)</span><br><span class="line">        NewPi = GreedyPolicy(env,V,gamma)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> NewPi==pi:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        pi = NewPi</span><br><span class="line">        <span class="comment"># print(V,pi)</span></span><br><span class="line">    <span class="keyword">return</span> pi,V</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env = GridWorld()</span><br><span class="line"></span><br><span class="line">V = defaultdict(<span class="keyword">lambda</span> :<span class="number">0</span>)</span><br><span class="line">pi = defaultdict(<span class="keyword">lambda</span> :&#123;<span class="string">&#x27;left&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;right&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;up&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;down&#x27;</span>:<span class="number">0.25</span>&#125;)</span><br><span class="line">V = PolicyEval(pi, V, env)</span><br><span class="line"></span><br><span class="line">BestPi,BestV = PolicyIter(pi,V,env)</span><br><span class="line"><span class="built_in">print</span>(BestPi)</span><br><span class="line"><span class="built_in">print</span>(BestV)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> State,Action <span class="keyword">in</span> BestPi.items():</span><br><span class="line">    <span class="built_in">print</span>(State,Action)</span><br></pre></td></tr></table></figure>
<h2 id="价值迭代法"><a href="#价值迭代法" class="headerlink" title="价值迭代法"></a>价值迭代法</h2><p>观察两个迭代公式：</p>
<script type="math/tex; mode=display">
V^{'}(s)=\sum_{a} \sum_{s^{'}}\pi(a|s)p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma V(s^{'})\}</script><script type="math/tex; mode=display">
\begin{aligned}
\pi^{'}(s) &= \argmax_{a}\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'})\}
\end{aligned}</script><p>观察到$\sum<em>{s^{‘}}p(s^{‘}|s,a){r(s,a,s^{‘})+\gamma \nu </em>{\pi}(s^{‘})}$部分的计算是<strong>重复的</strong>，可以合并为：</p>
<script type="math/tex; mode=display">
V^{'}(s)=\max_{a}\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma V(s^{'})\}</script><p>回忆贝尔曼最优方程：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\nu _{*}(s) =\max_a \sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{*}(s^{'})\}
\end{aligned}</script><p>可以看到<strong>上面的合并公式就是贝尔曼最优方程的迭代形式</strong></p>
<p><strong>价值迭代法的实现：</strong></p>
<ul>
<li><code>ValueIterOneStep</code>函数实现了上述迭代公式，将$\sum_{s^{‘}}p(s^{‘}|s,a){r(s,a,s^{‘})+\gamma V(s^{‘})}$存储到<code>ActionScores</code>，选出最大的值即可</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ValueIterOneStep</span>(<span class="params">V,env,gamma = <span class="number">0.9</span></span>):</span><br><span class="line">    <span class="keyword">for</span> Row <span class="keyword">in</span> <span class="built_in">range</span>(env.Height):</span><br><span class="line">        <span class="keyword">for</span> Col <span class="keyword">in</span> <span class="built_in">range</span>(env.Width):</span><br><span class="line">            State = (Row,Col)</span><br><span class="line">            <span class="keyword">if</span> State == env.GoalState:</span><br><span class="line">                V[State] = <span class="number">0</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            ActionScores = []</span><br><span class="line">            <span class="keyword">for</span> Action,StateChange <span class="keyword">in</span> env.Actions.items():</span><br><span class="line">                NextState = env.NextState(State,Action)</span><br><span class="line">                Reward = env.GetReward(NextState)</span><br><span class="line">                ActionScores.append(Reward+gamma*V[NextState])</span><br><span class="line"></span><br><span class="line">            V[State] = <span class="built_in">max</span>(ActionScores)</span><br><span class="line">    <span class="keyword">return</span> V</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ValueIter</span>(<span class="params">V,env,gamma=<span class="number">0.9</span>,threshold=<span class="number">0.001</span></span>):</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        OldV=V.copy()</span><br><span class="line">        V = ValueIterOneStep(V,env,gamma)</span><br><span class="line"></span><br><span class="line">        Delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> Key,Value <span class="keyword">in</span> V.items():</span><br><span class="line">            T = <span class="built_in">abs</span>(Value-OldV[Key])</span><br><span class="line">            <span class="keyword">if</span> T&gt;Delta:</span><br><span class="line">                Delta=T</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> Delta&lt;threshold:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> V</span><br><span class="line"></span><br><span class="line">env = GridWorld()</span><br><span class="line"></span><br><span class="line">V = defaultdict(<span class="keyword">lambda</span> :<span class="number">0</span>)</span><br><span class="line">pi = defaultdict(<span class="keyword">lambda</span> :&#123;<span class="string">&#x27;left&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;right&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;up&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;down&#x27;</span>:<span class="number">0.25</span>&#125;)</span><br><span class="line">V = PolicyEval(pi, V, env)</span><br><span class="line"></span><br><span class="line">V = ValueIter(V,env)</span><br><span class="line">pi = GreedyPolicy(env,V)</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202508241359358.png" width="50%" /></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>动态规划法本质上就是利用<strong>贝尔曼方程的迭代形式</strong>，其要求是<strong>已知环境模型</strong>，一步一步迭代而避免了求解复杂的联立方程，具体分为三个方法：</p>
<ul>
<li><strong>策略评估</strong>：已知策略迭代计算价值函数</li>
<li><strong>策略迭代法</strong>：策略和价值函数互相迭代</li>
<li><strong>价值迭代法</strong>：先迭代计算出最优价值函数，然后再直接得到最优策略</li>
</ul>
<h1 id="Chapter5：蒙特卡洛方法"><a href="#Chapter5：蒙特卡洛方法" class="headerlink" title="Chapter5：蒙特卡洛方法"></a>Chapter5：蒙特卡洛方法</h1><p>大部分情况下环境模型是位置的，我们必须从环境中的反复的采集来完成我们的估计，<strong>蒙特卡洛方法是对数据进行反复采样并根据结果进行估计的方法的总称。</strong></p>
<p>模型的表示方法分为<strong>分布模型</strong>和<strong>样本模型</strong></p>
<ul>
<li><strong>分布模型</strong>：表示出模型的具体概率分布</li>
<li><strong>样本模型</strong>：通过采样样本以估计模型的概率分布</li>
</ul>
<p>例如对于2个骰子的点数和模型，我们当然可以一一列举可能出现的36种可能性，表示每种点数和的概率，这个是分布模型。也可以不断的实验，通过采用的结果计算期望值，这就是<strong>蒙特卡洛方法</strong></p>
<blockquote>
<p>在前面的老虎机部分时，我们用采样的平均值估计数学期望，这就是蒙特卡洛方法</p>
</blockquote>
<h2 id="使用蒙特卡洛方法计算价值函数"><a href="#使用蒙特卡洛方法计算价值函数" class="headerlink" title="使用蒙特卡洛方法计算价值函数"></a>使用蒙特卡洛方法计算价值函数</h2><p>回顾价值函数的定义：</p>
<script type="math/tex; mode=display">
\nu _{\pi}(s) = \mathbb{E}_{\pi}[G_t|S_t = s]</script><p>按照“<strong>使用平均值进行期望值估计</strong>”的思想，价值函数估计值可以表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
V_{\pi}(s) &= \frac{G^{(1)}+G^{(2)}+...+G^{(n)}}{n}
\\&= V_{n-1}(s)+\frac{1}{n}\{G^{(n)}-V_{n-1}(s)\}  
\end{aligned}</script><p>其中$G^{(n)}$表示第$n$轮测试的收益，例如从某个初始点开始，进行$n$次测试，每次到达目标点为止（注意此处只能适用<strong>回合制问题</strong>）</p>
<p>在这个思路的前提下，假设一共有$m$个$State$，那么一共需要进行$m×n$次测试，显然这样的更新方式是效率很低的。例如从$A$状态出发，经过了$B$和$C$到达终点，其实这一次测试可以更新三个状态的价值函数，然而上面的思想却只更新$A$状态的价值函数。</p>
<p>假设从$A$状态出发，经过了$B$和$C$到达终点，$A$到$B$的收益是$R_0$，以此类推，那么各个状态的价值函数估计可以表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
G_A &= R_0 + \gamma R_1 + \gamma^2 R_2 = R_0+\gamma G_B
\\G_B&= R_1+\gamma R_2 = R_1+\gamma G_C
\\G_C&=R_2
\end{aligned}</script><p>因为更新$G_A$需要用到$G_B$，为了更新的独立性，我们<strong>从后往前</strong>更新：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
G_C&=R_2
\\G_B&= R_1+\gamma G_C
\\G_A &= R_0+\gamma G_B
\end{aligned}</script><p>仍然在<strong>GridWorld</strong>问题中应用，创建<code>RandomAgent</code>类实现：</p>
<ul>
<li><code>GetAction</code>：随机获取下一个行动</li>
<li><code>Move</code>：记录一次移动的状态和收益，并更新当前状态</li>
<li><code>Update</code>：更新价值函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RandomAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,StartState</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Cnts = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line">        <span class="variable language_">self</span>.StartState = StartState</span><br><span class="line">        <span class="variable language_">self</span>.State = StartState <span class="comment">#指示当前的状态</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.V = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.gamma = <span class="number">0.9</span></span><br><span class="line">        <span class="variable language_">self</span>.pi = defaultdict(<span class="keyword">lambda</span> : &#123;<span class="string">&#x27;left&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;right&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;up&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;down&#x27;</span>:<span class="number">0.25</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">GetAction</span>(<span class="params">self,State</span>):</span><br><span class="line">        ActionProbs = <span class="variable language_">self</span>.pi[State]</span><br><span class="line">        <span class="keyword">return</span> np.random.choice(<span class="built_in">list</span>(ActionProbs.keys()),p=<span class="built_in">list</span>(ActionProbs.values()))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Move</span>(<span class="params">self,NextState,State,Reward</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Records.append([State,Reward])</span><br><span class="line">        <span class="variable language_">self</span>.State = NextState</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Update</span>(<span class="params">self</span>):</span><br><span class="line">        G = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> Data <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="variable language_">self</span>.Records):</span><br><span class="line">            State, Reward = Data</span><br><span class="line">            G = Reward + <span class="variable language_">self</span>.gamma * G</span><br><span class="line">            <span class="variable language_">self</span>.Cnts[State] += <span class="number">1</span></span><br><span class="line">            <span class="variable language_">self</span>.V[State] += (G - <span class="variable language_">self</span>.V[State]) / <span class="variable language_">self</span>.Cnts[State]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Reset</span>(<span class="params">self</span>): <span class="comment"># 返回起点</span></span><br><span class="line">        <span class="variable language_">self</span>.State = <span class="variable language_">self</span>.StartState</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br></pre></td></tr></table></figure>
<p>运行上述方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">steps = <span class="number">1000</span></span><br><span class="line">env = GridWorld()</span><br><span class="line">Agent = RandomAgent(env.StartState)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">    Agent.Reset()</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        State = Agent.State</span><br><span class="line">        Action = Agent.GetAction(State)</span><br><span class="line">        NextState = env.NextState(State,Action)</span><br><span class="line">        Reward = env.GetReward(NextState)</span><br><span class="line">        Agent.Move(NextState,State,Reward)</span><br><span class="line">        <span class="keyword">if</span> NextState == env.GoalState:</span><br><span class="line">            Agent.Update()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        State = NextState</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202508242134498.png" width="50%" /></p>
<h2 id="使用蒙特卡洛方法实现策略控制"><a href="#使用蒙特卡洛方法实现策略控制" class="headerlink" title="使用蒙特卡洛方法实现策略控制"></a>使用蒙特卡洛方法实现策略控制</h2><p>回忆最优策略公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi_{*}(s) &= \argmax_{a}\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{*}(s^{'})\}
\\&=\argmax_{a}q_{*}(s,a)
\end{aligned}</script><p>在本例子中，我们不知道环境参数，也就是不知道$p(s^{‘}|s,a)$部分，所以只能实验<strong>Q函数公式</strong>来进行策略控制。</p>
<p>与价值函数迭代类似，Q函数迭代公式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Q_{n}(s,a) &= \frac{G^{(1)}+G^{(2)}+...+G^{(n)}}{n}
\\&= Q_{n-1}(s,a)+\frac{1}{n}\{G^{(n)}-Q_{n-1}(s,a)\}  
\end{aligned}</script><p><strong>蒙特卡洛方法实现策略控制：</strong></p>
<ul>
<li><code>GreedyProbs</code>实现了策略的迭代更新，主要就是找到Q函数最大的<code>Action</code></li>
<li><code>Update</code>函数增加了对策略的更新，其余不变</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">GreedyProbs</span>(<span class="params">Q,State,Actions = [<span class="string">&#x27;left&#x27;</span>,<span class="string">&#x27;right&#x27;</span>,<span class="string">&#x27;up&#x27;</span>,<span class="string">&#x27;down&#x27;</span>]</span>):</span><br><span class="line">    qs = [Q[(State,Action)] <span class="keyword">for</span> Action <span class="keyword">in</span> Actions]</span><br><span class="line">    BestAction = np.argmax(qs)</span><br><span class="line">    ActionProbs = &#123;Action: <span class="number">0</span> <span class="keyword">for</span> Action <span class="keyword">in</span> Actions&#125;</span><br><span class="line">    ActionProbs[Actions[BestAction]] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> ActionProbs</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">McAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,StartState</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Cnts = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line">        <span class="variable language_">self</span>.StartState = StartState</span><br><span class="line">        <span class="variable language_">self</span>.State = StartState <span class="comment">#指示当前的状态</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Q = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.gamma = <span class="number">0.9</span></span><br><span class="line">        <span class="variable language_">self</span>.pi = defaultdict(<span class="keyword">lambda</span> : &#123;<span class="string">&#x27;left&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;right&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;up&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;down&#x27;</span>:<span class="number">0.25</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">GetAction</span>(<span class="params">self,State</span>):</span><br><span class="line">        ActionProbs = <span class="variable language_">self</span>.pi[State]</span><br><span class="line">        <span class="keyword">return</span> np.random.choice(<span class="built_in">list</span>(ActionProbs.keys()),p=<span class="built_in">list</span>(ActionProbs.values()))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Move</span>(<span class="params">self,NextState,Action,State,Reward</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Records.append([State,Action,Reward])</span><br><span class="line">        <span class="variable language_">self</span>.State = NextState</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Update</span>(<span class="params">self</span>):</span><br><span class="line">        G = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> Data <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="variable language_">self</span>.Records):</span><br><span class="line">            State, Action, Reward = Data</span><br><span class="line">            G = Reward + <span class="variable language_">self</span>.gamma * G</span><br><span class="line">            Key = (State, Action)</span><br><span class="line">            <span class="variable language_">self</span>.Cnts[Key] += <span class="number">1</span></span><br><span class="line">            <span class="variable language_">self</span>.Q[Key] += (G - <span class="variable language_">self</span>.Q[Key]) / <span class="variable language_">self</span>.Cnts[Key]</span><br><span class="line">            <span class="variable language_">self</span>.pi[State] = GreedyProbs(<span class="variable language_">self</span>.Q,State)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Reset</span>(<span class="params">self</span>): <span class="comment"># 返回起点</span></span><br><span class="line">        <span class="variable language_">self</span>.State = <span class="variable language_">self</span>.StartState</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line">        </span><br><span class="line">steps = <span class="number">1000</span></span><br><span class="line">env = GridWorld()</span><br><span class="line">Agent = McAgent(env.StartState)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">    Agent.Reset()</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        State = Agent.State</span><br><span class="line">        Action = Agent.GetAction(State)</span><br><span class="line">        NextState = env.NextState(State,Action)</span><br><span class="line">        Reward = env.GetReward(NextState)</span><br><span class="line">        Agent.Move(NextState,Action,State,Reward)</span><br><span class="line">        <span class="keyword">if</span> NextState == env.GoalState:</span><br><span class="line">            Agent.Update()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        State = NextState</span><br></pre></td></tr></table></figure>
<p>使用上述方法有两个弊端：</p>
<ul>
<li>在<code>GreedyProbs</code>函数中不应该使用绝对的贪婪算法，这样会导致两种弊端：<ul>
<li>一旦确定从初始点到终点的最佳路径，那么这个路径不会变化，对于不在这个路径上的状态，其价值函数是无法更新的</li>
<li>如果在第一次更新时，在初始点的最佳行动是往左（在这个问题中是可能的，因为撞墙是不会有惩罚的，只会停在原地不动），那么程序会陷入死循环，即一直撞墙</li>
</ul>
</li>
<li>在 <code>Update</code>函数中不应该使用完全平均，而应该使用<strong>移动平均</strong>，具体原因可以类似于老虎机问题中的<strong>非稳态情况</strong></li>
</ul>
<h2 id="varepsilon-greedy算法"><a href="#varepsilon-greedy算法" class="headerlink" title="$\varepsilon$-greedy算法"></a>$\varepsilon$-greedy算法</h2><p>其实就是在选取最优行动时，不完全实现贪婪，而保持一些“探索性”，具体实现方式是<code>e_GreedyProbs</code>函数中，输出不再是<code>[0,0,1,0]</code>的形式，而是类似<code>[0.1,0.1,0.7,0.1]</code>这样在<strong>当前最优行动的选择概率最大的情况下，又有一定的概率选择其他行动的结果</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">e_GreedyProbs</span>(<span class="params">Q,State,Epsilon=<span class="number">0</span>, Actions = [<span class="string">&#x27;left&#x27;</span>,<span class="string">&#x27;right&#x27;</span>,<span class="string">&#x27;up&#x27;</span>,<span class="string">&#x27;down&#x27;</span>]</span>):</span><br><span class="line">    qs = [Q[(State,Action)] <span class="keyword">for</span> Action <span class="keyword">in</span> Actions]</span><br><span class="line">    BaseProbs = Epsilon/<span class="built_in">len</span>(Actions)</span><br><span class="line"></span><br><span class="line">    BestAction = np.argmax(qs)</span><br><span class="line">    ActionProbs = &#123;Action: BaseProbs <span class="keyword">for</span> Action <span class="keyword">in</span> Actions&#125;</span><br><span class="line">    ActionProbs[Actions[BestAction]] += (<span class="number">1</span> - Epsilon)</span><br><span class="line">    <span class="keyword">return</span> ActionProbs</span><br></pre></td></tr></table></figure>
<p>同时将<code>Update</code>函数更新，主要变化是引入常数$\alpha$以更新$Q$值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Update</span>(<span class="params">self</span>):</span><br><span class="line">    G = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> Data <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="variable language_">self</span>.Records):</span><br><span class="line">        State, Action, Reward = Data</span><br><span class="line">        G = Reward + <span class="variable language_">self</span>.gamma * G</span><br><span class="line">        Key = (State, Action)</span><br><span class="line">        <span class="variable language_">self</span>.Cnts[Key] += <span class="number">1</span></span><br><span class="line">        <span class="variable language_">self</span>.Q[Key] += (G - <span class="variable language_">self</span>.Q[Key]) * <span class="variable language_">self</span>.Alpha</span><br><span class="line">        <span class="variable language_">self</span>.pi[State] = e_GreedyProbs(<span class="variable language_">self</span>.Q,State,<span class="variable language_">self</span>.Epsilon)</span><br></pre></td></tr></table></figure>
<p>这样就不会有死循环的问题，也可以保证所有状态都有概率走到。</p>
<h2 id="异策略型"><a href="#异策略型" class="headerlink" title="异策略型"></a>异策略型</h2><p>在上面的方法中，我们采用$\varepsilon$-greedy算法进行策略的更新，事实上这是一种妥协，我们必须设置一定程度的“探索”概率以完成所有的状态更新。<strong>理想情况下我们想用完全的“贪婪”，不希望有“探索”行为。</strong>，那么可以采用<strong>异策略型</strong>。</p>
<p>先介绍一些概念：</p>
<ul>
<li><strong>目标策略</strong>：是我们希望学习和优化的策略。它通常是在理论分析或者算法设计中定义的理想策略，用于计算目标值（如在计算价值函数的目标值时），以此来引导行为策略朝着更好的方向学习。</li>
<li><strong>行为策略</strong>：智能体在与环境交互过程中用于生成动作的策略。简单来说，它决定了智能体在每个状态下如何实际选择动作</li>
<li><strong>同策略型</strong>：目标策略和行为策略没有区分</li>
<li><strong>异策略型</strong>：有区分，分布是两个策略分布</li>
</ul>
<p>在策略的更新中，我们想要最后的策略是完全的“贪婪”，但是如果我们真的按照完全”贪婪”则无法完成训练。所以我们可以设置一个策略，他具有“探索”行为，称之为<strong>行为策略</strong>，智能体的实际行动还是按照这个<strong>行为策略</strong>。又设置另外一个策略，完全按照”贪婪”,称之为<strong>目标策略</strong>，是我们希望的策略更新方向。</p>
<p>由于行为策略和目标策略不同，在估计目标策略的价值函数时会遇到问题。因为按照行为策略收集的数据来直接估计目标策略的价值函数是不准确的。<strong>重要性采样</strong>就是用于解决这个问题的技术。</p>
<p><strong>重要性采集</strong></p>
<p>假如我们要估计$\pi$分布的数学期望，那最简单的办法是采集这个分布的值，然后算平均数：</p>
<script type="math/tex; mode=display">
sampling:x_i\sim\pi

\\\mathbb{E}_{\pi}(x) =\sum x \pi(x)= \frac{x_1+x_2+x_3...x_n}{n}</script><p>假如$x$不是从$\pi$分布中采集的，而是从$b$分布中采集的，那如何估计$\pi$分布的数学期望呢？</p>
<script type="math/tex; mode=display">
sampling:x_i\sim b
\\
\mathbb{E}_{\pi}(x) =\sum x \frac{\pi(x)}{b(x)}b(x) = \mathbb{E}_{b}([x\frac{\pi(x)}{b(x)}]) = \mathbb{E}_{b}(\rho x)</script><p>也就是说，<strong>我们通过行为策略采集到了数据，但是我们需要估计的是目标策略的价值函数</strong>，所以通过采集$\rho x$来估计目标策略的价值函数。</p>
<p>当这两个分概率分布差别较大时，$\frac{\pi(x)}{b(x)}$不稳定，导致采集到的值$x\frac{\pi(x)}{b(x)}$与真实的数学期望之间的差距较大，采集到的值的<strong>方差很大</strong>，意味着方法的稳定性较差，<strong>保证两个分布的概率分布尽可能一样</strong>是解决这个问题的方法，<strong>在这里，两个分布的主要差异是探索系数，当<code>Epsilon</code>设置合理时，可以达到这个目的。</strong></p>
<p>下面介绍了蒙特卡洛方法中异型策略的实现方式，由于蒙特卡洛的数据是一个回合，所以从前面时间步到后</p>
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202508242149373.png" height="400px" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">McOffAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,StartState</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Cnts = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line">        <span class="variable language_">self</span>.StartState = StartState</span><br><span class="line">        <span class="variable language_">self</span>.State = StartState <span class="comment">#指示当前的状态</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Epsilon = <span class="number">0.2</span></span><br><span class="line">        <span class="variable language_">self</span>.Alpha = <span class="number">0.1</span></span><br><span class="line">        <span class="variable language_">self</span>.b = defaultdict(<span class="keyword">lambda</span> : &#123;<span class="string">&#x27;left&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;right&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;up&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;down&#x27;</span>:<span class="number">0.25</span>&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Q = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.gamma = <span class="number">0.9</span></span><br><span class="line">        <span class="variable language_">self</span>.pi = defaultdict(<span class="keyword">lambda</span> : &#123;<span class="string">&#x27;left&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;right&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;up&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;down&#x27;</span>:<span class="number">0.25</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">GetAction</span>(<span class="params">self,State</span>):</span><br><span class="line">        ActionProbs = <span class="variable language_">self</span>.b[State]</span><br><span class="line">        <span class="keyword">return</span> np.random.choice(<span class="built_in">list</span>(ActionProbs.keys()),p=<span class="built_in">list</span>(ActionProbs.values()))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Move</span>(<span class="params">self,NextState,Action,State,Reward</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Records.append([State,Action,Reward])</span><br><span class="line">        <span class="variable language_">self</span>.State = NextState</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Update</span>(<span class="params">self</span>):</span><br><span class="line">        G = <span class="number">0</span></span><br><span class="line">        rho = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> Data <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="variable language_">self</span>.Records):</span><br><span class="line">            State, Action, Reward = Data</span><br><span class="line">            G = Reward + <span class="variable language_">self</span>.gamma * G * rho</span><br><span class="line">            Key = (State, Action)</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.Q[Key] += (G - <span class="variable language_">self</span>.Q[Key]) * <span class="variable language_">self</span>.Alpha</span><br><span class="line">            rho *= <span class="variable language_">self</span>.pi[State][Action] / <span class="variable language_">self</span>.b[State][Action]</span><br><span class="line">            <span class="variable language_">self</span>.pi[State] = e_GreedyProbs(<span class="variable language_">self</span>.Q,State,<span class="number">0</span>)</span><br><span class="line">            <span class="variable language_">self</span>.b[State] = e_GreedyProbs(<span class="variable language_">self</span>.Q,State,<span class="variable language_">self</span>.Epsilon)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Reset</span>(<span class="params">self</span>): <span class="comment"># 返回起点</span></span><br><span class="line">        <span class="variable language_">self</span>.State = <span class="variable language_">self</span>.StartState</span><br></pre></td></tr></table></figure>
<h1 id="Chapter6：TD-方法"><a href="#Chapter6：TD-方法" class="headerlink" title="Chapter6：TD 方法"></a>Chapter6：TD 方法</h1><blockquote>
<p><strong>TD：Temporal Difference，时间差分</strong></p>
</blockquote>
<p>蒙特卡洛方法的弊端在于，其只能应用于<strong>回合制问题</strong>，且当一个回合过长时，其更新速度很慢。<br>面对连续性问题或回合很长的回合制问题时，<strong>TD方法</strong>更有效。</p>
<ul>
<li><p>动态规划法</p>
</li>
<li><p>蒙特卡洛方法</p>
</li>
<li><p>TD方法</p>
</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned} 
\nu _{\pi}(s) &=\sum_{a} \sum_{s^{'}}\pi(a|s)p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \nu _{\pi}(s^{'})\}
\\&=\mathbb{E}_{\pi}[R_t + \gamma V_{\pi}(S_{t+1})|S_t = s] 
\end{aligned}</script><p>TD方法使用的更新公式为：</p>
<script type="math/tex; mode=display">
V_{\pi}^{'} = V_{\pi}(S_t) + \alpha(R_t + \gamma V_{\pi}(S_{t+1})-V_{\pi}(S_{t}))</script><p>仍然在<strong>GridWorld</strong>中进行实验，主要关注<code>Update</code>方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TDAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,StartState</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Cnts = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line">        <span class="variable language_">self</span>.StartState = StartState</span><br><span class="line">        <span class="variable language_">self</span>.State = StartState <span class="comment">#指示当前的状态</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Alpha = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.V = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.gamma = <span class="number">0.9</span></span><br><span class="line">        <span class="variable language_">self</span>.pi = defaultdict(<span class="keyword">lambda</span> : &#123;<span class="string">&#x27;left&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;right&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;up&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;down&#x27;</span>:<span class="number">0.25</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">GetAction</span>(<span class="params">self,State</span>):</span><br><span class="line">        ActionProbs = <span class="variable language_">self</span>.pi[State]</span><br><span class="line">        <span class="keyword">return</span> np.random.choice(<span class="built_in">list</span>(ActionProbs.keys()),p=<span class="built_in">list</span>(ActionProbs.values()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Update</span>(<span class="params">self,Reward,State,NextState,GoalState</span>):</span><br><span class="line">        <span class="keyword">if</span> NextState == GoalState:</span><br><span class="line">            Target = Reward + <span class="variable language_">self</span>.gamma * <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:            </span><br><span class="line">            Target = Reward + <span class="variable language_">self</span>.gamma * <span class="variable language_">self</span>.V[NextState]</span><br><span class="line">        <span class="variable language_">self</span>.V[State] +=<span class="variable language_">self</span>.Alpha * (Target - <span class="variable language_">self</span>.V[State])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Reset</span>(<span class="params">self</span>): <span class="comment"># 返回起点</span></span><br><span class="line">        <span class="variable language_">self</span>.State = <span class="variable language_">self</span>.StartState</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">steps = <span class="number">10000</span></span><br><span class="line">env = GridWorld()</span><br><span class="line">Agent = TDAgent(env.StartState)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">    Agent.Reset()</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        State = Agent.State</span><br><span class="line">        Action = Agent.GetAction(State)</span><br><span class="line">        NextState = env.NextState(State,Action)</span><br><span class="line">        Reward = env.GetReward(NextState)</span><br><span class="line">        Agent.Update(Reward, State, NextState,env.GoalState) <span class="comment">#每一步都要更新</span></span><br><span class="line">        <span class="keyword">if</span> NextState == env.GoalState:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        Agent.State = NextState</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202508242135566.png" width="50%" /></p>
<h2 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA"></a>SARSA</h2><p>同样，在未知环境模型的情况下，我们还是实验Q函数来进行策略估计和策略控制</p>
<p>回忆TD方法的策略估计公式：</p>
<script type="math/tex; mode=display">
V_{\pi}^{'}(S_t) = V_{\pi}(S_t) + \alpha(R_t + \gamma V_{\pi}(S_{t+1})-V_{\pi}(S_{t}))</script><p>将其运用到<strong>Q函数</strong>中可以表示为：</p>
<script type="math/tex; mode=display">
Q_{\pi}^{'}(S_t,A_t) = Q_{\pi}(S_t,A_t) + \alpha(R_t + \gamma Q_{\pi}(S_{t+1},A_{t+1})-Q_{\pi}(S_t,A_t))</script><p><strong>使用SARSA方法进行策略控制：</strong></p>
<ul>
<li><code>Update</code>函数实现了Q函数的更新<ul>
<li><code>Records</code>中保存了$(S<em>t,A_t)$和$(S</em>{t+1},A_{t+1})$</li>
</ul>
</li>
<li>策略的更新采用了$\varepsilon$-greedy算法</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SarsaAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,StartState</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Cnts = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line">        <span class="variable language_">self</span>.StartState = StartState</span><br><span class="line">        <span class="variable language_">self</span>.State = StartState <span class="comment">#指示当前的状态</span></span><br><span class="line">        <span class="variable language_">self</span>.Alpha = <span class="number">0.8</span></span><br><span class="line">        <span class="variable language_">self</span>.Q = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.gamma = <span class="number">0.9</span></span><br><span class="line">        <span class="variable language_">self</span>.pi = defaultdict(<span class="keyword">lambda</span> : &#123;<span class="string">&#x27;left&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;right&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;up&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;down&#x27;</span>:<span class="number">0.25</span>&#125;)</span><br><span class="line">        <span class="variable language_">self</span>.Epsilon = <span class="number">0.2</span></span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">GetAction</span>(<span class="params">self,State</span>):</span><br><span class="line">        ActionProbs = <span class="variable language_">self</span>.pi[State]</span><br><span class="line">        <span class="keyword">return</span> np.random.choice(<span class="built_in">list</span>(ActionProbs.keys()),p=<span class="built_in">list</span>(ActionProbs.values()))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Update</span>(<span class="params">self,Reward,State,Action,GoalState</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Records.append((State,Action,Reward))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.Records)&lt;<span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        State,Action,Reward = <span class="variable language_">self</span>.Records[-<span class="number">2</span>]</span><br><span class="line">        NextState,NextAction,_ = <span class="variable language_">self</span>.Records[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> NextState == GoalState:</span><br><span class="line">            Target = Reward + <span class="variable language_">self</span>.gamma * <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Target = Reward + <span class="variable language_">self</span>.gamma * <span class="variable language_">self</span>.Q[(NextState,NextAction)]</span><br><span class="line">        <span class="variable language_">self</span>.Q[(State,Action)] +=<span class="variable language_">self</span>.Alpha * (Target - <span class="variable language_">self</span>.Q[(State,Action)])</span><br><span class="line">        <span class="variable language_">self</span>.pi[State] = e_GreedyProbs(<span class="variable language_">self</span>.Q,State,<span class="variable language_">self</span>.Epsilon)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Reset</span>(<span class="params">self</span>): <span class="comment"># 返回起点</span></span><br><span class="line">        <span class="variable language_">self</span>.State = <span class="variable language_">self</span>.StartState</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br></pre></td></tr></table></figure>
<!-- <img src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/391531a734d8444fbcc8eef3ba006499~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg54us5oap:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMzg1NTMwNjg3OTkzMzU4MiJ9&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1747028962&x-orig-sign=LTcZ0x5cfx2bb%2FnQyuGxr9xZhyM%3D" alt="TD1.png" width="50%"><img src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/
104965c33fca4f7292edcc74fbe7e922~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg54us5oap:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMzg1NTMwNjg3OTkzMzU4MiJ9&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1747028962&x-orig-sign=1zoz6zr6RkDSlEWDgqZzUgjgJMM%3D" alt="TD2.png" width="50%">
 -->
<!-- 
<img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202505052045599.png" alt="TD1.png" width="45%"><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202505052045647.png" alt="TD2.png" width="45%"> -->
<div style="display: flex; align - items: center;">
    <img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202505052045599.png" alt="TD1.png" width="50%">
    <img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202505052045647.png" alt="TD2.png" width="50%">
</div>

<h2 id="异策略型的SARSA"><a href="#异策略型的SARSA" class="headerlink" title="异策略型的SARSA"></a>异策略型的SARSA</h2><p>在TD问题中，由于只考虑了上下两层状态，所以权重表示为：</p>
<script type="math/tex; mode=display">
\rho = \frac{\pi(A_{t+1}|S_{t+1})}{b(A_{t+1}|S_{t+1})}</script><p>因此其更新公式为</p>
<script type="math/tex; mode=display">
sampling:A_{t+1}\sim b
\\
Q_{\pi}^{'}(S_t,A_t) = Q_{\pi}(S_t,A_t) + \alpha\{\rho(R_t + \gamma Q_{\pi}(S_{t+1},A_{t+1}))-Q_{\pi}(S_t,A_t)\}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SarsaOffAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,StartState</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Cnts = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line">        <span class="variable language_">self</span>.StartState = StartState</span><br><span class="line">        <span class="variable language_">self</span>.State = StartState <span class="comment">#指示当前的状态</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Alpha = <span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Q = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.gamma = <span class="number">0.9</span></span><br><span class="line">        <span class="variable language_">self</span>.pi = defaultdict(<span class="keyword">lambda</span> : &#123;<span class="string">&#x27;left&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;right&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;up&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;down&#x27;</span>:<span class="number">0.25</span>&#125;)</span><br><span class="line">        <span class="variable language_">self</span>.b = defaultdict(<span class="keyword">lambda</span>: &#123;<span class="string">&#x27;left&#x27;</span>: <span class="number">0.25</span>, <span class="string">&#x27;right&#x27;</span>: <span class="number">0.25</span>, <span class="string">&#x27;up&#x27;</span>: <span class="number">0.25</span>, <span class="string">&#x27;down&#x27;</span>: <span class="number">0.25</span>&#125;)</span><br><span class="line">        <span class="variable language_">self</span>.Epsilon = <span class="number">0.2</span></span><br><span class="line">        <span class="comment"># self.Records = deque(maxlen=2)</span></span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">GetAction</span>(<span class="params">self,State</span>):</span><br><span class="line">        ActionProbs = <span class="variable language_">self</span>.b[State]</span><br><span class="line">        <span class="keyword">return</span> np.random.choice(<span class="built_in">list</span>(ActionProbs.keys()),p=<span class="built_in">list</span>(ActionProbs.values()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Update</span>(<span class="params">self,Reward,State,Action,GoalState</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Records.append((State,Action,Reward))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.Records)&lt;<span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        State,Action,Reward = <span class="variable language_">self</span>.Records[-<span class="number">2</span>]</span><br><span class="line">        NextState,NextAction,_ = <span class="variable language_">self</span>.Records[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> NextState == GoalState:</span><br><span class="line">            Target = Reward + <span class="variable language_">self</span>.gamma * <span class="number">0</span></span><br><span class="line">            rho = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rho = <span class="variable language_">self</span>.pi[(NextState,NextAction)] / <span class="variable language_">self</span>.b[(NextState,NextAction)]</span><br><span class="line">            Target = rho * (Reward + <span class="variable language_">self</span>.gamma * <span class="variable language_">self</span>.Q[(NextState, NextAction)])</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Q[(State,Action)] +=<span class="variable language_">self</span>.Alpha * (Target - <span class="variable language_">self</span>.Q[(State,Action)])</span><br><span class="line">        <span class="variable language_">self</span>.pi[State] = e_GreedyProbs(<span class="variable language_">self</span>.Q,State,<span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.b[State] = e_GreedyProbs(<span class="variable language_">self</span>.Q,State,<span class="variable language_">self</span>.Epsilon)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Reset</span>(<span class="params">self</span>): <span class="comment"># 返回起点</span></span><br><span class="line">        <span class="variable language_">self</span>.State = <span class="variable language_">self</span>.StartState</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line"></span><br><span class="line">steps = <span class="number">10000</span></span><br><span class="line">env = GridWorld()</span><br><span class="line">Agent = SarsaAgent(env.StartState)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">    Agent.Reset()</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        State = Agent.State</span><br><span class="line">        Action = Agent.GetAction(State)</span><br><span class="line">        NextState = env.NextState(State,Action)</span><br><span class="line">        Reward = env.GetReward(NextState)</span><br><span class="line">        Agent.Update(Reward, State,Action,env.GoalState)</span><br><span class="line">        <span class="keyword">if</span> NextState ==env.GoalState:</span><br><span class="line">            Agent.Update(<span class="literal">None</span>,NextState,<span class="literal">None</span>,env.GoalState)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        Agent.State = NextState</span><br></pre></td></tr></table></figure>
<h2 id="Q学习（重要）"><a href="#Q学习（重要）" class="headerlink" title="Q学习（重要）"></a>Q学习（重要）</h2><p>重要性采集事实上容易变得不稳定，尤其当两者策略的概率分布差别变大时，权重$\rho$的变化就会大，SARSA中的更新方向就会发生变化，从而使得Q函数的更新变得不稳定，<strong>Q学习</strong>就是解决这个问题的方法。<strong>Q学习具有下列三个特点</strong>：</p>
<ul>
<li><strong>采用TD方法</strong></li>
<li><strong>异策略型</strong></li>
<li><strong>不使用重要性采样</strong></li>
</ul>
<p>为了联合SARS了解Q学习，回忆贝尔曼方程</p>
<script type="math/tex; mode=display">
\begin{aligned} 
q_{\pi}(s,a)&=\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \sum_{a^{'}}\pi(a^{'}|s^{'})q_{\pi}(s^{'},a^{'})\}
\\& =\mathbb{E}_{\pi}[R_t + \gamma Q_{\pi}(S_{t+1},A_{t+1})|S_t = s,A_t=a] 
\end{aligned}</script><p>其考虑到了状态转移概率下$p(s^{‘}|s,a)$的所有下一个状态，又考虑到了策略$\pi(a^{‘}|s^{‘})$下的所有下一个动作。</p>
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202508242108867.png" height="300px" /></p>
<p>回忆SARSA的公式：</p>
<script type="math/tex; mode=display">
Q_{\pi}^{'}(S_t,A_t) = Q_{\pi}(S_t,A_t) + \alpha\bigg\{R_t + \gamma Q_{\pi}(S_{t+1},A_{t+1})-Q_{\pi}(S_t,A_t)\bigg\}</script><p><strong>考虑SARSA的本质，其实是贝尔曼方程的一种采样</strong>，其先基于$p(s^{‘}|s,a)$对下一个状态进行采样，然后基于$\pi(a|s)$对下一步的行动采样，于是Q函数的更新方向就是$R<em>t + \gamma Q</em>{\pi}(S<em>{t+1},A</em>{t+1})$</p>
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202508242114311.png" height="250px" /></p>
<p><strong>如果说SARSA对应着贝尔曼方程，那么Q方法就是对应着贝尔曼最优方程</strong>，其在选择下一个动作时，<strong>不再依靠策略进行采样，而是直接选择最优的动作</strong>。</p>
<p>回顾贝尔曼最优方程</p>
<script type="math/tex; mode=display">
\begin{aligned} 
q_{*}(s,a)&=\sum_{s^{'}}p(s^{'}|s,a)\{r(s,a,s^{'})+\gamma \max_{a^{'}} q_{*}(s^{'},a^{'})\}
\\&=\mathbb{E}[R_t + \gamma \max_{a^{'}} q_{*}(s^{'},a^{'})|S_t = s,A_t=a] 
\end{aligned}</script><p>将其写成采样形式：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
Q^{'}(S_t,A_t)=\mathbb{E}[R_t + \gamma \max_{a^{'}}Q(S_{t+1},a^{'})] 
\end{aligned}</script><script type="math/tex; mode=display">
Q^{'}(S_t,A_t) = Q(S_t,A_t) + \alpha \bigg\{ R_t + \gamma \max_{a^{'}}Q(S_{t+1},a^{'})-Q(S_t,A_t)\bigg\}</script><p><strong>由于对$A_{t+1}$的选择直接使用的$max$，不需要重要性选择进行修正。</strong></p>
<blockquote>
<p>读到这里时可能会有读者产生疑问：<strong>在选择动作时，Q学习采样的是$max$，但是在状态转移时为什么却选择基于采样？</strong><br><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202508242117505.png" width="600px" /></p>
</blockquote>
<p><strong>Q学习的实现：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">QLearningAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,StartState</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Cnts = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line">        <span class="variable language_">self</span>.StartState = StartState</span><br><span class="line">        <span class="variable language_">self</span>.State = StartState <span class="comment">#指示当前的状态</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Alpha = <span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Q = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.gamma = <span class="number">0.9</span></span><br><span class="line">        <span class="variable language_">self</span>.pi = defaultdict(<span class="keyword">lambda</span> : &#123;<span class="string">&#x27;left&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;right&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;up&#x27;</span>:<span class="number">0.25</span>,<span class="string">&#x27;down&#x27;</span>:<span class="number">0.25</span>&#125;)</span><br><span class="line">        <span class="variable language_">self</span>.b = defaultdict(<span class="keyword">lambda</span>: &#123;<span class="string">&#x27;left&#x27;</span>: <span class="number">0.25</span>, <span class="string">&#x27;right&#x27;</span>: <span class="number">0.25</span>, <span class="string">&#x27;up&#x27;</span>: <span class="number">0.25</span>, <span class="string">&#x27;down&#x27;</span>: <span class="number">0.25</span>&#125;)</span><br><span class="line">        <span class="variable language_">self</span>.Epsilon = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">GetAction</span>(<span class="params">self,State</span>):</span><br><span class="line">        ActionProbs = <span class="variable language_">self</span>.b[State]</span><br><span class="line">        <span class="keyword">return</span> np.random.choice(<span class="built_in">list</span>(ActionProbs.keys()),p=<span class="built_in">list</span>(ActionProbs.values()))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Update</span>(<span class="params">self,Reward,State,Action,NextState,GoalState</span>):</span><br><span class="line">        <span class="keyword">if</span> NextState == GoalState:</span><br><span class="line">            MaxQNextState = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            NextStateScore = [<span class="variable language_">self</span>.Q[(NextState,a)] <span class="keyword">for</span> a <span class="keyword">in</span> [<span class="string">&#x27;up&#x27;</span>,<span class="string">&#x27;down&#x27;</span>,<span class="string">&#x27;left&#x27;</span>,<span class="string">&#x27;right&#x27;</span>]]</span><br><span class="line">            MaxQNextState = <span class="built_in">max</span>(NextStateScore)</span><br><span class="line">        Target = Reward+<span class="variable language_">self</span>.gamma * MaxQNextState</span><br><span class="line">        <span class="variable language_">self</span>.Q[(State,Action)] +=<span class="variable language_">self</span>.Alpha * (Target - <span class="variable language_">self</span>.Q[(State,Action)])</span><br><span class="line">        <span class="variable language_">self</span>.pi[State] = e_GreedyProbs(<span class="variable language_">self</span>.Q,State,<span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.b[State] = e_GreedyProbs(<span class="variable language_">self</span>.Q,State,<span class="variable language_">self</span>.Epsilon)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Reset</span>(<span class="params">self</span>): <span class="comment"># 返回起点</span></span><br><span class="line">        <span class="variable language_">self</span>.State = <span class="variable language_">self</span>.StartState</span><br><span class="line"></span><br><span class="line">steps = <span class="number">10000</span></span><br><span class="line">env = GridWorld()</span><br><span class="line">Agent = QLearningAgent(env.StartState)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">    Agent.Reset()</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        State = Agent.State</span><br><span class="line">        Action = Agent.GetAction(State)</span><br><span class="line">        NextState = env.NextState(State,Action)</span><br><span class="line">        Reward = env.GetReward(NextState)</span><br><span class="line">        Agent.Update(Reward, State,Action,NextState,env.GoalState)</span><br><span class="line">        <span class="keyword">if</span> NextState == env.GoalState:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        Agent.State = NextState</span><br></pre></td></tr></table></figure>
<h2 id="样本模型版的Q学习"><a href="#样本模型版的Q学习" class="headerlink" title="样本模型版的Q学习"></a>样本模型版的Q学习</h2><p><strong>样本模型</strong>是对应于<strong>分布模型</strong>，其最大的特点是不保存特定的概率分布。</p>
<ul>
<li>观察Q学习代码，其实<code>pi</code>完全没有参与Q函数的更新，由于其在需要的时候，可以由Q函数值马上得到，所以其实可以直接删除</li>
<li>对于策略<code>b</code>，其本质上也是根据Q函数的$\varepsilon$-greedy算法一种实现，在<code>GetAction</code>函数中再现这一过程即可，也可以删除</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">QLearningAgent_2</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,StartState</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Cnts = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.Records = []</span><br><span class="line">        <span class="variable language_">self</span>.StartState = StartState</span><br><span class="line">        <span class="variable language_">self</span>.State = StartState <span class="comment">#指示当前的状态</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Alpha = <span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Q = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.gamma = <span class="number">0.9</span></span><br><span class="line">        <span class="variable language_">self</span>.Epsilon = <span class="number">0.2</span></span><br><span class="line">        <span class="variable language_">self</span>.ActionSize = [<span class="string">&#x27;up&#x27;</span>,<span class="string">&#x27;down&#x27;</span>,<span class="string">&#x27;left&#x27;</span>,<span class="string">&#x27;right&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">GetAction</span>(<span class="params">self,State</span>):</span><br><span class="line">        <span class="keyword">if</span> np.random.rand()&lt;<span class="variable language_">self</span>.Epsilon:</span><br><span class="line">            <span class="keyword">return</span> np.random.choice(<span class="variable language_">self</span>.ActionSize)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            qs = &#123;a:<span class="variable language_">self</span>.Q[(State,a)] <span class="keyword">for</span> a <span class="keyword">in</span> <span class="variable language_">self</span>.ActionSize&#125;</span><br><span class="line">            BestQ = -<span class="number">1</span></span><br><span class="line">            BestAction = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">for</span> k,v <span class="keyword">in</span> qs.items():</span><br><span class="line">                <span class="keyword">if</span> v&gt;BestQ:</span><br><span class="line">                    BestQ = v</span><br><span class="line">                    BestAction = k</span><br><span class="line">            <span class="keyword">return</span> BestAction</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Update</span>(<span class="params">self,Reward,State,Action,NextState,GoalState</span>):</span><br><span class="line">        <span class="keyword">if</span> NextState == GoalState:</span><br><span class="line">            MaxQNextState = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            NextStateScore = [<span class="variable language_">self</span>.Q[(NextState,a)] <span class="keyword">for</span> a <span class="keyword">in</span> <span class="variable language_">self</span>.ActionSize]</span><br><span class="line">            MaxQNextState = <span class="built_in">max</span>(NextStateScore)</span><br><span class="line">        Target = Reward+<span class="variable language_">self</span>.gamma * MaxQNextState</span><br><span class="line">        <span class="variable language_">self</span>.Q[(State,Action)] +=<span class="variable language_">self</span>.Alpha * (Target - <span class="variable language_">self</span>.Q[(State,Action)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Reset</span>(<span class="params">self</span>): <span class="comment"># 返回起点</span></span><br><span class="line">        <span class="variable language_">self</span>.State = <span class="variable language_">self</span>.StartState</span><br></pre></td></tr></table></figure>
<h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h1><p>前面3-6章基本上都是针对贝尔曼方程进行的一些工作：</p>
<!-- ![强化学习前期.png](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/79018863bc6843d1b99aea53fd46aa42~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg54us5oap:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMzg1NTMwNjg3OTkzMzU4MiJ9&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1747028962&x-orig-sign=i3acqOdYW%2FvnhfvcnMgYL1TJWes%3D)
 -->
<p><img src="https://gitee.com/leeMX111/reinforcement_learning_imgs/raw/master/202505052056144.png" alt="强化学习前期.png"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://duqi666.github.io">Mercik</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://duqi666.github.io/2025/08/24/Reinforcement_Learning/">https://duqi666.github.io/2025/08/24/Reinforcement_Learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AE%97%E6%B3%95/">算法</a><a class="post-meta__tags" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0/">学习</a></div><div class="post-share"><div class="social-share" data-image="/imgs/Reinforcement_Learning.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/08/24/aaa/" title=""><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2"></div></div><div class="info-2"><div class="info-item-1">市第三大 </div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/05/05/Reinforcement_Learning_part2/" title="《深度学习4-强化学习》学习笔记-2"><img class="cover" src="/imgs/GPT2.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-05</div><div class="info-item-2">《深度学习4-强化学习》学习笔记-2</div></div><div class="info-2"><div class="info-item-1">  Chapter7：神经网络和Q学习在前面的方法中，我们都是用一个表格形式存储Q函数，但是复杂情况下，状态数量太多，无法将其存储为表格形式，用更加紧凑的函数近似Q函数是一种处理方法，其中最有效的便是神经网络 机器学习和深度学习的基础知识此处省略 将求Q函数的过程抽象为神经网络，有两种形式：  输入状态$s$和行动$a$，输出Q函数值$Q(s,a)$ 输入状态$s$，输入所有行动的Q函数列表  首先我们需要知道神经网络的输入输出，以及其误差函数的设计，这个地方书上写的比较模糊，书上采用了下面这个公式说明，也就是Q学习的迭代公式  Q_{\pi}^{'}(S_t,A_t) = Q_{\pi}(S_t,A_t) + \alpha \bigg\{ R_t + \gamma \max_{a^{'}}Q_{\pi}(S_{t+1},a^{'})-Q_{\pi}(S_t,A_t)\bigg\}然后书上的代码表达的意思是，神经网络拟合了Q函数值，然后误差函数的形式为：  loss( R_t +\gamma \max_{a^{'}}Q_{\pi}(S_{t+1},a^{'}),Q_{\pi}(S...</div></div></div></a><a class="pagination-related" href="/2025/04/20/%E7%AE%97%E6%B3%95/" title="一些算法"><img class="cover" src="/imgs/algorithm.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-20</div><div class="info-item-2">一些算法</div></div><div class="info-2"><div class="info-item-1">  排序算法https://zhuanlan.zhihu.com/p/567134257 冒泡排序它重复地走访过要排序的元素列，依次比较两个相邻的元素，如果顺序（如从大到小、首字母从Z到A）错误就把他们交换过来。走访元素的工作是重复地进行，直到没有相邻元素需要交换，也就是说该元素列已经排序完成。   比较相邻的元素。如果第一个比第二个大，就交换他们两个。  对每一对相邻元素做同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数。  针对所有的元素重复以上的步骤，除了最后一个。  持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。1234567def BubbleSort(mylist:list):    for z in range(len(mylist)):        #最后的z个已经排好序，不需要再排序了        for i in range(len(mylist) - z-1):            if mylist[i] &gt; mylist[i + 1]:                mylist[i], ...</div></div></div></a><a class="pagination-related" href="/2025/04/16/Diagrammatize_GPT/" title="《GPT图解》学习笔记"><img class="cover" src="/imgs/GPT2.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-16</div><div class="info-item-2">《GPT图解》学习笔记</div></div><div class="info-2"><div class="info-item-1">  Chapter1：N-grams &amp; Bag-of-wordsN-grams模型N-grams是指将文本分割为连续的长度为N的文本片段，统计每个片段的频数以计算每个片段出现的条件概率，从而计算完整句子的出现概率。 该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram，下面具体解释其数学实现： 对于一个有$m$个词语的语句，其条件概率为：  P(w_1,w_2,w_3...w_m) = P(w_1)P(w_2|w_1)P(w_3|w_2,w_1)...P(w_m|w_{m-1},w_{m-2}...w_1)可以利用马尔科夫假设（当前状态只与前面n个状态相关）简化上述公式，具体体现为：  P(w_m|w_1,w_2...w_{m-1}) = P(w_m|w_{m-1},w_{m-2}...w_{m-n})当n取1时，既每个状态只与前面一个状态相关，公式可以简化为  P(w_1,w_2,w_3.....</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comments</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/imgs/I.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Mercik</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Duqi666"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter1%EF%BC%9A%E8%80%81%E8%99%8E%E6%9C%BA%E9%97%AE%E9%A2%98"><span class="toc-number">1.</span> <span class="toc-text">Chapter1：老虎机问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E8%88%AC%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.</span> <span class="toc-text">一般问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E7%A8%B3%E6%80%81%E9%97%AE%E9%A2%98"><span class="toc-number">1.2.</span> <span class="toc-text">非稳态问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter2%EF%BC%9A%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">Chapter2：马尔可夫决策过程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%A1%A8%E8%BE%BE"><span class="toc-number">2.1.</span> <span class="toc-text">数学表达</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MDP%E7%9A%84%E7%9B%AE%E6%A0%87"><span class="toc-number">2.2.</span> <span class="toc-text">MDP的目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%B6%E7%9B%8A%EF%BC%88return%EF%BC%89"><span class="toc-number">2.3.</span> <span class="toc-text">收益（return）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter3%EF%BC%9A%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B"><span class="toc-number">3.</span> <span class="toc-text">Chapter3：贝尔曼方程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B"><span class="toc-number">3.1.</span> <span class="toc-text">贝尔曼方程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%8C%E5%8A%A8%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%EF%BC%88Q%E5%87%BD%E6%95%B0%EF%BC%89"><span class="toc-number">3.2.</span> <span class="toc-text">行动价值函数（Q函数）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B"><span class="toc-number">3.3.</span> <span class="toc-text">贝尔曼最优方程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E8%AE%A1"><span class="toc-number">3.4.</span> <span class="toc-text">总计</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter4%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">Chapter4：动态规划法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%AD%E4%BB%A3%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="toc-number">4.1.</span> <span class="toc-text">迭代策略评估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%AD%E4%BB%A3%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0%E7%9A%84%E5%85%B6%E4%BB%96%E6%96%B9%E5%BC%8F-%E8%A6%86%E7%9B%96%E6%96%B9%E5%BC%8F"><span class="toc-number">4.2.</span> <span class="toc-text">迭代策略评估的其他方式-覆盖方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GridWorld"><span class="toc-number">4.3.</span> <span class="toc-text">GridWorld</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E6%B3%95"><span class="toc-number">4.4.</span> <span class="toc-text">策略迭代法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E6%B3%95"><span class="toc-number">4.5.</span> <span class="toc-text">价值迭代法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.6.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter5%EF%BC%9A%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">Chapter5：蒙特卡洛方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95%E8%AE%A1%E7%AE%97%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">5.1.</span> <span class="toc-text">使用蒙特卡洛方法计算价值函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0%E7%AD%96%E7%95%A5%E6%8E%A7%E5%88%B6"><span class="toc-number">5.2.</span> <span class="toc-text">使用蒙特卡洛方法实现策略控制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#varepsilon-greedy%E7%AE%97%E6%B3%95"><span class="toc-number">5.3.</span> <span class="toc-text">$\varepsilon$-greedy算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%82%E7%AD%96%E7%95%A5%E5%9E%8B"><span class="toc-number">5.4.</span> <span class="toc-text">异策略型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter6%EF%BC%9ATD-%E6%96%B9%E6%B3%95"><span class="toc-number">6.</span> <span class="toc-text">Chapter6：TD 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SARSA"><span class="toc-number">6.1.</span> <span class="toc-text">SARSA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%82%E7%AD%96%E7%95%A5%E5%9E%8B%E7%9A%84SARSA"><span class="toc-number">6.2.</span> <span class="toc-text">异策略型的SARSA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89"><span class="toc-number">6.3.</span> <span class="toc-text">Q学习（重要）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B7%E6%9C%AC%E6%A8%A1%E5%9E%8B%E7%89%88%E7%9A%84Q%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.4.</span> <span class="toc-text">样本模型版的Q学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Discussion"><span class="toc-number">7.</span> <span class="toc-text">Discussion</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/24/Reinforcement_Learning/" title="《深度学习4-强化学习》学习笔记"><img src="/imgs/Reinforcement_Learning.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="《深度学习4-强化学习》学习笔记"/></a><div class="content"><a class="title" href="/2025/08/24/Reinforcement_Learning/" title="《深度学习4-强化学习》学习笔记">《深度学习4-强化学习》学习笔记</a><time datetime="2025-08-24T07:13:43.417Z" title="Created 2025-08-24 15:13:43">2025-08-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/24/aaa/" title="Untitled">Untitled</a><time datetime="2025-08-24T06:25:30.541Z" title="Created 2025-08-24 14:25:30">2025-08-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/05/Reinforcement_Learning_part2/" title="《深度学习4-强化学习》学习笔记-2"><img src="/imgs/GPT2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="《深度学习4-强化学习》学习笔记-2"/></a><div class="content"><a class="title" href="/2025/05/05/Reinforcement_Learning_part2/" title="《深度学习4-强化学习》学习笔记-2">《深度学习4-强化学习》学习笔记-2</a><time datetime="2025-05-05T06:46:28.344Z" title="Created 2025-05-05 14:46:28">2025-05-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/20/%E7%AE%97%E6%B3%95/" title="一些算法"><img src="/imgs/algorithm.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="一些算法"/></a><div class="content"><a class="title" href="/2025/04/20/%E7%AE%97%E6%B3%95/" title="一些算法">一些算法</a><time datetime="2025-04-20T08:38:29.680Z" title="Created 2025-04-20 16:38:29">2025-04-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/16/Diagrammatize_GPT/" title="《GPT图解》学习笔记"><img src="/imgs/GPT2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="《GPT图解》学习笔记"/></a><div class="content"><a class="title" href="/2025/04/16/Diagrammatize_GPT/" title="《GPT图解》学习笔记">《GPT图解》学习笔记</a><time datetime="2025-04-16T03:29:14.257Z" title="Created 2025-04-16 11:29:14">2025-04-16</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By Mercik</span></div><div class="footer_custom_text">I wish you to become your own sun, no need to rely on who's light.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll to Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const initValine = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyValine = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const valineConfig = {
      el: '#vcomment',
      appId: 'uu6tSxmBoCndulFYKGKobx2L-MdYXbMMI',
      appKey: 'jBho9VeFfJwBsfbRDlg0zM6B',
      avatar: '',
      serverURLs: 'https://uu6tsxmb.api.lncldglobal.com',
      emojiMaps: "",
      visitor: false,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || window.location.pathname
    }

    new Valine(valineConfig)
  }

  const loadValine = async (el, path) => {
    if (typeof Valine === 'function') {
      initValine(el, path)
    } else {
      await btf.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js')
      initValine(el, path)
    }
  }

  if (isShuoshuo) {
    'Valine' === 'Valine'
      ? window.shuoshuoComment = { loadComment: loadValine }
      : window.loadOtherComment = loadValine
    return
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><script src="/js/custom.js"></script><script src="/js/mode_change.js"></script><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/js/cat.js"></script><script type="text/javascript" src="https://unpkg.zhimg.com/jquery@latest/dist/jquery.min.js"></script><script async data-pjax src="/js/txmap.js"></script><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://duqi666.github.io/categories/大模型/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 大模型LLM (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://duqi666.github.io/categories/算法/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 算法 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="https://duqi666.github.io/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: var(--btn-bg)}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1s');
    arr[i].setAttribute('data-wow-delay', '0s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1s');
    arr[i].setAttribute('data-wow-delay', '0s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('post-class');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '0.5s');
    arr[i].setAttribute('data-wow-delay', '0s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('archive-class');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1s');
    arr[i].setAttribute('data-wow-delay', '0s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('page-class');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1s');
    arr[i].setAttribute('data-wow-delay', '0s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/05/05/Reinforcement_Learning_part2/" alt=""><img width="48" height="48" src="/imgs/GPT2.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-05-05</span><a class="blog-slider__title" href="2025/05/05/Reinforcement_Learning_part2/" alt="">《深度学习4-强化学习》学习笔记-2</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/05/05/Reinforcement_Learning_part2/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/08/24/Reinforcement_Learning/" alt=""><img width="48" height="48" src="/imgs/Reinforcement_Learning.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-08-24</span><a class="blog-slider__title" href="2025/08/24/Reinforcement_Learning/" alt="">《深度学习4-强化学习》学习笔记</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/08/24/Reinforcement_Learning/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/04/20/算法/" alt=""><img width="48" height="48" src="/imgs/algorithm.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-20</span><a class="blog-slider__title" href="2025/04/20/算法/" alt="">一些算法</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/04/20/算法/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/04/16/Diagrammatize_GPT/" alt=""><img width="48" height="48" src="/imgs/GPT2.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-16</span><a class="blog-slider__title" href="2025/04/16/Diagrammatize_GPT/" alt="">《GPT图解》学习笔记</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/04/16/Diagrammatize_GPT/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>